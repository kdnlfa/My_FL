第一部分 系统模型
1）联邦学习过程
考虑一个基于共享网络资源的联邦学习场景，该场景包含一组服务提供商${R}=\{1,\cdots,r,\cdots,{R}\}$
以及相关联的一组客户端${N}=\{1,\cdots,i,\cdots,N\}.$
与服务 r 相关联的客户端 i 的本地数据集表示为 $\mathcal{D}_{i,r}$，每个样本 $\xi\in\mathcal{D}_{i,r}$ ，由特征集及其相应标签组成。因此，每个客户端 i 的本地损失函数可以定义为：
(1)
$$L_{i,r}\left(\boldsymbol{\omega}\right)=\frac{1}{|\mathcal{D}_{i,r}|}\sum_{\xi\in\mathcal{D}_{i,r}}l\left(\boldsymbol{\omega};\xi\right),$$

其中，函数 $l\left(\boldsymbol{\omega};\xi\right)$ 用于衡量基于参数$\omega=$$[\omega_{1},\cdots,\omega_{d},\cdots,\omega_{|\omega|}]$ 和采样数据 $\xi$ 的预测值与真实值之间的差异。相应地，服务 r 的全局最小化问题可以定义为最小化多个本地损失函数的聚合，即：
(2)
$$\omega^{*}=\arg\min_{\boldsymbol{\omega}}L_{r}\left(\boldsymbol{\omega}\right)=\arg\min_{\boldsymbol{\omega}}\sum_{i=1}^{N}\kappa_{i,r}L_{i,r}\left(\boldsymbol{\omega}\right),$$
其中，聚合权重为$\kappa_{i,r}=|\mathcal{D}_{i,r}|/\sum_{i=1}^N|\mathcal{D}_{i,r}|.$
考虑服务 r 的第 t 轮全局训练，其中公式 (2) 中的全局模型表示为 ${\mathbf{\omega }}_{r,t}$ 
服务器将模型  ${\mathbf{\omega }}_{r,t}$  分发给选定的客户端，然后客户端执行 $\tau$ 步基于随机梯度下降（SGD）的本地更新，以完成一轮完整的训练。从数学角度，本地更新方法可以表示为：
(3)
$$\boldsymbol{\omega}_{i,r,t}^{(k)}=\boldsymbol{\omega}_{i,r,t}^{(k-1)}-\eta_{k}\tilde{\nabla}L_{i,r}\left(\boldsymbol{\omega}_{i,r,t}^{(k-1)}\right),$$
其中$k\in\{1,\cdots,\tau\},$  $\eta_{k}$ 表示与k有关的学习率，$\tilde{\nabla}L_{i,r}\left(\cdot\right)$表示本地损失函数的随机梯度，并且$\omega_{i,r,t}^{(0)}=\omega_{r,t}.$ 。在执行 $\tau$ 次本地更新后，客户端获得一个新模型，并将相应参数 $\omega_{i,r,t}^{(\tau)}$ 的量化值 $\Psi(\omega_{i,r,t}^{(\tau)})$ 上传到指定服务器。在服务器端，选定客户端上传的这些参数将按如下方式聚合：
(4)
$$\omega_{r,t+1}=\sum_{i=1}^N\kappa_{i,r}\Psi\left(\boldsymbol{\omega}_{i,r,t}^{(\tau)}\right).$$
客户端根据服务器指定的量化策略 $\Psi \left( \cdot \right)$ 在上传模型前对其进行压缩。值得注意的是，不同 SP 的服务器也可以校准合适的聚合策略，如客户端选择，以及有效的资源分配技术，这些将在后续讨论。

2）量化过程
通过对模型参数$\omega$的元素进行统计量化，将其量化为一些离散级别，以此来减小数据流的大小。其第d个元素的最大q级量化表示为：
(5)
$$\Psi_q\left(\omega_d\right)=\left\|\boldsymbol{\omega}\right\|_p\cdot\mathrm{sgn}(\omega_d)\cdot\Xi_q\left(\omega_d,q\right),$$
其中，$\|\cdot\|_{p}$表示p范数，$\mathrm{sgn}(\cdot)$表示符号函数，$\Xi_{q}(\omega_{d},q)$是一个随机映射，定义如下：
(6)
$$\Xi_q\left(\omega_d,q\right)=\begin{cases}u/q,&\varepsilon\leq1-P\left(\frac{\left|\omega_d\right|}{\left\|\omega\right\|_p},q\right)\\\left(u+1\right)/q,&\text{otherwise}\end{cases}$$
其中，$\varepsilon$ 表示一个均匀分布随机变量的概率输出。$P\left( {e,q}\right) = {eq}-u$，其中$0\leq e\leq1$，u是满足$u\in[0,q)$ 且 $\frac{u}{q}\leq\frac{\left|\omega_d\right|}{\left\|\omega\right\|_p}\leq\frac{u + 1}{q}$ 的任意整数 。由于每个元素量化后的值用 $\left\lceil {{\log }_{2}q}\right\rceil + 1$ 位（包括一位符号位）表示，再加上 32 位的梯度范数，在第 t 轮训练中，客户端 i 为服务 r 上传的长度为 $\left|\mathbf{\omega}\right|$ 的量化参数向量的通信量从原来的 $32\left|\omega\right|$ 减少到：
(7)
$$\mathrm{vol}_{r,t}=|\boldsymbol{\omega}|\left(\lceil\log_{2}q_{i,r,t}\rceil+1\right)+32,$$
其中，${q}_{i,r,t}$ 是服务r的客户端i在第t轮选择的量化级别。因此，单个服务的总通信量为${\operatorname{vol}}_{r,t}=\mathop{\sum }\limits_{{i \in \mathcal{N}}}{\operatorname{vol}}_{i,r,t}$

3）通信延迟和能量消耗
在无线网络场景中，联邦学习训练的延迟和能耗主要产生于本地模型更新以及模型参数传输阶段。在第t轮针对服务r的训练中，客户端i的计算能耗可表示为：
(8)
$$E_{i,r,t}^{cmp} = \mu_i c_{i,r} |D_{i,r}| f_{i,r,t}^2$$
其中，$\mu_i$ 是由芯片架构决定的有效电容常数，$f_{i,r,t}$ 表示客户端i在第 t 轮的 CPU 周期频率，$c_{i,r}$ 代表客户端i为服务r执行一个样本所需的 CPU 周期数。同时，产生的本地计算延迟可写为：
(9)
$$T_{i,r,t}^{\mathrm{cmp}}=c_{i,r}|\mathcal{D}_{i,r}|/f_{i,r,t}.$$
另一方面，在模型参数传输阶段，我们考虑在客户端和服务提供商之间的通信采用频分多址（FDMA）技术。根据香农公式，客户端i的传输速率可表示为：
(10)
$$v_{i,r,t}=B_{i,r,t}\log_2\left(1+\frac{g_{i,t}p_{i,t}}{B_{i,r,t}N_0}\right),$$
其中，$B_{i,r,t}$ 代表在第 t 轮全局训练中分配给客户端 i 的与服务 r 相关的带宽，$g_{i,t}$ 和 $p_{i,t}$ 分别表示客户端i对应的信道增益和传输功率，$N_0$ 是单边白噪声功率谱密度。因此，客户端i的通信延迟和传输能耗可计算为：
(11)
$$T_{i,r,t}^\mathrm{com}=\frac{\mathrm{vol}_{i,r,t}}{v_{i,r,t}},$$
(12)
$$E_{i,r,t}^{\mathrm{com}}=T_{i,r,t}^{\mathrm{com}}p_{i,t}=\frac{\mathrm{vol}_{i,r,t}p_{i,t}}{v_{i,r,t}}.$$
综上所述，服务r在第t轮的总能耗和总延迟表示如下：
(13a)
$$E_{r,t}^{\text{total}}=\sum_{i\in\mathcal{N}}\left(E_{i,r,t}^{\text{com}}+E_{i,r,t}^{\text{cmp}}\right),$$
(13b)
$$T_{r,t}^{\text{total}}=\max_{i\in\mathcal{N}}\left(T_{i,r,t}^{\text{cmp}}+T_{i,r,t}^{\text{com}}\right).$$
以上公式详细说明了在联邦学习过程中，如何计算客户端在本地计算和通信过程中的能耗与延迟，以及如何汇总得到整个服务在某一轮训练中的总能耗和总延迟，为后续研究资源优化分配和系统性能评估提供了量化依据。

第二部分 问题建模
每个服务提供商的主要目标是制定最优的客户端选择策略${n}_{r, t}$、量化等级${q}_{r, t}$以及通信与(平均)计算资源分配方案(即${B}_{r, t} = \mathop{\sum }\limits_{{i \in  \mathcal{N}}}{B}_{i, r, t}$和${f}_{r, t}$)，以最大化联邦学习过程的性能。客户端选择与量化被识别为显著影响联邦学习模型训练精度的关键因素。一般而言，增加参与客户端数量${n}_{r, t}$和提高量化等级${q}_{r, t}$有助于提升训练精度。然而，服务提供商有限的可用带宽对通信和计算资源形成了双重约束。每个服务提供商 $r$ 会自然请求更多带宽${B}_{r, t}$以满足其训练目标。此外，通过优化本地CPU频率${f}_{r, t}$可最小化参与客户端在联邦学习训练过程中的能耗。不同服务根据其复杂度需要不同的计算频率——例如简单模型可在较低频率下运行以节省功耗。因此，我们将${f}_{r, t}$纳入优化框架的决策变量集合。
我们在有限时间跨度内考虑优化目标，即在联邦学习连续多轮全局训练中，实现训练时间、能耗与模型精度之间的平衡权衡。各服务提供商根据自身需求和限制独立优化策略:
(14)
$$
\mathop{\min }\limits_{{\mathbf{f},\mathbf{n},\mathbf{q},\mathbf{B}}}\mathop{\sum }\limits_{{t = 0}}^{{T - 1}}{\gamma }^{t}\Upsilon \left( {{L}_{r}\left( {\mathbf{\omega }}_{r, t}\right) ,{\operatorname{vol}}_{r, t},{E}_{r, t}^{\text{total }},{T}_{r, t}^{\text{total }}}\right)
$$
$$
\text{s.t.}\;{C1}\;{E}_{r, t}^{\text{total }} \leq  {E}_{r}^{\max },{T}_{r, t}^{\text{total }} \leq  {T}_{r}^{\max },\forall t, r
$$
$$
{C2}{n}_{r, t} \in  \left\lbrack  {1, N}\right\rbrack  ,\forall t, r
$$
$$
{C3}{f}^{\min } \leq  {f}_{r, t} \leq  {f}^{\max },\forall t, r
$$
$$
\text{C4}{B}^{\min } \leq  \mathop{\sum }\limits_{r}{B}_{r, t} \leq  {B}^{\max },\forall t, r
$$
$$
\text{C5}{q}_{r, t} \in  \left\lbrack  {1,{2}^{32}}\right\rbrack  ,\forall t, r
$$
其中$\gamma$表示折扣因子，单调递减函数$\Upsilon \left( {{L}_{r}\left( {\mathbf{\omega }}_{r, t}\right) ,{\operatorname{vol}}_{r, t},{E}_{r, t}^{\text{total }},{T}_{r, t}^{\text{total }}}\right)$对其中各分量进行加权，其具体形式将在后文讨论。此外，$\mathbf{{C1}},\mathbf{{C3}}$和$\mathbf{{C4}}$施加了与服务能耗${E}_{r}^{\max }$、总延迟${T}_{r}^{\max }$、计算设备能力$\left\lbrack  {{f}^{\min },{f}^{\max }}\right\rbrack$及带宽预算$\left\lbrack  {{B}^{\min },{B}^{\max }}\right\rbrack$相关的常见约束，而$\mathbf{{C2}}$与$\mathbf{{C5}}$则规定了客户端选择范围和量化级别的自然限制。值得注意的是，延迟按客户端测量，可能显著小于服务提供商(SP)的决策周期。这一特性允许多个SP通过空中接口同步获取决策结果。
求解公式(14)定义的问题面临重大挑战，主要源于目标函数的隐含特性——它并不直接依赖于决策变量。此外，该问题属于混合整数非线性优化问题，离散决策变量${n}_{r, t}$和${q}_{r, t}$与连续变量并存，进一步增加了求解难度。为处理这种复杂性，我们将公式(14)重构为马尔可夫决策过程(MDP)，实现决策变量的迭代更新。随后采用多智能体强化学习(MARL)应对序列决策挑战，通过试错学习获取最优策略。同时引入非合作博弈论方法，以解决SP间因专有性和隐私顾虑导致策略信息无法共享的问题。各SP基于对其他方最优联合策略的推测独立制定策略，通过非合作决策过程迭代调整直至达到均衡状态。

第三部分 马尔可夫决策过程
我们将多SP联邦学习(FL)的通信与计算协同优化问题建模为序列决策问题，采用MDP框架表示。该MDP由五元组$\left( {\mathcal{O},\mathcal{A},\operatorname{rwd},\Lambda ,\gamma }\right)$描述，其中$\mathcal{O}$和$\mathcal{A}$分别表示观测空间与动作空间，rwd是通过当前状态与动作映射获得的奖励函数，$\Lambda$表征环境动态的状态转移概率函数，$\gamma$为折扣因子。观测、动作及奖励的具体定义如下。
观测空间：第$t$轮FL训练后智能体$r$的观测可表示为
(15)
$$
{o}_{r, t} = \left\{  {{Z}_{r, t}\left( \mathbf{\omega }\right) ,{\Theta }_{r, t},{\mathbf{B}}_{t}}\right\}
$$
其中${Z}_{r, t}\left( \mathbf{\omega }\right)  = \left\{  {t,{L}_{r}\left( {\mathbf{\omega }}_{r, t}\right) ,{\Gamma }_{r, t},{q}_{r, t}}\right\}$记录联邦学习训练的轮次时间、损失值、模型精度和量化级别，${\Theta }_{r, t} = \left\{  {{T}_{r, t}^{\text{total }},{E}_{r, t}^{\text{total }},{\operatorname{vol}}_{r, t}}\right\}$包含当前联邦学习模型的训练状态，${\mathbf{B}}_{t} = {\left\{  {B}_{r, t}\right\}  }_{r \in  \mathcal{R}}$表示所有服务提供商的带宽分配。这种状态设置具有实际意义:一方面，由于不同服务提供商通常不愿公开具体运营细节，前两个元素${\Theta }_{r, t}$和${Z}_{r, t}$仅包含内部可用信息；另一方面，为建立多智能体强化学习中智能体间的博弈关系，我们将带宽分配信息视为公开信息，因其可通过网络运营商获取。
动作空间：动作空间包含公式(14)中所有决策变量，可表示为：
(16)
$$
{a}_{r, t} = \left\{  {{n}_{r, t},{f}_{r, t},{B}_{r, t},{q}_{r, t}}\right\}   \in  \mathcal{A}
$$
底层客户端据此进一步确定其行为。具体而言，假设带宽${B}_{r, t}$在各服务内选定的客户端间平均分配，每个客户端的CPU频率${f}_{i, r, t}$和量化级别${q}_{i, r, t}$会在给定动作下小幅波动，即${f}_{i, r, t} \sim G\left( {{f}_{r, t},{\sum }_{f}}\right) ,{q}_{i, r, t} \sim  G\left( {{q}_{r, t},{\sum }_{s}}\right) ,$  $G$ 服从高斯分布。此外，我们定义${\mathbf{a}}_{t} = \left\lbrack  {{a}_{1, t},\cdots ,{a}_{R, t}}\right\rbrack$，并假设其他智能体的动作可从网络运营商及参与客户端处获取。

奖励函数：为匹配公式(14)中的目标函数$\Upsilon$，我们将奖励函数定义为：
(17)
$$
{\operatorname{rwd}}_{r, t} = {\sigma }_{1}{\Gamma }_{r, t} + {\sigma }_{2}{\Phi }_{r, t}\left( \mathbf{q}\right)  - {\sigma }_{3}{E}_{r, t}^{\text{total }} - {\sigma }_{4}{T}_{r, t}^{\text{total }}
$$
其中权重系数${\sigma }_{j}, j \in  \{ 1,\cdots ,4\}$为正的常数，负号将时延与能耗的最小化问题转化为奖励最大化问题。此外，定义对抗因子$\Phi$来显式表征服务提供商间争夺通信资源的量化策略博弈:
(18)
$$
{\Phi }_{r, t}\left( \mathbf{q}\right)  = \frac{{n}_{r, t}{q}_{r, t}}{\epsilon  \times  {\operatorname{vol}}_{r, t} + \mathop{\sum }\limits_{{j \in  \mathcal{R}/\{ r\} }}{n}_{j, t}{q}_{j, t}}
$$
这里$\epsilon$为常数。顾名思义，该对抗因子强调增加量化位数虽加速联邦学习收敛，却会增加通信开销；同时其他服务产生的数据流会对当前服务造成不利影响。值得注意的是，智能体决策时并不知晓其他智能体的动作，但当各智能体最终执行确定动作后，部分$\mathop{\sum }\limits_{{j \in  \mathcal{R}/\{ r\} }}{n}_{j, t}{q}_{j, t}$信息可通过网络运营商获知。
通过这个马尔可夫决策过程，每个服务提供商基于博弈论策略感知环境执行动作，并获取相应奖励。下面我们将讨论如何采用PAC算法获得由$\phi$参数化的学习策略${\pi }_{r}$，该策略根据特定观察决定动作${a}_{r, t}$，在保障服务质量的同时最小化通信与计算成本。

第四部分 基于帕累托行动者-评论家的多服务提供商博弈
本部分介绍PAC-MCoFL的算法框架。首先定义联合策略$\mathbf{\pi } = \left( {{\pi }_{r},{\pi }_{-r}}\right)$，其中${\pi }_{-r}$表示所有其他智能体的策略集合(记为$- r$)。此外，PAC-MCoFL中强化学习具有有限时间跨度结构。对于每个服务提供商$r$，我们将联合策略$\pi$下的累积期望奖励表示为：
(19)
$$
{J}_{r}\left( \mathbf{\pi }\right)  = {\mathbb{E}}_{{o}_{r,0}}\left\lbrack  {{V}_{r}^{\mathbf{\pi }}\left( {o}_{r,0}\right) }\right\rbrack   = {\mathbb{E}}_{{o}_{r,0}}\left\lbrack  {\mathop{\sum }\limits_{{t = 0}}^{{T - 1}}{\gamma }^{t}{\mathbb{E}}_{\mathbf{\pi }}\left\lbrack  {{\operatorname{rwd}}_{r, t} \mid  {o}_{r,0},\mathbf{\pi }}\right\rbrack  }\right\rbrack
$$
其中${V}_{r}^{\pi }$表示智能体$r$在联合策略$\pi$下的状态价值函数，$\gamma  \in  \lbrack 0,1)$为折扣因子。等价地可表示为：
(20)
$$
{J}_{r}\left( \mathbf{\pi }\right)  = \mathop{\sum }\limits_{{o}_{r, t}}{d}^{\mathbf{\pi }}\left( {o}_{r, t}\right) \left\lbrack  {{V}_{r}^{\mathbf{\pi }}\left( {o}_{r, t}\right) }\right\rbrack
$$
式中${d}^{\pi }\left( o\right)$表示马尔可夫决策过程在$\pi$下的平滑分布中，从初始观测${o}_{r,0}$到观测$o$的转移概率。此外，$Q$函数可表达为${Q}_{r}^{\mathbf{\pi }}\left( {{o}_{r, t},{\mathbf{a}}_{t}}\right)  = {\operatorname{rwd}}_{r, t}\left( {{o}_{r, t},{\mathbf{a}}_{t}}\right)  + \gamma \mathbb{E}\left\lbrack  {{V}_{r}^{\mathbf{\pi }}\left( {o}_{r, t + 1}\right) }\right\rbrack  .$
帕累托最优均衡是指任何智能体都无法在不降低其他智能体期望收益的情况下提升自身期望收益的状态。借鉴囚徒困境理论，若智能体能够有效协调并确信彼此不会偏离均衡，由此产生的联合策略不仅能最大化单个智能体的期望收益，也能实现所有参与者的整体最优。
换言之，对于智能体$r$，若其他智能体坚持${\pi }_{-r}^{ \dagger  }$最大化${J}_{r}\left( {{\pi }_{r},{\pi }_{-r}}\right)$，则
(21)
$$
{\pi }_{-r}^{ \dagger  } \in  \arg \mathop{\max }\limits_{{\pi }_{-r}}{J}_{r}\left( {{\pi }_{r},{\pi }_{-r}}\right)
$$
基于PAC的智能体可以学习到最大化${J}_{r}\left( {{\pi }_{r},{\pi }_{-r}^{ \dagger  }}\right)$的策略${\pi }_{r}$。该过程最终形成的联合策略${\mathbf{\pi }}^{ \dagger  } = \left( {{\pi }_{r},{\pi }_{-r}^{ \dagger  }}\right)$能使所有智能体的累积奖励最大化，从而用帕累托最优均衡替代纳什均衡，即
(22)
$$
{\pi }_{r} \in  \arg \mathop{\max }\limits_{{\pi }_{r}}{J}_{r}\left( {{\pi }_{r},{\pi }_{-r}^{ \dagger  }}\right)
$$
因此，难点转为推测联合策略${\pi }_{-r}^{ \dagger  }$。实践中可通过评估其他智能体所有可能的联合行动
 (i.e. 对所有 $a_{-r,t} \in \mathcal{A}_{-r}, \mathcal{A}_{-r} = \bigcup_{j \neq r} \mathcal{A}_j$)  的联合$Q$值取最大值算子)来近似求解，形式化表示为：
 (23)
 $$
{\pi }_{-r}^{ \dagger  } \in  \arg \mathop{\max }\limits_{{a}_{-r, t}}{Q}_{r}^{{\pi }^{ \dagger  }}\left( {{o}_{r, t},{a}_{r, t},{a}_{-r, t}}\right)
$$
(24)
$$
{M}_{r}\left( \mathbf{\pi }\right)  \triangleq  {\mathbb{E}}_{{a}_{r, t},{a}_{r, t + 1} \sim  {\pi }_{r},{a}_{-r, t} \sim  {\pi }_{-r}}\left\lbrack  {\left( {\operatorname{rwd}}_{r, t} + \gamma \mathop{\max }\limits_{{a}_{-r, t}}{Q}_{r}^{{\mathbf{\pi }}^{ \dagger  }}\left( {o}_{r, t + 1},{a}_{r, t + 1},{a}_{-r, t}\right)  - {Q}_{r}^{{\mathbf{\pi }}^{ \dagger  }}\left( {o}_{r, t},{a}_{r, t},{a}_{-r, t}\right) \right) }^{2}\right\rbrack
$$
(25)
$$\begin{aligned}\nabla_{\phi_{r}}J_{r}\left(\boldsymbol{\pi}\right)&=\sum_{o_{r,t}}d^{\boldsymbol{\pi}}(o_{r,t})\sum_{a_{r,t},a_{-r,t}}\nabla_{\phi_{r}}\pi^{\dagger}(\boldsymbol{a}_{t}\mid o_{r,t})Q_{r}^{\boldsymbol{\pi}^{\dagger}}(o_{r,t},a_{r,t},a_{-r,t})\\&=\sum_{o_{r,t}}d^{\boldsymbol{\pi}}(o_{r,t})\sum_{a_{r,t},a_{-r,t}}\pi_{-r}^{\dagger}\left(a_{-r,t}\mid o_{r,t},a_{r,t}\right)\nabla_{\phi_{r}}\pi_{r}\left(a_{r,t}\mid o_{r,t}\right)Q_{r}^{\boldsymbol{\pi}^{\dagger}}\left(o_{r,t},a_{r,t},a_{-r,t}\right)\\&=\sum_{o_{r,t}}d^{\boldsymbol{\pi}}(o_{r,t})\sum_{a_{r,t},a_{-r,t}}\pi_{r}\left(a_{r,t}\mid o_{r,t}\right)\pi_{-r}^{\dagger}\left(a_{r,t}\mid o_{r,t},a_{r,t}\right)\cdot\nabla_{\phi_{r}}\log\pi_{r}\left(a_{r,t}\mid o_{r,t}\right)Q_{r}^{\boldsymbol{\pi}^{\dagger}}\left(o_{r,t},a_{r,t},a_{-r,t}\right)\end{aligned}$$

根据标准离轨策略actor-critic设计，为训练评判网络${Q}_{r}^{{\mathbf{\pi }}^{ \dagger  }}$，我们定义选定动作与目标之间的时序差分(TD)误差${M}_{r}\left( \mathbf{\pi }\right)$如式(24)所示，并相应更新评判网络：
(26)
$$
{\theta }_{r} = {\theta }_{r} - \alpha {\nabla }_{{\theta }_{r}}{M}_{r}\left( \mathbf{\pi }\right)
$$
其中$\alpha$表示更新学习率。另一方面，为最大化期望折现累积奖励，策略网络可通过下式更新:
(27)
$$
{\phi }_{r} = {\phi }_{r} + \zeta {\nabla }_{{\phi }_{r}}{J}_{r}\left( \mathbf{\pi }\right)
$$
其中策略梯度可通过求取$\phi$的导数计算得出，如公式(25)所示，学习率为$\zeta$。需要说明的是，通常从${J}_{r}\left( \mathbf{\pi }\right)$中减去基线值，这不会改变梯度计算过程，但能降低梯度估计的偏差。我们可以通过$\mathop{\sum }\limits_{{a}_{r, t}}{\pi }_{r}\left( {{a}_{r, t} \mid  {o}_{r, t};{\phi }_{r}}\right) {Q}_{r}^{{\mathbf{\pi }}^{ \dagger  }}\left( {{o}_{r, t},{a}_{r, t},{a}_{-r, t}}\right)$计算该基线值，从而避免训练额外的状态价值网络。
总体而言，相较于现有架构，PAC-MCoFL通过策略对抗猜想(PAC)机制推测其他智能体的策略，从而协同优化量化等级、资源分配和客户端分配等决策。该优势能有效防止博弈陷入局部最优解。PAC-MCoFL的完整算法流程如算法1所示。

算法1:PAC-MCoFL(Privacy-Aware Collaborative Multi-Center Federated Learning)的训练流程，从代理(即服务提供商SP)视角出发的算法。
输入：总 episodes 数$num_{max}$；智能体r的行动者 actor 初始化参数 $\phi_{r}$ 和评论家 critic 初始化参数$\theta_{r}$；联邦学习全局模型 $\omega_{r}$，联邦学习全局训练轮数T；总客户端集合 $\mathcal{N}$。  
输出：智能体r的行动者训练后的参数 $\phi_{r}$ 和评论家训练后的参数$\theta_{r}$ 。
1 Load the training dataset of FL service; num$_\mathrm{eps}\leftarrow0;$
2 while each episode num$_\mathrm{eps}\leq \mathrm{num}_\mathrm{max}$ do
3 Initialize global model parameters $\omega_{r,0}$ and client features, e.g., $|\mathcal{D}_i,r|,{o_{r,0}},f_{r,0};$
4 for each round $t=0,1,\cdots, T-1$ in do
5 Get observation $o_{r,t}$ and $o_{-r,t}$；
6 Sample $a_{r,t}$ = {${n_{r,t}, f_{r,t}, B_{r,t}, q_{r,t}}$} from $\pi_{r}(a_{r,t}|o_{r,t})$ and $a_{-r,t} \sim \pi_{-r}(a_{-r,t}|o_{-r,t})$;
7 for each selected client $i \in \mathcal{N}$ do
8 Local update by Eq. (3) and get $\omega_i,r,t;$
9 Obtain ${E} _{i, t, r}^{\mathrm{cmp}}, {T} _{i, t, r}^{\mathrm{cmp}}$ by Eq. (9) and (8)
10 Obtain $E_{i, t, r}^{\text{com}}, T_{i, t, r}^{\text{com}}$ by Eq.(11) and (12)
11 end
12 Global update $\omega_{r,t+1} = \sum_{i=1}^{N} \frac{|\mathcal{D}_{i,r,t}| \Psi(\omega_{i,r,t})}{\sum_{i=1}^{N} |\mathcal{D}_{i,r,t}|}$
13 Distributes global model $\omega_{r,t+1}$ to all clients;
14 Obtain $\Gamma_{r,t}$ through model test;
15 Obtain $E_{t,r}^{\text{total}}$, $T_{t,r}^{\text{total}}$ by Eq. (13);
16 Get $o_{r,t+1}$, $o_{-r,t+1}$ by Eq. (15);
17 Compute $\text{rwd}_{r,t}$ by Eq. (17);
18 Put $(o_{r,t}$, $a_{r,t}$, $\text{rwd}_{r,t}$, $o_{r,t+1})$ into an episode batch;
19 end
20 Insert the episode batch into a replay buffer H;
21 Clear the episode batch.
22 if H reaches the buffer size for training then
23 Sample a single batch from H;
24 Obtain virtual joint policy $\pi_{-r}^\dagger \in \arg \max_{a_{-r,t}} Q_r^{\pi^\dagger}(o_{r,t},a_{r,t},a_{-r,t}),\forall t;$
25 Update the critic network by Eq. (26);
26 Update the actor network by Eq. (27);
27 end
28 $\mathrm{num}_{\mathrm{eps}}\longleftarrow\mathrm{num}_{\mathrm{eps}}+1.$
29 end

第五部分 降维动作空间变换

PAC中通过式(23)计算猜想联合策略的特性，本质上限制了其仅适用于离散动作空间场景。虽然动作网格和动作编码等技术可将多维连续动作空间离散化，但常面临网格边界突变或需要复杂神经网络训练等挑战。笛卡尔积方法提供了更优解，可独立控制各维度离散化粒度并生成可解释的离散动作。为应对维度爆炸问题，我们将动作空间各维度转换为三元表示，并按以下方式重构动作剖面，确保与PAC算法的无缝兼容。
针对动作向量${a}_{r, t} =$$\left\{  {{n}_{r, t},{f}_{r, t},{B}_{r, t},{q}_{r, t}}\right\}$中的4个元素，按特定粒度离散化每个元素$m \in$$\{ 1,\cdots ,4\}$，并重新定义三元变量${a}_{\left( m\right) }^{\prime } = \{  - 1,0,1\}$以简化动作空间。其中0表示保持当前值，-1和1分别对应按单位粒度增减当前值。经变换后，动作空间${\mathcal{A}}^{\prime } = \mathop{\prod }\limits_{m}{a}_{\left( m\right) }^{\prime }$的笛卡尔积在本研究场景中最多存在${3}^{4}$种可能性。这种采用简化增量动作变量的方法，显著降低了算法执行过程中需要遍历的$Q$函数空间维度，从而提升算法稳定性。

第六部分 设计实验

我们构建包含五个客户端(即$N = 5$)的联邦学习环境，由三个服务提供商(即$R = 3$)协调。这些服务提供商调度客户端分别训练CIFAR-10、FashionMNIST和MNIST数据集。根据任务难度，采用三种图像分类模型。

参数表
<table><tbody><tr><td>参数</td><td>描述</td><td>取值</td></tr><tr><td>$N$</td><td>客户端数量</td><td>5</td></tr><tr><td>$R$</td><td>任务数量</td><td>3</td></tr><tr><td>$\rho$</td><td>非独立同分布数据程度</td><td>1</td></tr><tr><td>$\tau$</td><td>联邦学习本地更新步数</td><td>3</td></tr><tr><td>$T$</td><td>联邦学习全局训练轮次</td><td>35</td></tr><tr><td>${g}_{i, t}$</td><td>信道增益</td><td>$\left\lbrack  {-{63}, - {73}}\right\rbrack  \mathrm{{dB}}$</td></tr><tr><td>${N}_{0}$</td><td>噪声功率谱密度</td><td>$\left\lbrack  {-{124}, - {174}}\right\rbrack  \mathrm{{dBm}}/\mathrm{{Hz}}$</td></tr><tr><td>${p}_{i, t}$</td><td>客户端发射功率</td><td>$\left\lbrack  {{10},{33}}\right\rbrack  \mathrm{{dBm}}$</td></tr><tr><td>${\mu }_{i}$</td><td>有效切换电容常数</td><td>${10}^{-{27}}$</td></tr><tr><td>${c}_{i,1},{c}_{i,2}$</td><td>每样本CPU周期消耗量(${r}_{1},{r}_{2}$)</td><td>$\left\lbrack  {{6.07},{7.41}}\right\rbrack   \times  {10}^{5}$</td></tr><tr><td>${c}_{i,3}$</td><td>每样本CPU周期消耗量(${r}_{3}$)</td><td>$\left\lbrack  {{1.10},{1.34}}\right\rbrack   \times  {10}^{8}$</td></tr><tr><td>${\sigma }_{1}$</td><td>权重因子1(${r}_{1},{r}_{2},{r}_{3}$)</td><td>100,100,100</td></tr><tr><td>${\sigma }_{2}$</td><td>权重因子2(${r}_{1},{r}_{2},{r}_{3}$)</td><td>4.8,31.25,12.5</td></tr><tr><td>${\sigma }_{3},{\sigma }_{4}$</td><td>权重因子3与4(${r}_{1},{r}_{2},{r}_{3}$)</td><td>0.8,25,16.6</td></tr><tr><td>$\zeta ,\alpha$</td><td>行动者-评论者网络学习率</td><td>0.001 , 0.001</td></tr><tr><td>${\sum }_{q},{\sum }_{f}$</td><td>量化与CPU频率抖动因子</td><td>0.25,0.5</td></tr><tr><td>${f}^{\min },{f}^{\max }$</td><td>客户端CPU频率范围</td><td>$\left\lbrack  {{0.5},{3.5}}\right\rbrack  \mathrm{{GHz}}$</td></tr><tr><td>${B}^{\min },{B}^{\max }$</td><td>服务提供商带宽范围</td><td>$\left\lbrack  {0,{30}}\right\rbrack  \mathrm{{MHz}}$</td></tr></tbody></table>

- 任务${r}_{1}$的模型包含一个四层卷积神经网络(CNN)，其卷积层分别具有48、96、192和256个滤波器，各采用$3 \times  3$核尺寸，后接一个全连接网络，包含512、64和10个神经元层。
- 任务${r}_{2}$的模型采用双层CNN架构，卷积层分别配置32和64个尺寸为$5 \times  5$的滤波器，后续接两个含64和10个神经元的全连接层。
- 任务${r}_{3}$的模型由两个全连接层构成，分别包含128和10个神经元。

数据集以独立同分布(IID)方式分配给客户端，默认非IID数据分布程度为$\rho  = 1$。值得注意的是，$\rho  = 1$表示数据样本包含总标签类别中的${100}\%$。联邦学习训练采用Adam优化器，学习率设为${\eta }_{k} = {0.001}, k \in  \{ 1,\cdots ,\tau \}$，本地迭代次数$\tau  = 3$，最小批处理量为64。训练过程最多执行$T = {35}$轮。我们使用公式(17)中的奖励函数及其中各项指标来评估特定服务提供商(SP)的性能。在PAC算法中，$Q$网络设计为含64和128个神经元的双层全连接结构，策略网络则包含64、128和64个神经元的三层架构。其他仿真参数详见上表。

我们引入三种方法进行对比:1) MAPPO ，该方法在集中训练阶段共享奖励函数，但在协同优化执行阶段未引入推测联合策略；2) 统一方案，参与客户端、CPU频率和量化方法固定，带宽资源在服务提供商(SP)间平均分配；3) FedDQ ，其量化策略基于权重更新进行优化并逐步递减，且智能体(SP)仅关注自身策略。