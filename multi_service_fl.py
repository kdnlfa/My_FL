#!/usr/bin/env python3
"""
å°† FLSim åº“ä¸è®ºæ–‡çš„ç³»ç»Ÿæ¨¡å‹ç›¸é›†æˆï¼Œä»¥å®ç°ï¼š
- æ‹¥æœ‰å…±äº«ç½‘ç»œèµ„æºçš„å¤šä¸ªæœåŠ¡æä¾›å•†
- é‡åŒ–æ„ŸçŸ¥è”é‚¦å­¦ä¹ 
- é€šä¿¡æ„ŸçŸ¥çš„å®¢æˆ·ç«¯é€‰æ‹©ä¸èµ„æºåˆ†é…
- èƒ½é‡ä¸å»¶è¿Ÿä¼˜åŒ–
"""


try:
    from flsim.trainers.sync_trainer import SyncTrainer
    from flsim.servers.sync_servers import SyncServer, SyncServerConfig
    from flsim.optimizers.server_optimizers import FedAvgWithLROptimizerConfig
    from flsim.optimizers.local_optimizers import LocalOptimizerSGDConfig
    from flsim.active_user_selectors.simple_user_selector import UniformlyRandomActiveUserSelector, UniformlyRandomActiveUserSelectorConfig
    from flsim.data.data_provider import IFLDataProvider, IFLUserData, FLDataProviderFromList
    from flsim.data.data_sharder import SequentialSharder
    from flsim.data.dataset_data_loader import FLDatasetDataLoaderWithBatch
    from flsim.interfaces.data_loader import IFLDataLoader
    from flsim.interfaces.model import IFLModel
    from flsim.utils.config_utils import fl_config_from_json
    from hydra.utils import instantiate
    FLSIM_AVAILABLE = True
    print("FLSim æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"FLSim æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    FLSIM_AVAILABLE = False

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import copy
import json
from dataclasses import dataclass
from pathlib import Path

from quantization import QuantizationModule, AdaptiveQuantization
import math
from communication import SystemMetrics, ClientMetrics

@dataclass
class ServiceProviderConfig:
    """æœåŠ¡æä¾›å•†çš„é…ç½®"""
    service_id: int
    name: str
    client_ids: List[int]
    model_architecture: Dict[str, Any]
    quantization_level: int = 8
    local_epochs: int = 1
    learning_rate: float = 0.01
    users_per_round: int = 10 # æœåŠ¡æ¯è½®é€‰å‡ ä¸ªå®¢æˆ·ç«¯

# ç±»ä¸­çš„å‚æ•°æœ€åéƒ½ä¼šå½±å“åˆ°ä¼˜åŒ–é—®é¢˜
@dataclass 
class ClientResourceConfig:
    """å®¢æˆ·ç«¯èµ„æºé…ç½®"""
    client_id: int
    mu_i: float  # æœ‰æ•ˆç”µå®¹å¸¸æ•°
    c_ir: float  # æ¯ä¸ªæ ·æœ¬æ‰€éœ€çš„CPUå‘¨æœŸæ•°
    max_frequency: float  # æœ€å¤§CPUé¢‘ç‡
    max_power: float  # æœ€å¤§ä¼ è¾“åŠŸç‡
    channel_gain: float  # ä¿¡é“å¢ç›Š
    dataset_size: int  # æœ¬åœ°æ•°æ®é›†å¤§å°


class MultiServiceFLDataLoader(IFLDataLoader):
    """
    å¤šæœåŠ¡è”é‚¦å­¦ä¹ æ•°æ®åŠ è½½å™¨ã€‚
    æ‰©å±•FLSimçš„æ•°æ®åŠ è½½å™¨ä»¥æ”¯æŒå¤šæœåŠ¡ã€‚
    """
    
    def __init__(self, 
                    datasets: Dict[int, torch.utils.data.Dataset],
                    service_assignments: Dict[int, int],  # client_id -> service_id
                    batch_size: int = 32,
                    drop_last: bool = False,
                    eval_datasets: Optional[Dict[int, torch.utils.data.Dataset]] = None,
                    test_datasets: Optional[Dict[int, torch.utils.data.Dataset]] = None,
                    train_ratio: float = 0.8,
                    eval_ratio: float = 0.1,
                    test_ratio: float = 0.1,
                    random_seed: int = 42):
        self.datasets = datasets
        self.service_assignments = service_assignments
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.random_seed = random_seed

        # æ•°æ®åˆ†å‰²æ¯”ä¾‹
        assert abs(train_ratio + eval_ratio + test_ratio - 1.0) < 1e-6, "æ•°æ®åˆ†å‰²æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº1"
        self.train_ratio = train_ratio
        self.eval_ratio = eval_ratio
        self.test_ratio = test_ratio   

        # åˆ†å‰²åçš„æ•°æ®é›†
        self.train_datasets: Dict[int, torch.utils.data.Dataset] = {}
        self.eval_datasets: Dict[int, torch.utils.data.Dataset] = {}
        self.test_datasets: Dict[int, torch.utils.data.Dataset] = {}     
        
        # å¦‚æœæä¾›äº†ç‹¬ç«‹çš„è¯„ä¼°å’Œæµ‹è¯•æ•°æ®é›†ï¼Œç›´æ¥ä½¿ç”¨
        if eval_datasets is not None:
            self.eval_datasets = eval_datasets
        if test_datasets is not None:
            self.test_datasets = test_datasets

        # è¿›è¡Œæ•°æ®åˆ†å‰²
        self._split_all_datasets()  

        # æŒ‰æœåŠ¡åˆ†ç»„å®¢æˆ·ç«¯
        '''
        ç»“æœå¯èƒ½æ˜¯
        self.service_clients = {
            1: [1, 2, 3],  # æœåŠ¡1å¯¹åº”å®¢æˆ·ç«¯1,2,3
            2: [4, 5, 6],  # æœåŠ¡2å¯¹åº”å®¢æˆ·ç«¯4,5,6
        }
        '''
        self.service_clients: Dict[int, List[int]] = {}
        for client_id, service_id in service_assignments.items():
            if service_id not in self.service_clients:
                self.service_clients[service_id] = []
            self.service_clients[service_id].append(client_id)
    
    def fl_train_set(self, **kwargs) -> List[Dict[str, Any]]:
        """è¿”å›æ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒæ•°æ®ã€‚"""
        return self._create_data_batches(self.train_datasets, shuffle=True)
    
    def fl_eval_set(self, **kwargs):
        """
        è¿”å›è¯„ä¼°æ•°æ®ã€‚
        
        è¯„ä¼°æ•°æ®ç”¨äºï¼š
        1. è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ¨¡å‹æ€§èƒ½
        2. è¶…å‚æ•°è°ƒä¼˜
        3. æ—©åœç­–ç•¥
        4. æ£€æµ‹è¿‡æ‹Ÿåˆ
        
        ç‰¹ç‚¹ï¼š
        - ä¸å‚ä¸è®­ç»ƒï¼Œåªç”¨äºè¯„ä¼°
        - ä¸æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆç¡®ä¿è¯„ä¼°ç»“æœå¯é‡ç°ï¼‰
        - ä¸ä¸¢å¼ƒæœ€åä¸€æ‰¹æ•°æ®ï¼ˆç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«è¯„ä¼°ï¼‰
        """
        return self._create_data_batches(self.eval_datasets, shuffle=False)
    
    def fl_test_set(self, **kwargs) -> List[Dict[str, Any]]:
        """
        è¿”å›æµ‹è¯•æ•°æ®ã€‚
        
        æµ‹è¯•æ•°æ®ç”¨äºï¼š
        1. æœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼°
        2. ä¸å…¶ä»–æ–¹æ³•æ¯”è¾ƒ
        3. å‘å¸ƒæ¨¡å‹å‰çš„æœ€ç»ˆéªŒè¯
        
        ç‰¹ç‚¹ï¼š
        - åªåœ¨è®­ç»ƒå®Œå…¨ç»“æŸåä½¿ç”¨
        - ä¸æ‰“ä¹±æ•°æ®é¡ºåº
        - ä¸ä¸¢å¼ƒä»»ä½•æ•°æ®
        - ä»£è¡¨æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„è¡¨ç°
        """
        return self._create_data_batches(self.test_datasets, shuffle=False)

    def _split_dataset(self, dataset: torch.utils.data.Dataset, client_id: int) -> Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset, torch.utils.data.Dataset]:
        """
        å°†å•ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ/è¯„ä¼°/æµ‹è¯•ä¸‰éƒ¨åˆ†ã€‚
        
        å‚æ•°:
            dataset: è¦åˆ†å‰²çš„æ•°æ®é›†
            client_id: å®¢æˆ·ç«¯IDï¼Œç”¨äºç”Ÿæˆç¡®å®šæ€§çš„éšæœºç§å­
            
        è¿”å›:
            (train_dataset, eval_dataset, test_dataset)
        """        
        dataset_size = len(dataset)
        # è®¡ç®—å„éƒ¨åˆ†å¤§å°
        train_size = int(self.train_ratio * dataset_size)
        eval_size = int(self.eval_ratio * dataset_size)
        test_size = dataset_size - train_size - eval_size  # å‰©ä½™çš„éƒ½ç»™æµ‹è¯•é›†

        # ä½¿ç”¨ç¡®å®šæ€§éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡åˆ†å‰²ç»“æœä¸€è‡´
        generator = torch.Generator().manual_seed(self.random_seed + client_id)
        
        return torch.utils.data.random_split(
            dataset, 
            [train_size, eval_size, test_size],
            generator=generator
        )

    def _split_all_datasets(self):
        """ä¸ºæ‰€æœ‰å®¢æˆ·ç«¯åˆ†å‰²æ•°æ®é›†ã€‚"""
        for client_id, dataset in self.datasets.items():
            # å¦‚æœæ²¡æœ‰æä¾›ç‹¬ç«‹çš„è¯„ä¼°/æµ‹è¯•æ•°æ®é›†ï¼Œåˆ™è¿›è¡Œåˆ†å‰²
            if client_id not in self.eval_datasets or client_id not in self.test_datasets:
                train_ds, eval_ds, test_ds = self._split_dataset(dataset, client_id)
                # åªä¿å­˜è®­ç»ƒæ•°æ®é›†ï¼ˆåŸå§‹æ•°æ®é›†ç°åœ¨åªç”¨äºåˆ†å‰²ï¼‰
                self.train_datasets[client_id] = train_ds
                
                # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æä¾›è¯„ä¼°æ•°æ®é›†ï¼Œä½¿ç”¨åˆ†å‰²çš„ç»“æœ
                if client_id not in self.eval_datasets:
                    self.eval_datasets[client_id] = eval_ds
                    
                # å¦‚æœæ²¡æœ‰ç‹¬ç«‹æä¾›æµ‹è¯•æ•°æ®é›†ï¼Œä½¿ç”¨åˆ†å‰²çš„ç»“æœ
                if client_id not in self.test_datasets:
                    self.test_datasets[client_id] = test_ds
            else:
                # å¦‚æœæä¾›äº†ç‹¬ç«‹çš„è¯„ä¼°å’Œæµ‹è¯•æ•°æ®é›†ï¼Œè®­ç»ƒé›†å°±æ˜¯åŸå§‹æ•°æ®é›†
                self.train_datasets[client_id] = dataset

    def _create_data_batches(self, datasets: Dict[int, torch.utils.data.Dataset], shuffle: bool = True) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºæ•°æ®æ‰¹æ¬¡çš„é€šç”¨æ–¹æ³•ã€‚
        
        å‚æ•°:
            datasets: å®¢æˆ·ç«¯æ•°æ®é›†å­—å…¸
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            
        è¿”å›:
            æ ¼å¼åŒ–çš„æ•°æ®æ‰¹æ¬¡åˆ—è¡¨
        """
        data = []
        for client_id, dataset in datasets.items():
            # ä¸ºæ­¤å®¢æˆ·ç«¯åˆ›å»ºæ•°æ®åŠ è½½å™¨
            client_loader = torch.utils.data.DataLoader(
                dataset, 
                batch_size=self.batch_size, 
                shuffle=shuffle,
                drop_last=self.drop_last if shuffle else False  # è¯„ä¼°/æµ‹è¯•æ—¶ä¸ä¸¢å¼ƒæ•°æ®
            )
            # å°†PyTorchçš„æ ‡å‡†æ•°æ®æ ¼å¼è½¬æ¢ä¸ºFLSimæ¡†æ¶è¦æ±‚çš„å­—å…¸æ ¼å¼
            '''
                batch = (
                    torch.tensor([[1.0, 2.0], [3.0, 4.0]]),  # features: [batch_size, feature_dim]
                    torch.tensor([0, 1])                      # labels: [batch_size]
                )
                è½¬æ¢å
                batch_dict = {
                    'features': torch.tensor([[1.0, 2.0], [3.0, 4.0]]),
                    'labels': torch.tensor([0, 1])
                }
            '''
            batches = []
            for batch in client_loader:
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    features, labels = batch
                    batches.append({
                        'features': features,
                        'labels': labels
                    })
            
            data.append({
                'client_id': client_id,
                'service_id': self.service_assignments[client_id],
                'batches': batches
            })
        
        return data

    def get_dataset_info(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®é›†åˆ†å‰²ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•å’ŒéªŒè¯ã€‚
        
        è¿”å›:
            åŒ…å«å„å®¢æˆ·ç«¯æ•°æ®é›†å¤§å°ä¿¡æ¯çš„å­—å…¸
        """
        info = {
            'total_clients': len(self.datasets),
            'data_split_ratios': {
                'train': self.train_ratio,
                'eval': self.eval_ratio, 
                'test': self.test_ratio
            },
            'clients_info': {}
        }
        for client_id in self.datasets.keys():
            original_size = len(self.datasets[client_id])
            train_size = len(self.train_datasets[client_id]) if client_id in self.train_datasets else 0
            eval_size = len(self.eval_datasets[client_id]) if client_id in self.eval_datasets else 0
            test_size = len(self.test_datasets[client_id]) if client_id in self.test_datasets else 0
            
            info['clients_info'][client_id] = {
                'original_size': original_size,
                'train_size': train_size,
                'eval_size': eval_size,
                'test_size': test_size,
                'service_id': self.service_assignments[client_id]
            }
    
        return info

class QuantizedFLModel(IFLModel):
    """
    èåˆé‡åŒ–åŠŸèƒ½çš„è”é‚¦å­¦ä¹ æ¨¡å‹åŒ…è£…å™¨ã€‚
    åœ¨FLSimçš„FLModelåŸºç¡€ä¸Šæ‰©å±•é‡åŒ–èƒ½åŠ›ã€‚
    """
    
    def __init__(self, 
                 model: nn.Module, 
                 device: Optional[str] = None,
                 quantization_level: int = 8):
        """
        åˆå§‹åŒ–é‡åŒ–è”é‚¦å­¦ä¹ æ¨¡å‹ã€‚
        
        å‚æ•°:
            model: PyTorchæ¨¡å‹
            device: è¿è¡Œè®¾å¤‡
            quantization_level: æ¨¡å‹å‚æ•°çš„é‡åŒ–çº§åˆ«
        """
        self.model = model
        self.device = device
        self.quantization_level = quantization_level
        self.quantizer = QuantizationModule()
        
        # å­˜å‚¨åŸå§‹å‚æ•°ç”¨äºæ¯”è¾ƒ
        self.original_params = None
        self.quantized_params = None
        self.communication_volume = 0
    
    def fl_forward(self, batch):
        """ä½¿ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€‚"""
        features = batch['features']
        labels = batch['labels']
        
        if self.device:
            features = features.to(self.device)
            labels = labels.to(self.device)
        
        output = self.model(features)
        loss = torch.nn.functional.cross_entropy(output, labels.long())
        
        # è¿”å›FLSimå…¼å®¹çš„æ‰¹æ¬¡æŒ‡æ ‡
        if FLSIM_AVAILABLE:
            try:
                from flsim.utils.simple_batch_metrics import FLBatchMetrics
                return FLBatchMetrics(
                    loss=loss,
                    num_examples=labels.size(0),
                    predictions=output.detach().cpu(),
                    targets=labels.detach().cpu(),
                    model_inputs=features
                )
            except ImportError:
                pass
        
        return SimpleBatchMetrics(
            loss=loss,
            num_examples=labels.size(0),
            predictions=output.detach().cpu(),
            targets=labels.detach().cpu()
        )
    
    def fl_create_training_batch(self, **kwargs):
        """åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡ã€‚"""
        return kwargs
    
    def fl_get_module(self) -> nn.Module:
        """è·å–åº•å±‚çš„PyTorchæ¨¡å—ã€‚"""
        return self.model
    
    def fl_cuda(self):
        """å°†æ¨¡å‹ç§»åŠ¨åˆ°CUDAè®¾å¤‡ã€‚"""
        if self.device:
            self.model = self.model.to(self.device)
    
    def get_eval_metrics(self, batch):
        """è·å–è¯„ä¼°æŒ‡æ ‡ã€‚"""
        with torch.no_grad():
            return self.fl_forward(batch)
    
    def get_num_examples(self, batch) -> int:
        """è·å–æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°é‡ã€‚"""
        return batch['labels'].size(0)
    
    def quantize_parameters(self) -> Tuple[Dict[str, torch.Tensor], int]:
        """
        é‡åŒ–æ¨¡å‹å‚æ•°å¹¶è¿”å›é€šä¿¡é‡ã€‚
        
        è¿”å›:
            å…ƒç»„ (é‡åŒ–åçš„çŠ¶æ€å­—å…¸, é€šä¿¡é‡)
        """
        state_dict = self.model.state_dict()
        quantized_state_dict = {}
        total_volume = 0
        
        # é‡åŒ–å‚æ•°ï¼Œé™ä½é€šä¿¡å¼€é”€
        for name, param in state_dict.items():
            quantized_param, norm_value = self.quantizer.quantize_parameters(
                param, self.quantization_level
            )
            quantized_state_dict[name] = quantized_param
            
            # è®¡ç®—æ­¤å‚æ•°çš„é€šä¿¡é‡
            param_volume = self.quantizer.calculate_communication_volume(
                param.numel(), self.quantization_level
            )
            total_volume += param_volume
        
        # é‡åŒ–åçš„å‚æ•°å’Œé€šä¿¡é‡
        self.communication_volume = total_volume
        self.quantized_params = quantized_state_dict
        
        return quantized_state_dict, total_volume


class MultiServiceFLSystem:
    """
    å¤šæœåŠ¡æä¾›å•†è”é‚¦å­¦ä¹ çš„ä¸»ç³»ç»Ÿç±»ã€‚
    é›†æˆäº†é‡åŒ–ã€é€šä¿¡å»ºæ¨¡å’Œ FLSim è®­ç»ƒã€‚
    """
    
    def __init__(self,
                 service_configs: List[ServiceProviderConfig],
                 client_configs: Dict[int, ClientResourceConfig],
                 base_config_path: Optional[str] = None,
                 debug_disable_quant: bool = False,
                 debug_verbose: bool = False):
        """åˆå§‹åŒ–å¤šæœåŠ¡è”é‚¦å­¦ä¹ ç³»ç»Ÿã€‚"""
        # ä¿å­˜é…ç½®
        self.service_configs: Dict[int, ServiceProviderConfig] = {cfg.service_id: cfg for cfg in service_configs}
        self.client_configs: Dict[int, ClientResourceConfig] = client_configs

        # å­ç³»ç»Ÿå®¹å™¨
        self.system_metrics = SystemMetrics()
        self.service_models: Dict[int, QuantizedFLModel] = {}
        self.service_trainers: Dict[int, SyncTrainer] = {}
        self.service_data_providers: Dict[int, IFLDataProvider] = {}
        self.service_data_loaders: Dict[int, MultiServiceFLDataLoader] = {}

        # æ¯è½®ç”±RLè¦†ç›–æ³¨å…¥çš„æœåŠ¡åŠ¨ä½œå‚æ•°ï¼ˆä¸æ”¹å˜å®¢æˆ·ç«¯é€‰æ‹©é€»è¾‘ï¼‰
        # ç»“æ„: { service_id: { 'n_clients': int, 'cpu_frequency': float, 'bandwidth': float, 'quantization_level': int } }
        self.service_action_overrides: Dict[int, Dict[str, Any]] = {}

        # åŸºç¡€é…ç½®ä¸è°ƒè¯•æ ‡å¿—
        self.base_config = self._load_base_config(base_config_path)
        self.debug_disable_quant = debug_disable_quant
        self.debug_verbose = debug_verbose

        print(f"åˆå§‹åŒ–å¤šæœåŠ¡è”é‚¦å­¦ä¹ ç³»ç»Ÿï¼ŒåŒ…å« {len(service_configs)} ä¸ªæœåŠ¡ (debug_disable_quant={debug_disable_quant}, debug_verbose={debug_verbose})")

    def _ensure_scalar_quant_channel(self, trainer, n_bits: int) -> None:
        """ç¡®ä¿è®­ç»ƒå™¨ä¸æœåŠ¡å™¨ä½¿ç”¨æ ‡é‡é‡åŒ–é€šé“ï¼Œå¹¶è®¾ç½®ä½å®½ï¼Œç”¨äºä¸Šä¼ å‰é‡åŒ–ã€‚"""
        try:
            from flsim.channels.scalar_quantization_channel import ScalarQuantizationChannel
        except Exception:
            return

        # å°† qLevel(çº§æ•°) æ˜ å°„ä¸ºä½æ•°ï¼šbits = ceil(log2(q)), å¹¶è£å‰ªåˆ° [1, 8]
        n_bits = int(max(1, min(8, n_bits)))

        # è‹¥å·²æ˜¯æ ‡é‡é‡åŒ–é€šé“ï¼Œåˆ™æ›´æ–°ä½å®½åŠç›¸å…³é‡åŒ–å™¨
        if hasattr(trainer, 'channel') and isinstance(trainer.channel, ScalarQuantizationChannel):
            ch = trainer.channel
            # æ›´æ–° cfg ä¸å†…éƒ¨é‡åŒ–è¾¹ç•Œ/è§‚å¯Ÿå™¨
            try:
                ch.cfg.n_bits = n_bits
            except Exception:
                pass
            ch.quant_min = -(2 ** (n_bits - 1))
            ch.quant_max = (2 ** (n_bits - 1)) - 1
            ch.observer, ch.quantizer = ch.get_observers_and_quantizers()
            # ç¡®ä¿æœåŠ¡å™¨ä¹Ÿä½¿ç”¨åŒä¸€é€šé“å®ä¾‹
            if hasattr(trainer, 'server') and hasattr(trainer.server, '_channel'):
                trainer.server._channel = ch
            return

        # è‹¥å½“å‰ä¸ºå…¶å®ƒé€šé“ï¼Œåˆ™åˆ›å»ºæ–°çš„é‡åŒ–é€šé“å¹¶æ›¿æ¢
        try:
            new_channel = ScalarQuantizationChannel(n_bits=n_bits)
            # è®­ç»ƒå™¨ä¸æœåŠ¡å™¨åŒæ—¶æ›¿æ¢ä¸ºåŒä¸€å®ä¾‹ï¼Œä¿è¯ä¸€è‡´
            if hasattr(trainer, 'channel'):
                trainer.channel = new_channel
            if hasattr(trainer, 'server') and hasattr(trainer.server, '_channel'):
                trainer.server._channel = new_channel
        except Exception:
            # å¿½ç•¥å¤±è´¥ï¼Œä¿æŒåŸé€šé“
            pass

    def set_service_action(self,
                           service_id: int,
                           n_clients: Optional[int] = None,
                           cpu_frequency: Optional[float] = None,
                           bandwidth: Optional[float] = None,
                           quantization_level: Optional[int] = None) -> None:
        """ç”±RLåœ¨æ¯æ­¥è°ƒç”¨ï¼Œæ³¨å…¥æœ¬è½®æœåŠ¡çš„åŠ¨ä½œå‚æ•°ä»¥å½±å“èƒ½è€—/æ—¶å»¶/é€šä¿¡é‡è®¡ç®—ã€‚
        æ³¨æ„ï¼šä¸æ”¹å˜FLSimå†…éƒ¨å®¢æˆ·ç«¯é€‰æ‹©ï¼Œä»…ç”¨äºç³»ç»ŸæŒ‡æ ‡ä¸é€šä¿¡å»ºæ¨¡ã€‚
        """
        ov = self.service_action_overrides.get(service_id, {})
        if n_clients is not None:
            ov['n_clients'] = int(max(1, n_clients))
        if cpu_frequency is not None:
            ov['cpu_frequency'] = float(max(1e3, cpu_frequency))
        if bandwidth is not None:
            ov['bandwidth'] = float(max(0.0, bandwidth))
        if quantization_level is not None:
            ov['quantization_level'] = int(max(1, quantization_level))
        self.service_action_overrides[service_id] = ov
        
    def _deep_merge(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        result = copy.deepcopy(base) if base else {}
        for k, v in (override or {}).items():
            if isinstance(v, dict) and isinstance(result.get(k), dict):
                result[k] = self._deep_merge(result[k], v)
            else:
                result[k] = v
        return result

    def _get_service_overrides(self, service_id: int) -> Dict[str, Any]:
        services = self.base_config.get("services", {}) or {}
        return services.get(str(service_id), {}) or {}

    def _load_base_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½åŸºç¡€ FLSim é…ç½®ï¼Œä»…ä»JSONæ–‡ä»¶è¯»å–ã€‚"""
        candidates: List[Path] = []
        if config_path:
            candidates.append(Path(config_path))
        else:
            # ä¼˜å…ˆä½¿ç”¨ä¸å½“å‰æ–‡ä»¶åŒçº§çš„ configs
            try:
                here = Path(__file__).resolve().parent
                candidates.append(here / "configs" / "flsim_base_config.json")
            except Exception:
                pass
            # å·¥ä½œç›®å½•ä¸‹çš„ My_FL/configs
            candidates.append(Path.cwd() / "My_FL" / "configs" / "flsim_base_config.json")
            # å·¥ä½œç›®å½•ä¸‹çš„ configs
            candidates.append(Path.cwd() / "configs" / "flsim_base_config.json")
        
        config_file = None
        for p in candidates:
            if p and p.exists():
                config_file = p
                break
        if config_file is None:
            tried = "\n".join(str(p) for p in candidates)
            raise FileNotFoundError(
                "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨äºä»¥ä¸‹ä»»ä¸€è·¯å¾„:\n" + tried
            )
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            return config
        except json.JSONDecodeError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            raise Exception(f"é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    def setup_service(self, 
                 service_id: int,
                 model: nn.Module,
                 train_datasets: Dict[int, torch.utils.data.Dataset],
                 eval_datasets: Optional[Dict[int, torch.utils.data.Dataset]] = None,
                 test_datasets: Optional[Dict[int, torch.utils.data.Dataset]] = None):

        """
        è®¾ç½®å…·ä½“æœåŠ¡åŠå…¶æ¨¡å‹å’Œæ•°æ®ã€‚
        
        å‚æ•°:
            service_id: æœåŠ¡æ ‡è¯†ç¬¦
            model: è¯¥æœåŠ¡çš„PyTorchæ¨¡å‹
            train_datasets: æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒæ•°æ®é›†
            eval_datasets: å¯é€‰çš„è¯„ä¼°æ•°æ®é›†
            test_datasets: å¯é€‰çš„æµ‹è¯•æ•°æ®é›†
        """
        if service_id not in self.service_configs:
            raise ValueError(f"æœåŠ¡ {service_id} æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°")

        config = self.service_configs[service_id]
        
        # åˆ›å»ºé‡åŒ–FLæ¨¡å‹
        device = "cuda" if torch.cuda.is_available() else "cpu"
        fl_model = QuantizedFLModel(
            model=model,
            device=device,
            quantization_level=config.quantization_level
        )

        if torch.cuda.is_available():
            fl_model.fl_cuda()

        self.service_models[service_id] = fl_model

        # åˆ›å»ºæœåŠ¡åˆ†é…æ˜ å°„
        service_assignments = {client_id: service_id for client_id in config.client_ids}

        # è¿‡æ»¤è¯¥æœåŠ¡çš„å®¢æˆ·ç«¯æ•°æ®é›†
        service_datasets = {cid: train_datasets[cid] for cid in config.client_ids 
                        if cid in train_datasets}

        # âœ… ä½¿ç”¨ JSON ä¸­çš„ DataLoader é…ç½®ï¼ˆå¯è¢« services[service_id].data_loader_overrides è¦†ç›–ï¼‰
        dl_base = self.base_config.get("data_loader", {}) or {}
        dl_over = self._get_service_overrides(service_id).get("data_loader_overrides", {}) or {}
        dl_cfg = self._deep_merge(dl_base, dl_over)
        # âœ… ä½¿ç”¨ MultiServiceFLDataLoader åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = MultiServiceFLDataLoader(
            datasets=service_datasets,
            service_assignments=service_assignments,
            batch_size=dl_cfg.get("batch_size", 32),
            drop_last=dl_cfg.get("drop_last", False),
            eval_datasets=eval_datasets,
            test_datasets=test_datasets,
            train_ratio=dl_cfg.get("train_ratio", 0.8),
            eval_ratio=dl_cfg.get("eval_ratio", 0.1),
            test_ratio=dl_cfg.get("test_ratio", 0.1)
        )


        # è·å–è®­ç»ƒæ•°æ®
        train_data = data_loader.fl_train_set()

        # å°è¯•ä½¿ç”¨FLSim
        if not FLSIM_AVAILABLE:
            raise RuntimeError("FLSim ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºè®­ç»ƒå™¨ã€‚è¯·æ£€æŸ¥å®‰è£…ã€‚")

        # å°† MultiServiceFLDataLoader è½¬æ¢ä¸º FLSim å…¼å®¹çš„æ•°æ®æä¾›å™¨ï¼ˆä¸¥æ ¼å¤±è´¥ï¼‰
        data_provider = self._create_flsim_data_provider_from_loader(data_loader, config)
        self.service_data_providers[service_id] = data_provider

        # âœ… è®­ç»ƒå™¨é…ç½®å®Œå…¨ç”± JSONï¼ˆå« per-service è¦†ç›–ï¼‰å†³å®š
        trainer_config = self._create_trainer_config(config)
        try:
            # å°†å¯èƒ½å­˜åœ¨çš„åµŒå¥— trainer_config æ‰å¹³åŒ–ä¸º Hydra æœŸæœ›çš„é¡¶å±‚å­—æ®µ
            def _flatten_trainer_cfg(cfg: dict) -> dict:
                if not isinstance(cfg, dict):
                    return cfg
                if "trainer_config" in cfg and isinstance(cfg["trainer_config"], dict):
                    inner = copy.deepcopy(cfg["trainer_config"])
                    # ä½¿ç”¨å¤–å±‚çš„ç›®æ ‡ä½œä¸ºæœ€ç»ˆç›®æ ‡ï¼ˆSyncTrainerï¼‰
                    inner["_target_"] = cfg.get("_target_", inner.get("_target_"))
                    return inner
                return cfg

            hydra_payload = _flatten_trainer_cfg(trainer_config)

            # åŸºæœ¬æ ¡éªŒï¼šå¿…é¡»åŒ…å« _target_ï¼ˆSyncTrainerï¼‰
            if not isinstance(hydra_payload, dict) or "_target_" not in hydra_payload:
                raise ValueError("trainer é…ç½®ç¼ºå°‘ _target_")

            trainer = instantiate(
                hydra_payload,
                model=fl_model,
                cuda_enabled=torch.cuda.is_available()
            )
        except Exception as e:
            try:
                cfg_str = json.dumps(hydra_payload if 'hydra_payload' in locals() else trainer_config, ensure_ascii=False, indent=2)
            except Exception:
                cfg_str = str(hydra_payload if 'hydra_payload' in locals() else trainer_config)
            raise RuntimeError(f"å®ä¾‹åŒ– FLSim Trainer å¤±è´¥: {e}\né…ç½®=\n{cfg_str}") from e

        if trainer is None:
            raise RuntimeError("FLSim Trainer å®ä¾‹ä¸º Noneï¼Œè¯·æ£€æŸ¥é…ç½®ä¸ä¾èµ–ã€‚")

        print(f"âœ… ä½¿ç”¨FLSimè®­ç»ƒå™¨è®¾ç½®æœåŠ¡ {service_id}")

        self.service_trainers[service_id] = trainer
        self.service_data_loaders[service_id] = data_loader  # ä¿å­˜æ•°æ®åŠ è½½å™¨å¼•ç”¨
        print(f"Set up service {service_id} with {len(service_datasets)} clients")
    


    def _create_flsim_data_provider_from_loader(self, 
                                            data_loader: MultiServiceFLDataLoader,
                                            config: ServiceProviderConfig) -> IFLDataProvider:
        """ä» MultiServiceFLDataLoader åˆ›å»º FLSim å…¼å®¹çš„æ•°æ®æä¾›å™¨ã€‚"""
        
        # è·å–è®­ç»ƒæ•°æ®
        train_data = data_loader.fl_train_set()
        eval_data = data_loader.fl_eval_set()
        test_data = data_loader.fl_test_set()
        
        # åˆ›å»º FLSim å…¼å®¹çš„æ•°æ®æä¾›å™¨
        class MultiServiceDataProvider(IFLDataProvider):
            def __init__(self, train_data, eval_data, test_data):
                # ä¿ç•™åŸå§‹åŸå­æ•°æ®ï¼ˆå« client_idã€service_idã€batchesï¼‰
                self._raw_train_data = train_data
                self._raw_eval_data = eval_data
                self._raw_test_data = test_data

                # é¢„æ„å»º IFLUserData å¯¹è±¡åˆ—è¡¨
                self._train_users = [self._create_fl_user_data(d["batches"], split="train") for d in train_data]
                self._eval_users = [self._create_fl_user_data(d["batches"], split="eval") for d in eval_data]
                self._test_users = [self._create_fl_user_data(d["batches"], split="eval") for d in test_data]  # æµ‹è¯•åŒæ ·èµ° eval_data()

                # å¯é€‰ï¼šä¿ç•™ç”¨æˆ·IDï¼ˆclient_idï¼‰ï¼Œä¾›éœ€è¦æ—¶ä½¿ç”¨
                self._train_user_ids = [d["client_id"] for d in train_data]
                self._eval_user_ids = [d["client_id"] for d in eval_data]
                self._test_user_ids = [d["client_id"] for d in test_data]
                
            def train_users(self):
                return self._train_users

            def eval_users(self):
                return self._eval_users

            def test_users(self):
                return self._test_users

            def train_user_ids(self):
                # è®­ç»ƒé˜¶æ®µè‹¥éœ€è¦â€œç´¢å¼•â€ï¼Œè¿”å› [0..n-1]
                return list(range(len(self._train_users)))

            def num_train_users(self):
                return len(self._train_users)

            def num_eval_users(self):
                return len(self._eval_users)

            def num_test_users(self):
                return len(self._test_users)


            def get_train_user(self, user_id: int):
                if 0 <= user_id < len(self._train_users):
                    return self._train_users[user_id]
                return self._create_empty_user_data()

            def get_eval_user(self, user_id: int):
                print(f"ğŸ” è·å–è¯„ä¼°ç”¨æˆ·æ•°æ®: user_id={user_id}")
                if 0 <= user_id < len(self._eval_users):
                    return self._eval_users[user_id]
                print(f"âŒ è¯„ä¼°ç”¨æˆ· {user_id} ä¸å­˜åœ¨ï¼Œæœ‰æ•ˆèŒƒå›´: 0-{len(self._eval_users)-1}")
                return self._create_empty_user_data()

            def get_test_user(self, user_id: int):
                print(f"ğŸ” è·å–æµ‹è¯•ç”¨æˆ·æ•°æ®: user_id={user_id}")
                if 0 <= user_id < len(self._test_users):
                    return self._test_users[user_id]
                print(f"âŒ æµ‹è¯•ç”¨æˆ· {user_id} ä¸å­˜åœ¨ï¼Œæœ‰æ•ˆèŒƒå›´: 0-{len(self._test_users)-1}")
                return self._create_empty_user_data()

            def _create_empty_user_data(self):
                empty_batch = {
                    "features": torch.zeros(1, 10),
                    "labels": torch.zeros(1, dtype=torch.long),
                }
                # ç©ºç”¨æˆ·èµ° eval splitï¼Œé¿å…å‚ä¸è®­ç»ƒ
                return self._create_fl_user_data([empty_batch], split="eval")

            def _create_fl_user_data(self, batches, split: str = "train"):
                """å°†æ‰¹æ¬¡æ•°æ®è½¬æ¢ä¸ºFLSimç”¨æˆ·æ•°æ®æ ¼å¼; split in {'train','eval'}"""
                if not batches:
                    print("âš ï¸  è­¦å‘Š: æ‰¹æ¬¡æ•°æ®ä¸ºç©ºï¼Œå°†æ³¨å…¥è™šæ‹Ÿæ‰¹æ¬¡")
                    batches = [{"features": torch.zeros(1, 10), "labels": torch.zeros(1, dtype=torch.long)}]

                from flsim.data.data_provider import IFLUserData

                class SimpleUserData(IFLUserData):
                    def __init__(self, batches, split):
                        self._split = split
                        if split == "train":
                            self._train_data = batches
                            self._eval_data = []
                        else:
                            self._train_data = []
                            self._eval_data = batches

                    def __iter__(self):
                        # é»˜è®¤è¿”å›è®­ç»ƒæ•°æ®çš„è¿­ä»£
                        for b in self._train_data:
                            yield b

                    def __len__(self):
                        return len(self._train_data) if self._split == "train" else len(self._eval_data)

                    def train_data(self):
                        for b in self._train_data:
                            yield b

                    def eval_data(self):
                        for b in self._eval_data:
                            yield b

                    def num_train_batches(self) -> int:
                        return len(self._train_data)

                    def num_eval_batches(self) -> int:
                        return len(self._eval_data)

                    def num_train_examples(self) -> int:
                        total = 0
                        for batch in self._train_data:
                            total += len(batch["labels"]) if isinstance(batch, dict) and "labels" in batch else len(batch)
                        return total

                    def num_eval_examples(self) -> int:
                        total = 0
                        for batch in self._eval_data:
                            total += len(batch["labels"]) if isinstance(batch, dict) and "labels" in batch else len(batch)
                        return total

                return SimpleUserData(batches, split) 
            
        return MultiServiceDataProvider(train_data, eval_data, test_data)
        

    def _create_trainer_config(self, service_config: ServiceProviderConfig):
        """ä» JSON è¯»å– Trainer é…ç½®ï¼Œå¹¶åº”ç”¨ per-service è¦†ç›–ï¼›æŒ‰è®ºæ–‡å‚æ•°è®¾ç½®è®­ç»ƒé…ç½®ã€‚"""
        base_trainer = copy.deepcopy(self.base_config.get("trainer", {}) or {})
        service_overrides = self._get_service_overrides(service_config.service_id).get("trainer_overrides", {}) or {}

        # æ·±åº¦åˆå¹¶ï¼šå…¨å±€ trainer + å½“å‰æœåŠ¡è¦†ç›–
        trainer_config = self._deep_merge(base_trainer, service_overrides)

        # æŒ‰è®ºæ–‡è¦æ±‚è®¾ç½®è®­ç»ƒå‚æ•°
        if isinstance(trainer_config, dict):
            inner = trainer_config.get("trainer_config")
            if isinstance(inner, dict):
                # è®¾ç½®è½®æ•°ä¸º35ï¼ˆè®ºæ–‡T=35ï¼‰
                inner["epochs"] = 35
                
                # è®¾ç½®æ¯è½®ç”¨æˆ·æ•°ä¸ºè¯¥æœåŠ¡çš„å®¢æˆ·ç«¯æ€»æ•°
                if inner.get("users_per_round") is None:
                    inner["users_per_round"] = len(service_config.client_ids)
                
                # è®¾ç½®æœ¬åœ°è®­ç»ƒè½®æ•°ä¸º3ï¼ˆè®ºæ–‡Ï„=3ï¼‰
                if inner.get("client", {}).get("epochs") is None:
                    if "client" not in inner:
                        inner["client"] = {}
                    inner["client"]["epochs"] = 3
                
                # è®¾ç½®å­¦ä¹ ç‡ä¸º0.001ï¼ˆè®ºæ–‡Î·_k=0.001ï¼‰
                if inner.get("client", {}).get("optimizer", {}).get("lr") is None:
                    if "client" not in inner:
                        inner["client"] = {}
                    if "optimizer" not in inner["client"]:
                        inner["client"]["optimizer"] = {}
                    inner["client"]["optimizer"]["lr"] = 0.001
                    
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆé¡¶å±‚ç›´æ¥è®¾ç½®å‚æ•°ï¼‰
                if trainer_config.get("users_per_round") is None:
                    trainer_config["users_per_round"] = len(service_config.client_ids)
                trainer_config["epochs"] = 35

        return trainer_config
    
    def train_service(self, 
                     service_id: int, 
                     num_rounds: int = 10,
                     enable_metrics: bool = True) -> Tuple[QuantizedFLModel, Dict[str, Any]]:
        """
        ä½¿ç”¨è”é‚¦å­¦ä¹ è®­ç»ƒæŒ‡å®šçš„æœåŠ¡ã€‚
        
        å‚æ•°:
            service_id: è¦è®­ç»ƒçš„æœåŠ¡ID
            num_rounds: è®­ç»ƒè½®æ•°
            enable_metrics: æ˜¯å¦è®¡ç®—é€šä¿¡æŒ‡æ ‡
            
        è¿”å›:
            (è®­ç»ƒåçš„æ¨¡å‹, æŒ‡æ ‡æ‘˜è¦) çš„å…ƒç»„
        """
        if service_id not in self.service_trainers:
            raise ValueError(f"æœåŠ¡ {service_id} å°šæœªè®¾ç½®")
        
        trainer = self.service_trainers[service_id]
        base_data_provider = self.service_data_providers[service_id]
        model = self.service_models[service_id]

        # è¯»å–å½“å‰è½®RLåŠ¨ä½œè¦†ç›–ï¼ˆè‹¥æœ‰ï¼‰
        action_ov = self.service_action_overrides.get(service_id, {})

        # 1) ä¸Šä¼ å‰é‡åŒ–ï¼šä¸å†è¿›è¡Œâ€œå…¥è®­é‡åŒ–â€æƒé‡æ³¨å…¥ï¼›æ”¹ä¸ºåœ¨å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒå®Œæˆã€ä¸Šä¼ å‰é€šè¿‡é€šé“è¿›è¡Œé‡åŒ–ã€‚
        #    è¿™é‡Œä»…åŒæ­¥ qLevel åˆ°æ¨¡å‹è®°å½•ä¸é€šé“ä½å®½è®¾ç½®ã€‚
        try:
            if 'quantization_level' in action_ov:
                model.quantization_level = int(action_ov['quantization_level'])
        except Exception:
            pass

        # å°† qLevel(çº§æ•°) è½¬ä¸ºé€šé“ä½å®½ï¼Œå¹¶ä¿è¯ä½å®½åœ¨ [1, 8]
        try:
            q_level = int(action_ov.get('quantization_level', getattr(model, 'quantization_level', 8)))
            # bits â‰ˆ ceil(log2(q_level))ï¼Œé¿å… 0
            n_bits = int(max(1, math.ceil(math.log2(max(2, q_level)))))
            self._ensure_scalar_quant_channel(trainer, n_bits)
            if self.debug_verbose:
                print(f"[DEBUG][S{service_id}] é…ç½®ä¸Šä¼ å‰é‡åŒ–ä½å®½: q_level={q_level} -> n_bits={n_bits}")
        except Exception as e:
            if self.debug_verbose:
                print(f"[DEBUG][S{service_id}] é…ç½®ä¸Šä¼ å‰é‡åŒ–å¤±è´¥: {e}")

        # 2) n çš„è£å‰ªæ–¹æ¡ˆï¼šåŸºäºRLè¦†ç›–çš„ n é™åˆ¶æœ¬è½®å®é™…å‚ä¸è®­ç»ƒçš„ç”¨æˆ·æ•°
        #    é€šè¿‡åŒ…è£…ä¸€ä¸ªè£å‰ªåçš„æ•°æ®æä¾›å™¨å®ç°ï¼Œå¯¹FLSimé€æ˜ã€‚
        try:
            base_num_users = base_data_provider.num_train_users()
        except Exception:
            base_num_users = -1
        n_override = int(action_ov.get('n_clients', base_num_users)) if base_num_users > 0 else base_num_users
        if base_num_users > 0:
            n_effective = max(1, min(n_override, base_num_users))
        else:
            n_effective = base_num_users  # ç»´æŒä¸å¯ç”¨çŠ¶æ€

        data_provider = base_data_provider
        if base_num_users > 0 and n_effective != base_num_users:
            from flsim.data.data_provider import IFLDataProvider as _IFLDP

            class _TrimmedDataProvider(_IFLDP):
                def __init__(self, base_dp, take_n: int):
                    self._base = base_dp
                    self._n = int(take_n)

                # è®­ç»ƒç›¸å…³è£å‰ª
                def train_users(self):
                    users = self._base.train_users()
                    try:
                        return users[: self._n]
                    except Exception:
                        return [self._base.get_train_user(i) for i in range(self._n)]

                def train_user_ids(self):
                    return list(range(self._n))

                def num_train_users(self):
                    return self._n

                def get_train_user(self, user_id: int):
                    if 0 <= user_id < self._n:
                        return self._base.get_train_user(user_id)
                    # è¶…ç•Œè¿”å›ä¸€ä¸ªç©ºç”¨æˆ·ï¼Œé¿å…è®­ç»ƒè®¿é—®è¶Šç•Œ
                    return self._create_empty_user_data()

                # è¯„ä¼°/æµ‹è¯•é€ä¼ 
                def eval_users(self):
                    return self._base.eval_users()

                def test_users(self):
                    return self._base.test_users()

                def num_eval_users(self):
                    return self._base.num_eval_users()

                def num_test_users(self):
                    return self._base.num_test_users()

                def get_eval_user(self, user_id: int):
                    return self._base.get_eval_user(user_id)

                def get_test_user(self, user_id: int):
                    return self._base.get_test_user(user_id)

                # ä¸æˆ‘ä»¬å†…éƒ¨æ„é€ çš„æ•°æ®æä¾›å™¨å¯¹é½çš„ç©ºç”¨æˆ·
                def _create_empty_user_data(self):
                    try:
                        from flsim.data.data_provider import IFLUserData as _IFLUser
                    except Exception:
                        _IFLUser = object  # å…œåº•

                    class _EmptyUser(_IFLUser):
                        def __iter__(self):
                            return iter([])

                        def __len__(self):
                            return 0

                        def train_data(self):
                            return iter([])

                        def eval_data(self):
                            return iter([])

                        def num_train_batches(self) -> int:
                            return 0

                        def num_eval_batches(self) -> int:
                            return 0

                        def num_train_examples(self) -> int:
                            return 0

                        def num_eval_examples(self) -> int:
                            return 0

                    return _EmptyUser()

            data_provider = _TrimmedDataProvider(base_data_provider, n_effective)
            print(f"[DEBUG][S{service_id}] æœ¬è½®è®­ç»ƒå°†è£å‰ªå‚ä¸ç”¨æˆ·æ•°: {base_num_users} -> {n_effective}")
        
        try:
            from flsim.interfaces.metrics_reporter import Channel
            from flsim.utils.example_utils import MetricsReporter
            metrics_reporter = MetricsReporter([Channel.STDOUT])
        except ImportError:
            metrics_reporter = None
        
        print(f"\nå¼€å§‹ä¸ºæœåŠ¡ {service_id} è¿›è¡Œè”é‚¦å­¦ä¹ è®­ç»ƒ...")

        # ---------------- è¯Šæ–­é˜¶æ®µ: è®­ç»ƒå‰ç»Ÿè®¡ ----------------
        try:
            num_users = data_provider.num_train_users()
        except Exception:
            num_users = -1
        print(f"[DEBUG][S{service_id}] è®­ç»ƒç”¨æˆ·æ•°: {num_users}")
        # ç»Ÿè®¡å‰5ä¸ªç”¨æˆ·çš„æ‰¹æ¬¡æ•°ä¸æ ·æœ¬æ•°
        try:
            for u in range(min(5, num_users)):
                user = data_provider.get_train_user(u)
                if hasattr(user, 'num_train_batches'):
                    nb = user.num_train_batches()
                else:
                    nb = sum(1 for _ in user)
                if hasattr(user, 'num_train_examples'):
                    ne = user.num_train_examples()
                else:
                    ne = 0
                print(f"  ç”¨æˆ·{u}: batches={nb}, examples={ne}")
        except Exception as e:
            print(f"[DEBUG] è®­ç»ƒç”¨æˆ·ç»Ÿè®¡å¤±è´¥: {e}")

    # å–é¦–ä¸ª batch åšå‰å‘ï¼Œè®°å½•åˆå§‹æŸå¤±/ç²¾åº¦ï¼ˆåœ¨å…¥è®­é‡åŒ–ä¹‹åï¼‰
        def _get_first_batch():
            try:
                for u in range(num_users):
                    user = data_provider.get_train_user(u)
                    if hasattr(user, 'num_train_batches') and user.num_train_batches() == 0:
                        continue
                    for batch in user.train_data():
                        feats = batch.get('features') if isinstance(batch, dict) else None
                        labels = batch.get('labels') if isinstance(batch, dict) else None
                        if feats is not None and labels is not None and len(labels) > 0:
                            return feats, labels
            except Exception:
                return None, None
            return None, None

        device = next(model.model.parameters()).device
        first_x, first_y = _get_first_batch()
        criterion = torch.nn.CrossEntropyLoss()
        if first_x is not None:
            first_x = first_x.to(device)
            first_y = first_y.to(device)
            model.model.eval()
            with torch.no_grad():
                try:
                    logits = model.model(first_x)
                    pre_loss = criterion(logits, first_y).item()
                    pre_acc = (logits.argmax(1) == first_y).float().mean().item()
                    print(f"[DEBUG][S{service_id}] è®­ç»ƒå‰å•æ‰¹: loss={pre_loss:.4f}, acc={pre_acc:.4f}")
                except Exception as e:
                    print(f"[DEBUG] é¢„è®­ç»ƒæ‰¹æ¬¡å‰å‘å¤±è´¥: {e}")
        else:
            print(f"[DEBUG][S{service_id}] æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ‰¹æ¬¡ç”¨äºé¢„æ£€")

        # è®°å½•é¦–å±‚å‚æ•°ç”¨äºå·®å¼‚æ¯”è¾ƒ
        try:
            first_param_before = next(model.model.parameters()).detach().cpu().clone()
        except Exception:
            first_param_before = None

        # å¯é€‰: æš‚æ—¶ç¦ç”¨é‡åŒ–ï¼ˆè°ƒè¯•ï¼‰
        if self.debug_disable_quant:
            if hasattr(model, 'quantization_level'):
                print(f"[DEBUG][S{service_id}] è°ƒè¯•æ¨¡å¼: ç¦ç”¨é‡åŒ–è¿‡ç¨‹ (quantization_level={model.quantization_level})")

        # ---------------- è°ƒç”¨FLSimè®­ç»ƒ ----------------
        final_model, eval_score = trainer.train(
            data_provider=data_provider,
            metrics_reporter=metrics_reporter,
            num_total_users=data_provider.num_train_users() if data_provider else 1,
            distributed_world_size=1
        )

        # ---------------- è®­ç»ƒåè¯Šæ–­ ----------------
        try:
            first_param_after = next(model.model.parameters()).detach().cpu()
            if first_param_before is not None:
                delta = (first_param_after - first_param_before).pow(2).sum().sqrt().item()
                print(f"[DEBUG][S{service_id}] é¦–å±‚å‚æ•°L2å˜åŒ–: {delta:.6f}")
                if delta < 1e-6:
                    print(f"[WARN][S{service_id}] æƒé‡å‡ ä¹æœªå˜åŒ–ï¼Œå¯èƒ½æœªå‘ç”Ÿæœ‰æ•ˆè®­ç»ƒï¼")
        except Exception as e:
            print(f"[DEBUG] è®­ç»ƒåå‚æ•°æ¯”è¾ƒå¤±è´¥: {e}")

        # å†æ¬¡å‰å‘åŒä¸€æ‰¹æ¬¡
        if first_x is not None:
            model.model.eval()
            with torch.no_grad():
                try:
                    logits2 = model.model(first_x)
                    post_loss = criterion(logits2, first_y).item()
                    post_acc = (logits2.argmax(1) == first_y).float().mean().item()
                    print(f"[DEBUG][S{service_id}] è®­ç»ƒåå•æ‰¹: loss={post_loss:.4f}, acc={post_acc:.4f}")
                except Exception as e:
                    print(f"[DEBUG] è®­ç»ƒåå‰å‘å¤±è´¥: {e}")

        # å¿«é€Ÿæµ‹è¯•é›†è¯„ä¼°ï¼ˆé™åˆ¶å‰20æ‰¹ï¼‰
        try:
            dl = self.service_data_loaders.get(service_id)
            test_set_batches = dl.fl_test_set() if dl else []
            test_examples = 0
            test_correct = 0
            taken_batches = 0
            model.model.eval()
            with torch.no_grad():
                for entry in test_set_batches:
                    for batch in entry.get('batches', [])[:5]:  # æ¯ä¸ªå®¢æˆ·ç«¯æœ€å¤šå–5æ‰¹
                        feats = batch.get('features'); labels = batch.get('labels')
                        if feats is None or labels is None:
                            continue
                        feats = feats.to(device); labels = labels.to(device)
                        try:
                            out = model.model(feats)
                        except Exception as e:
                            print(f"[DEBUG] æµ‹è¯•æ‰¹å‰å‘å¤±è´¥: {e}")
                            continue
                        preds = out.argmax(1)
                        test_correct += (preds == labels).sum().item()
                        test_examples += labels.size(0)
                        taken_batches += 1
                        if taken_batches >= 20:
                            break
                    if taken_batches >= 20:
                        break
            if test_examples > 0:
                print(f"[DEBUG][S{service_id}] å¿«é€Ÿæµ‹è¯•å‡†ç¡®ç‡: {test_correct/test_examples:.4f} ({test_correct}/{test_examples}) åœ¨ {taken_batches} æ‰¹æ¬¡")
            else:
                print(f"[DEBUG][S{service_id}] æ— æµ‹è¯•æ ·æœ¬ç”¨äºå¿«é€Ÿè¯„ä¼°")
        except Exception as e:
            print(f"[DEBUG] å¿«é€Ÿæµ‹è¯•è¯„ä¼°å¤±è´¥: {e}")
        
        # è®¡ç®—é€šä¿¡å’Œèƒ½é‡æ¶ˆè€—
        metrics_summary = {}
        if enable_metrics:
            metrics_summary = self._calculate_service_metrics(service_id, num_rounds)

        print(f"å®ŒæˆæœåŠ¡ {service_id} çš„è®­ç»ƒ")

        return final_model, metrics_summary
    
    def _calculate_service_metrics(self, service_id: int, num_rounds: int) -> Dict[str, Any]:
        """è®¡ç®—æœåŠ¡çš„ç»¼åˆæŒ‡æ ‡ã€‚"""
        config = self.service_configs[service_id]
        model = self.service_models[service_id]

        # è¯»å–å½“å‰è½®çš„åŠ¨ä½œè¦†ç›–å‚æ•°
        action_ov = self.service_action_overrides.get(service_id, {})

        # è‹¥RLè¦†ç›–äº†é‡åŒ–çº§åˆ«ï¼Œåˆ™åº”ç”¨åˆ°æ¨¡å‹å†è®¡ç®—é€šä¿¡é‡
        if 'quantization_level' in action_ov and not self.debug_disable_quant:
            try:
                model.quantization_level = int(action_ov['quantization_level'])
            except Exception:
                pass

        # è®¡ç®—é‡åŒ–æŒ‡æ ‡ï¼ˆé€šä¿¡é‡éšqå˜åŒ–ï¼‰
        _, comm_volume = model.quantize_parameters()

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æŒ‡æ ‡
        service_metrics = []
        # è¦†ç›–çš„å®¢æˆ·ç«¯æ•°ç”¨äºå¸¦å®½åœ¨å®¢æˆ·ç«¯é—´çš„ç­‰åˆ†ï¼ˆä¸æ”¹å˜é€‰æ‹©é›†åˆï¼‰
        n_clients_override = int(action_ov.get('n_clients', len(config.client_ids)))
        # æœåŠ¡å¸¦å®½æŒ‰é€‰ç”¨å®¢æˆ·ç«¯æ•°ç­‰åˆ†ç»™å•ä¸ªå®¢æˆ·ç«¯ï¼ˆFDMAç®€åŒ–ï¼‰
        service_bandwidth = float(action_ov.get('bandwidth', 1e6))  # é»˜è®¤1MHz
        per_client_bandwidth = max(service_bandwidth / max(n_clients_override, 1), 1e3)  # è‡³å°‘1kHzé¿å…é™¤é›¶
        for client_id in config.client_ids:
            if client_id in self.client_configs:
                client_config = self.client_configs[client_id]
                
                # è®¡ç®—å®¢æˆ·ç«¯æŒ‡æ ‡
                client_metrics = self.system_metrics.calculate_client_metrics(
                    client_id=client_id,
                    service_id=service_id,
                    mu_i=client_config.mu_i,
                    c_ir=client_config.c_ir,
                    dataset_size=client_config.dataset_size,
                    # ä½¿ç”¨RLè¦†ç›–çš„CPUé¢‘ç‡ï¼Œå¦åˆ™é€€å›åˆ°å®¢æˆ·ç«¯ä¸Šé™
                    cpu_frequency=float(action_ov.get('cpu_frequency', client_config.max_frequency)),
                    communication_volume=comm_volume,
                    # ä½¿ç”¨RLè¦†ç›–çš„å¸¦å®½ï¼ˆæŒ‰å®¢æˆ·ç«¯å¹³å‡ï¼‰
                    bandwidth=per_client_bandwidth,
                    channel_gain=client_config.channel_gain,
                    transmit_power=client_config.max_power
                )
                
                self.system_metrics.add_client_metrics(client_metrics)
                service_metrics.append(client_metrics)
        
        # å¾—åˆ°æœåŠ¡ç­‰çº§çš„æ€»ç»“
        summary = self.system_metrics.get_service_metrics_summary(service_id)
        
        # æ·»åŠ é‡åŒ–ä¿¡æ¯
        total_params = sum(p.numel() for p in model.model.parameters())
        q_for_ratio = getattr(model, 'quantization_level', config.quantization_level)
        compression_ratio = (
            model.quantizer.get_compression_ratio(total_params, q_for_ratio)
            if hasattr(model, "quantizer") and model.quantizer is not None
            else 0
        )
        
        summary.update({
            # æŠ¥å‘Šæ¨¡å‹å½“å‰é‡åŒ–çº§åˆ«ï¼ˆå¯èƒ½è¢«RLè¦†ç›–ï¼‰
            'quantization_level': getattr(model, 'quantization_level', config.quantization_level),
            'communication_volume_per_round': comm_volume,
            'total_parameters': total_params,
            'compression_ratio': compression_ratio,
            'total_communication_volume': comm_volume * num_rounds
        })

        # å°†å¿«é€Ÿæµ‹è¯•ç»“æœå¹¶å…¥æŒ‡æ ‡ï¼ˆè‹¥å·²è®¡ç®—ï¼‰
        if 'test_correct' in locals():
            summary.update({
                'quick_test_accuracy': test_correct / test_examples if 'test_examples' in locals() and test_examples > 0 else None,
                'quick_test_examples': test_examples if 'test_examples' in locals() else None
            })
        
        return summary
    
    def train_all_services(self, 
                          num_rounds: int = 10,
                          parallel: bool = False) -> Dict[int, Tuple[QuantizedFLModel, Dict[str, Any]]]:
        """
        è®­ç»ƒæ‰€æœ‰æœåŠ¡ã€‚
        
        å‚æ•°:
            num_rounds: è®­ç»ƒè½®æ•°
            parallel: æ˜¯å¦å¹¶è¡Œè®­ç»ƒæœåŠ¡ï¼ˆæœªå®ç°ï¼‰
            
        è¿”å›:
            å°†service_idæ˜ å°„åˆ°(æ¨¡å‹, æŒ‡æ ‡)çš„å­—å…¸
        """
        results = {}
        
        for service_id in self.service_configs.keys():
            if service_id in self.service_trainers:
                print(f"\n{'='*50}")
                print(f"è®­ç»ƒæœåŠ¡ {service_id}")
                print(f"{'='*50}")
                
                model, metrics = self.train_service(service_id, num_rounds)
                results[service_id] = (model, metrics)
        
        return results
    
    def get_system_summary(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç³»ç»Ÿæ‘˜è¦ã€‚"""
        summary = {
            'num_services': len(self.service_configs),
            'num_clients': len(self.client_configs),
            'service_summaries': {}
        }
        
        for service_id in self.service_configs.keys():
            service_summary = self.system_metrics.get_service_metrics_summary(service_id)
            summary['service_summaries'][service_id] = service_summary
        
        return summary


def create_sample_system() -> MultiServiceFLSystem:
    """åˆ›å»ºç”¨äºæµ‹è¯•çš„ç¤ºä¾‹å¤šæœåŠ¡è”é‚¦å­¦ä¹ ç³»ç»Ÿã€‚"""
    
    # å®šä¹‰æœåŠ¡é…ç½®
    service_configs = [
        ServiceProviderConfig(
            service_id=1,
            name="å›¾åƒåˆ†ç±»æœåŠ¡", 
            client_ids=[1, 2, 3],
            model_architecture={"type": "cnn", "num_classes": 10},
            quantization_level=8,
            users_per_round=2
        ),
        ServiceProviderConfig(
            service_id=2,
            name="æ–‡æœ¬åˆ†ç±»æœåŠ¡",
            client_ids=[4, 5, 6],
            model_architecture={"type": "mlp", "num_classes": 5},
            quantization_level=4,
            users_per_round=2
        )
    ]
    
    # å®šä¹‰å®¢æˆ·ç«¯èµ„æºé…ç½®
    client_configs = {
        1: ClientResourceConfig(1, 1e-28, 1000, 1e9, 0.1, 1e-3, 1000),
        2: ClientResourceConfig(2, 1.2e-28, 1200, 1.2e9, 0.12, 1.2e-3, 1200),
        3: ClientResourceConfig(3, 0.8e-28, 800, 0.8e9, 0.08, 0.8e-3, 800),
        4: ClientResourceConfig(4, 1.1e-28, 1100, 1.1e9, 0.11, 1.1e-3, 1100),
        5: ClientResourceConfig(5, 0.9e-28, 900, 0.9e9, 0.09, 0.9e-3, 900),
        6: ClientResourceConfig(6, 1.3e-28, 1300, 1.3e9, 0.13, 1.3e-3, 1300),
    }
    
    return MultiServiceFLSystem(service_configs, client_configs)


if __name__ == "__main__":
    print("å¤šæœåŠ¡è”é‚¦å­¦ä¹ ç³»ç»Ÿ")
    print("åˆ›å»ºç¤ºä¾‹ç³»ç»Ÿ...")
    
    system = create_sample_system()
    print("ç¤ºä¾‹ç³»ç»Ÿåˆ›å»ºæˆåŠŸï¼")
    
    # æ‰“å°ç³»ç»Ÿé…ç½®
    summary = system.get_system_summary()
    print(f"ç³»ç»Ÿæ‘˜è¦: {json.dumps(summary, indent=2, ensure_ascii=False)}")