#!/usr/bin/env python3
"""
PAC-MCoFL: å¸•ç´¯æ‰˜æ¼”å‘˜-è¯„è®ºå®¶å¤šæœåŠ¡æä¾›å•†åä½œè”é‚¦å­¦ä¹ 

å®ç°ç¬¬4èŠ‚ä¸­æè¿°çš„PAC-MCoFLç®—æ³•æ¡†æ¶:
- è”åˆç­–ç•¥å’Œç´¯ç§¯æœŸæœ›å¥–åŠ± (æ–¹ç¨‹ 19-20)
- å¸•ç´¯æ‰˜æœ€ä¼˜å‡è¡¡æœºåˆ¶ (æ–¹ç¨‹ 21-23)
- åŸºäºç­–ç•¥æ¢¯åº¦çš„æ¼”å‘˜-è¯„è®ºå®¶ç½‘ç»œæ›´æ–° (æ–¹ç¨‹ 24-27)
- å®Œæ•´çš„PAC-MCoFLè®­ç»ƒç®—æ³• (ç®—æ³• 1)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
from collections import deque, defaultdict
import copy
import random

try:
    from .mdp_framework import MultiServiceFLEnvironment, Action, Observation
    from .optimization_problem import OptimizationConstraints
    from .multi_service_fl import MultiServiceFLSystem
    from .action_space_transform import ActionSpaceTransformer, ActionGranularity
except ImportError:
    # å¯¹äºç›´æ¥æ‰§è¡Œï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    from mdp_framework import MultiServiceFLEnvironment, Action, Observation
    from optimization_problem import OptimizationConstraints
    from multi_service_fl import MultiServiceFLSystem
    from action_space_transform import ActionSpaceTransformer, ActionGranularity


@dataclass
class PACConfig:
    """PAC-MCoFLç®—æ³•çš„é…ç½®ï¼ŒæŒ‰ç…§è®ºæ–‡ç²¾ç¡®å‚æ•°è®¾ç½®."""
    
    # è®­ç»ƒå‚æ•°
    num_episodes: int = 5        # è°ƒè¯•é˜¶æ®µå‡å°‘å›åˆæ•°ï¼ŒåŠ é€ŸRL-FLè”åŠ¨éªŒè¯
    max_rounds_per_episode: int = 35  # ç®—æ³•1ä¸­çš„Tï¼Œè®ºæ–‡è®¾ç½®ä¸º35
    buffer_size: int = 10000        # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°H
    batch_size: int = 64            # è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œè®ºæ–‡è®¾ç½®ä¸º64
    
    # ç½‘ç»œæ¶æ„ï¼ˆæŒ‰ç…§è®ºæ–‡è¦æ±‚ï¼‰
    actor_hidden_dim: int = 64      # ç­–ç•¥ç½‘ç»œï¼š64-128-64æ¶æ„
    critic_hidden_dim: int = 64     # Qç½‘ç»œï¼š64-128æ¶æ„
    num_layers: int = 3             # ç­–ç•¥ç½‘ç»œå±‚æ•°ï¼ˆ64-128-64ï¼‰
    critic_layers: int = 2          # Qç½‘ç»œå±‚æ•°ï¼ˆ64-128ï¼‰ï¼Œè®ºæ–‡æ˜ç¡®è¯´"åŒå±‚"
    
    # å­¦ä¹ å‚æ•°ï¼ˆæŒ‰ç…§è®ºæ–‡å‚æ•°è¡¨ï¼‰
    actor_lr: float = 0.001         # æ–¹ç¨‹(27)ä¸­çš„Î¶=0.001
    critic_lr: float = 0.001        # æ–¹ç¨‹(26)ä¸­çš„Î±=0.001
    gamma: float = 0.95             # æŠ˜æ‰£å› å­Î³
    tau: float = 0.005              # è½¯æ›´æ–°å‚æ•°
    
    # PACç‰¹å®šå‚æ•°
    joint_action_samples: int = 100  # ç”¨äºè¿‘ä¼¼è”åˆç­–ç•¥çš„æ ·æœ¬æ•°
    baseline_regularization: float = 0.01  # åŸºçº¿æ­£åˆ™åŒ–æƒé‡
    
    # è®­ç»ƒè°ƒåº¦
    update_frequency: int = 4       # æ¯Næ­¥æ›´æ–°ç½‘ç»œ
    eval_frequency: int = 100       # è¯„ä¼°é¢‘ç‡
    save_frequency: int = 500       # æ¨¡å‹ä¿å­˜é¢‘ç‡


class ActorNetwork(nn.Module):
    """
    PAC-MCoFLçš„æ¼”å‘˜ç½‘ç»œï¼ŒæŒ‰ç…§è®ºæ–‡è¦æ±‚å®ç°ç­–ç•¥ç½‘ç»œæ¶æ„.
    
    è®ºæ–‡æ¶æ„ï¼šä¸‰å±‚å…¨è¿æ¥ç½‘ç»œï¼ˆ64ã€128ã€64ä¸ªç¥ç»å…ƒï¼‰
    å®ç°Ï€_r(a_{r,t} | o_{r,t}; Ï†_r)æ¥ç”ŸæˆåŠ¨ä½œ.
    """
    
    def __init__(self, 
                 observation_dim: int,
                 action_dim: int,
                 hidden_dim: int = 64,
                 num_layers: int = 3,
                 action_bounds: Optional[Tuple[np.ndarray, np.ndarray]] = None):
        """
        åˆå§‹åŒ–æ¼”å‘˜ç½‘ç»œï¼ŒæŒ‰ç…§è®ºæ–‡ç­–ç•¥ç½‘ç»œæ¶æ„.
        
        å‚æ•°:
            observation_dim: è§‚å¯Ÿç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            hidden_dim: ç¬¬ä¸€å±‚éšè—å±‚ç»´åº¦ï¼ˆ64ï¼‰
            num_layers: ç½‘ç»œå±‚æ•°ï¼ˆ3å±‚ï¼‰
            action_bounds: åŠ¨ä½œç¼©æ”¾çš„(æœ€å°è¾¹ç•Œ, æœ€å¤§è¾¹ç•Œ)
        """
        super().__init__()
        
        self.action_bounds = action_bounds
        
        # æŒ‰ç…§è®ºæ–‡è¦æ±‚æ„å»ºç­–ç•¥ç½‘ç»œï¼š64-128-64æ¶æ„
        if num_layers == 3:
            # è®ºæ–‡æŒ‡å®šçš„ä¸‰å±‚æ¶æ„
            self.network = nn.Sequential(
                nn.Linear(observation_dim, 64),    # ç¬¬ä¸€å±‚ï¼š64ä¸ªç¥ç»å…ƒ
                nn.ReLU(),
                nn.Linear(64, 128),                # ç¬¬äºŒå±‚ï¼š128ä¸ªç¥ç»å…ƒ
                nn.ReLU(),
                nn.Linear(128, 64),                # ç¬¬ä¸‰å±‚ï¼š64ä¸ªç¥ç»å…ƒ
                nn.ReLU(),
                nn.Linear(64, action_dim),         # è¾“å‡ºå±‚
                nn.Tanh()  # è¾“å‡ºåœ¨[-1, 1]èŒƒå›´å†…
            )
        else:
            # å…¼å®¹å…¶ä»–é…ç½®çš„é€šç”¨æ¶æ„
            layers = []
            input_dim = observation_dim
            
            for _ in range(num_layers):
                layers.append(nn.Linear(input_dim, hidden_dim))
                layers.append(nn.ReLU())
                input_dim = hidden_dim
            
            layers.append(nn.Linear(hidden_dim, action_dim))
            layers.append(nn.Tanh())  # è¾“å‡ºåœ¨[-1, 1]èŒƒå›´å†…
            
            self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡."""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
    
    def forward(self, observation: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ç”ŸæˆåŠ¨ä½œ.
        
        å‚æ•°:
            observation: è¾“å…¥è§‚å¯Ÿ
            
        è¿”å›:
            ç”Ÿæˆçš„åŠ¨ä½œ
        """
        action = self.network(observation)
        
        # å¦‚æœæä¾›äº†åŠ¨ä½œè¾¹ç•Œï¼Œåˆ™ç¼©æ”¾åˆ°åŠ¨ä½œè¾¹ç•Œ
        if self.action_bounds is not None:
            min_bounds, max_bounds = self.action_bounds
            min_bounds = torch.tensor(min_bounds, dtype=torch.float32, device=action.device)
            max_bounds = torch.tensor(max_bounds, dtype=torch.float32, device=action.device)
            
            # ä»[-1, 1]ç¼©æ”¾åˆ°[min_bounds, max_bounds]
            action = min_bounds + 0.5 * (action + 1) * (max_bounds - min_bounds)
        
        return action
    
    def get_action_log_prob(self, observation: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        è·å–ç»™å®šè§‚å¯Ÿä¸‹åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡.
        
        è¿™ç”¨äºæ–¹ç¨‹(25)ä¸­çš„ç­–ç•¥æ¢¯åº¦è®¡ç®—.
        """
        # å¯¹äºè¿ç»­åŠ¨ä½œï¼Œæˆ‘ä»¬å‡è®¾é«˜æ–¯ç­–ç•¥
        # å®é™…ä¸­ï¼Œæ‚¨å¯èƒ½æƒ³è¦ä½¿ç”¨æ›´å¤æ‚çš„åˆ†å¸ƒ
        
        mean_action = self.forward(observation)
        
        # ä¸ºç®€å•èµ·è§ï¼Œå‡è®¾å›ºå®šæ ‡å‡†å·®
        std = torch.ones_like(mean_action) * 0.1
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        log_prob = -0.5 * torch.sum(((action - mean_action) / std) ** 2 + 2 * torch.log(std), dim=-1)
        
        return log_prob
    
    def sample_action(self, observation: torch.Tensor, add_noise: bool = True) -> torch.Tensor:
        """
        ä»ç­–ç•¥ä¸­é‡‡æ ·åŠ¨ä½œ.
        
        å‚æ•°:
            observation: è¾“å…¥è§‚å¯Ÿ
            add_noise: æ˜¯å¦æ·»åŠ æ¢ç´¢å™ªå£°
            
        è¿”å›:
            é‡‡æ ·çš„åŠ¨ä½œ
        """
        mean_action = self.forward(observation)
        
        if add_noise:
            # æ·»åŠ é«˜æ–¯å™ªå£°è¿›è¡Œæ¢ç´¢
            noise = torch.randn_like(mean_action) * 0.1
            action = mean_action + noise
        else:
            action = mean_action
        
        # è£å‰ªåˆ°åŠ¨ä½œè¾¹ç•Œ
        if self.action_bounds is not None:
            min_bounds, max_bounds = self.action_bounds
            min_bounds = torch.tensor(min_bounds, dtype=torch.float32, device=action.device)
            max_bounds = torch.tensor(max_bounds, dtype=torch.float32, device=action.device)
            action = torch.clamp(action, min_bounds, max_bounds)
        
        return action


class CriticNetwork(nn.Module):
    """
    PAC-MCoFLçš„è¯„è®ºå®¶ç½‘ç»œï¼ŒæŒ‰ç…§è®ºæ–‡è¦æ±‚å®ç°Qç½‘ç»œæ¶æ„.
    
    è®ºæ–‡æ¶æ„ï¼šåŒå±‚å…¨è¿æ¥ç»“æ„ï¼ˆ64å’Œ128ä¸ªç¥ç»å…ƒï¼‰
    å®ç°Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t})è¿›è¡Œä»·å€¼ä¼°è®¡.
    """
    
    def __init__(self,
                 observation_dim: int,
                 own_action_dim: int,
                 joint_action_dim: int,
                 hidden_dim: int = 64,
                 num_layers: int = 2):
        """
        åˆå§‹åŒ–è¯„è®ºå®¶ç½‘ç»œï¼ŒæŒ‰ç…§è®ºæ–‡Qç½‘ç»œæ¶æ„.
        
        å‚æ•°:
            observation_dim: è§‚å¯Ÿç©ºé—´ç»´åº¦
            own_action_dim: è‡ªèº«åŠ¨ä½œç»´åº¦
            joint_action_dim: è”åˆåŠ¨ä½œç»´åº¦(æ‰€æœ‰æ™ºèƒ½ä½“)
            hidden_dim: ç¬¬ä¸€å±‚éšè—å±‚ç»´åº¦ï¼ˆ64ï¼‰
            num_layers: ç½‘ç»œå±‚æ•°ï¼ˆ2å±‚ï¼‰
        """
        super().__init__()
        
        input_dim = observation_dim + own_action_dim + joint_action_dim
        
        # æŒ‰ç…§è®ºæ–‡è¦æ±‚æ„å»ºQç½‘ç»œï¼šåŒå±‚å…¨è¿æ¥ï¼ˆ64å’Œ128ä¸ªç¥ç»å…ƒï¼‰
        if num_layers == 2:
            # è®ºæ–‡æŒ‡å®šçš„åŒå±‚æ¶æ„ï¼šç¬¬ä¸€å±‚64ä¸ªç¥ç»å…ƒï¼Œç¬¬äºŒå±‚128ä¸ªç¥ç»å…ƒï¼Œæœ€åç›´æ¥è¾“å‡º
            self.network = nn.Sequential(
                nn.Linear(input_dim, 64),          # ç¬¬ä¸€å±‚ï¼š64ä¸ªç¥ç»å…ƒ
                nn.ReLU(),
                nn.Linear(64, 128),                # ç¬¬äºŒå±‚ï¼š128ä¸ªç¥ç»å…ƒ
                nn.ReLU(),
                nn.Linear(128, 1)                  # è¾“å‡ºå±‚ï¼šå•ä¸ªQå€¼
            )
        else:
            # å…¼å®¹å…¶ä»–é…ç½®çš„é€šç”¨æ¶æ„
            layers = []
            current_dim = input_dim
            
            for _ in range(num_layers):
                layers.append(nn.Linear(current_dim, hidden_dim))
                layers.append(nn.ReLU())
                current_dim = hidden_dim
            
            layers.append(nn.Linear(hidden_dim, 1))  # è¾“å‡ºå•ä¸ªQå€¼
            
            self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡."""
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.0)
    
    def forward(self, 
                observation: torch.Tensor,
                own_action: torch.Tensor,
                joint_action: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ä¼°è®¡Qå€¼.
        
        å‚æ•°:
            observation: æ™ºèƒ½ä½“çš„è§‚å¯Ÿ
            own_action: æ™ºèƒ½ä½“è‡ªèº«çš„åŠ¨ä½œ
            joint_action: æ‰€æœ‰æ™ºèƒ½ä½“çš„è”åˆåŠ¨ä½œ
            
        è¿”å›:
            Qå€¼ä¼°è®¡
        """
        # è¿æ¥è¾“å…¥
        x = torch.cat([observation, own_action, joint_action], dim=-1)
        
        return self.network(x)


class ReplayBuffer:
    """PAC-MCoFLçš„ç»éªŒå›æ”¾ç¼“å†²åŒº."""
    
    def __init__(self, capacity: int, observation_dim: int, action_dim: int, num_agents: int):
        """
        åˆå§‹åŒ–å›æ”¾ç¼“å†²åŒº.
        
        å‚æ•°:
            capacity: æœ€å¤§ç¼“å†²åŒºå¤§å°
            observation_dim: è§‚å¯Ÿç»´åº¦
            action_dim: æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç»´åº¦
            num_agents: æ™ºèƒ½ä½“æ•°é‡
        """
        self.capacity = capacity
        self.observation_dim = observation_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        
        # ç¼“å†²åŒºå­˜å‚¨
        self.observations = np.zeros((capacity, observation_dim))
        self.own_actions = np.zeros((capacity, action_dim))
        self.joint_actions = np.zeros((capacity, (num_agents - 1) * action_dim))  # æ’é™¤è‡ªèº«åŠ¨ä½œ
        self.rewards = np.zeros((capacity,))
        self.next_observations = np.zeros((capacity, observation_dim))
        self.dones = np.zeros((capacity,), dtype=bool)
        
        self.ptr = 0
        self.size = 0
    
    def add(self, 
            observation: np.ndarray,
            own_action: np.ndarray,
            joint_action: np.ndarray,
            reward: float,
            next_observation: np.ndarray,
            done: bool):
        """å‘ç¼“å†²åŒºæ·»åŠ ç»éªŒ."""
        
        self.observations[self.ptr] = observation
        self.own_actions[self.ptr] = own_action
        self.joint_actions[self.ptr] = joint_action
        self.rewards[self.ptr] = reward
        self.next_observations[self.ptr] = next_observation
        self.dones[self.ptr] = done
        
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)
    
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:
        """ä»ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡."""
        
        indices = np.random.choice(self.size, batch_size, replace=False)
        
        return (
            torch.FloatTensor(self.observations[indices]),
            torch.FloatTensor(self.own_actions[indices]),
            torch.FloatTensor(self.joint_actions[indices]),
            torch.FloatTensor(self.rewards[indices]),
            torch.FloatTensor(self.next_observations[indices]),
            torch.BoolTensor(self.dones[indices])
        )
    
    def __len__(self):
        return self.size


class PACAgent:
    """
    å®ç°å¸•ç´¯æ‰˜æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•çš„PAC-MCoFLæ™ºèƒ½ä½“.
    
    ä»æœåŠ¡æä¾›å•†çš„è§’åº¦å®ç°ç®—æ³•.
    """
    
    def __init__(self,
                 agent_id: int,
                 observation_dim: int,
                 action_dim: int,
                 num_agents: int,
                 config: PACConfig,
                 constraints: OptimizationConstraints):
        """
        åˆå§‹åŒ–PACæ™ºèƒ½ä½“.
        
        å‚æ•°:
            agent_id: æ™ºèƒ½ä½“æ ‡è¯†ç¬¦
            observation_dim: è§‚å¯Ÿç©ºé—´ç»´åº¦
            action_dim: åŠ¨ä½œç©ºé—´ç»´åº¦
            num_agents: æ™ºèƒ½ä½“æ€»æ•°
            config: PACé…ç½®
            constraints: åŠ¨ä½œçº¦æŸ
        """
        self.agent_id = agent_id
        self.observation_dim = observation_dim
        self.action_dim = action_dim
        self.num_agents = num_agents
        self.config = config
        self.constraints = constraints
        # ç»Ÿä¸€ä¸‰å…ƒåŠ¨ä½œç©ºé—´ï¼šå¼•å…¥å˜æ¢å™¨ï¼ˆå•ä¸€å®ç°æ¥æºï¼‰
        self.action_transformer = ActionSpaceTransformer(self.constraints, ActionGranularity())
        
        # è®¾ç½®åŠ¨ä½œè¾¹ç•Œ
        self.action_bounds = (
            np.array([constraints.min_clients, constraints.min_frequency, 
                     constraints.min_bandwidth, constraints.min_quantization]),
            np.array([constraints.max_clients, constraints.max_frequency,
                     constraints.max_bandwidth, constraints.max_quantization])
        )
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self.actor = ActorNetwork(
            observation_dim, action_dim, 
            config.actor_hidden_dim, config.num_layers, self.action_bounds
        )
        
        self.critic = CriticNetwork(
            observation_dim, action_dim, (num_agents - 1) * action_dim,  # æ’é™¤è‡ªèº«åŠ¨ä½œ
            config.critic_hidden_dim, config.critic_layers  # ä½¿ç”¨è®ºæ–‡æŒ‡å®šçš„åŒå±‚æ¶æ„
        )
        
        # ç”¨äºç¨³å®šè®­ç»ƒçš„ç›®æ ‡ç½‘ç»œ
        self.actor_target = copy.deepcopy(self.actor)
        self.critic_target = copy.deepcopy(self.critic)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.critic_lr)
        
        # å›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(
            config.buffer_size, observation_dim, action_dim, num_agents
        )
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.training_step = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.actor_losses = []
        self.critic_losses = []
        self.q_values = []
    
    def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:
        """
        ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ.
        
        å®ç°ä»Ï€_r(a_{r,t} | o_{r,t})é‡‡æ ·.
        """
        observation_tensor = torch.FloatTensor(observation).unsqueeze(0)
        
        with torch.no_grad():
            action = self.actor.sample_action(observation_tensor, add_noise=training)
        
        return action.squeeze(0).numpy()
    
    # (å·²åˆ é™¤é‡å¤çš„ compute_baseline_value æ—§ç‰ˆæœ¬ï¼Œä¿ç•™åæ–¹æ”¹è¿›ç‰ˆæœ¬)
    
    def compute_virtual_joint_policy(self, 
                                   observation: torch.Tensor,
                                   own_action: torch.Tensor,
                                   current_action_state: torch.Tensor = None) -> torch.Tensor:
        """
        å®ç°æ–¹ç¨‹(23)çš„è™šæ‹Ÿè”åˆç­–ç•¥è®¡ç®—: Ï€_{-r}^â€  âˆˆ arg max_{a_{-r,t}} Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t})
        
        ç®€åŒ–çš„ä¸‰å…ƒåŠ¨ä½œç©ºé—´å®ç°ï¼Œç›´æ¥åœ¨tensorå±‚é¢æ“ä½œä»¥æé«˜æ•ˆç‡ã€‚
        
        å‚æ•°:
            observation: å½“å‰è§‚å¯Ÿ o_{r,t} [batch_size, obs_dim]
            own_action: è‡ªèº«åŠ¨ä½œ a_{r,t} [batch_size, action_dim]
            current_action_state: å½“å‰åŠ¨ä½œçŠ¶æ€ï¼Œç”¨äºä¸‰å…ƒå˜æ¢ [batch_size, action_dim]
            
        è¿”å›:
            æ ¹æ®Qå‡½æ•°å¾—åˆ°çš„æœ€ä½³è”åˆåŠ¨ä½œï¼ˆè™šæ‹Ÿè”åˆç­–ç•¥ï¼‰[batch_size, joint_action_dim]
        """
        batch_size = observation.shape[0]
        other_agents_num = self.num_agents - 1
        joint_action_dim = other_agents_num * self.action_dim
        
        # å¦‚æœæ²¡æœ‰æä¾›å½“å‰çŠ¶æ€ï¼Œä½¿ç”¨çº¦æŸä¸­ç‚¹ä½œä¸ºé»˜è®¤å€¼ï¼ˆç‰©ç†å•ä½ï¼‰
        if current_action_state is None:
            default = np.array([
                (self.constraints.min_clients + self.constraints.max_clients) // 2,
                (self.constraints.min_frequency + self.constraints.max_frequency) / 2.0,
                (self.constraints.min_bandwidth + self.constraints.max_bandwidth) / 2.0,
                (self.constraints.min_quantization + self.constraints.max_quantization) // 2
            ], dtype=np.float32)
            current_action_state = torch.tensor(default).unsqueeze(0).repeat(batch_size, 1)

        num_samples = 50
        best_q_values = torch.full((batch_size,), float('-inf'))
        best_joint_actions = torch.zeros(batch_size, joint_action_dim)

        for _ in range(num_samples):
            joint_action_batch = []
            for b in range(batch_size):
                parts = []
                cur_np = current_action_state[b].cpu().numpy()
                for _ in range(other_agents_num):
                    idx = np.random.randint(0, self.action_transformer.get_action_space_size())
                    ternary = self.action_transformer.get_action_by_index(idx)
                    next_action = self.action_transformer.apply_ternary_action(cur_np, ternary)
                    parts.extend(next_action.tolist())
                joint_action_batch.append(parts)

            joint_action_tensor = torch.FloatTensor(joint_action_batch)
            with torch.no_grad():
                q_values = self.critic_target(observation, own_action, joint_action_tensor).squeeze()
                if q_values.dim() == 0:
                    q_values = q_values.unsqueeze(0)

            better_mask = q_values > best_q_values
            best_q_values[better_mask] = q_values[better_mask]
            best_joint_actions[better_mask] = joint_action_tensor[better_mask]

        return best_joint_actions
    
    def compute_baseline_value(self, observation: torch.Tensor, own_action: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—åŸºçº¿å€¼ä»¥å‡å°‘æ–¹å·®ï¼Œå¦‚è®ºæ–‡æ–¹ç¨‹(25)åæ‰€è¿°:
        Î£_{a_{r,t}} Ï€_r(a_{r,t}|o_{r,t}; Ï†_r) Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t})
        
        å‚æ•°:
            observation: å½“å‰è§‚å¯Ÿ
            own_action: å½“å‰åŠ¨ä½œ
            
        è¿”å›:
            åŸºçº¿å€¼ç”¨äºå‡å°‘æ¢¯åº¦ä¼°è®¡æ–¹å·®
        """
        batch_size = observation.shape[0]
        
        # ä»ç­–ç•¥ç½‘ç»œç”Ÿæˆå¤šä¸ªåŠ¨ä½œæ ·æœ¬
        num_action_samples = 10
        action_samples = []
        
        for _ in range(num_action_samples):
            # é‡æ–°å‚æ•°åŒ–æŠ€å·§é‡‡æ ·åŠ¨ä½œ
            sampled_actions = self.actor(observation)
            action_samples.append(sampled_actions)
        
        action_samples = torch.stack(action_samples, dim=1)  # [batch, num_samples, action_dim]
        
        # ä¸ºæ¯ä¸ªåŠ¨ä½œæ ·æœ¬ç”Ÿæˆè”åˆåŠ¨ä½œ
        other_action_dim = (self.num_agents - 1) * self.action_dim
        joint_actions = torch.randn(batch_size, num_action_samples, other_action_dim)
        
        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œæ ·æœ¬çš„Qå€¼
        obs_expanded = observation.unsqueeze(1).repeat(1, num_action_samples, 1)
        
        # é‡å¡‘ç”¨äºæ‰¹å¤„ç†
        obs_flat = obs_expanded.reshape(-1, self.observation_dim)
        action_flat = action_samples.reshape(-1, self.action_dim)
        joint_flat = joint_actions.reshape(-1, other_action_dim)
        
        with torch.no_grad():
            q_values = self.critic_target(obs_flat, action_flat, joint_flat)
            q_values = q_values.reshape(batch_size, num_action_samples)
            
        # è®¡ç®—åŠ æƒå¹³å‡ä½œä¸ºåŸºçº¿å€¼
        baseline = torch.mean(q_values, dim=1, keepdim=True)
        
        return baseline
    
    def update_critic(self, batch: Tuple[torch.Tensor, ...]) -> float:
        """
        ä½¿ç”¨æ–¹ç¨‹(24)çš„TDè¯¯å·®æ›´æ–°è¯„è®ºå®¶ç½‘ç»œï¼Œä¸¥æ ¼æŒ‰ç…§è®ºæ–‡å…¬å¼å®ç°ã€‚
        
        å®ç°: M_r(Ï€) = E[(rwd_{r,t} + Î³ max_{a_{-r,t}} Q_r^Ï€â€ (o_{r,t+1}, a_{r,t+1}, a_{-r,t}) - Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t}))Â²]
        å¯¹åº”æ–¹ç¨‹(26): Î¸_r = Î¸_r - Î± âˆ‡_{Î¸_r} M_r(Ï€)
        """
        observations, own_actions, joint_actions, rewards, next_observations, dones = batch
        
        batch_size = observations.shape[0]
        
        # å½“å‰Qå€¼: Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t})
        current_q = self.critic(observations, own_actions, joint_actions)
        
        # è®¡ç®—ç›®æ ‡Qå€¼ï¼Œå®ç°æ–¹ç¨‹(24)çš„maxæ“ä½œç¬¦
        with torch.no_grad():
            # ä»ç›®æ ‡ç­–ç•¥ç½‘ç»œè·å–ä¸‹ä¸€ä¸ªåŠ¨ä½œ a_{r,t+1}
            next_own_actions = self.actor_target(next_observations)
            
            # è·å–å½“å‰åŠ¨ä½œçŠ¶æ€ï¼ˆç”¨äºä¸‰å…ƒå˜æ¢ï¼‰
            current_action_states = own_actions
            
            # è®¡ç®—è™šæ‹Ÿè”åˆç­–ç•¥ Ï€_{-r}^â€  (æ–¹ç¨‹23)ï¼Œä½¿ç”¨ä¸‰å…ƒåŠ¨ä½œç©ºé—´
            # è¿™æ˜¯æ–¹ç¨‹(24)ä¸­maxæ“ä½œçš„æ ¸å¿ƒï¼Œç°åœ¨ä½¿ç”¨ç¦»æ•£åŠ¨ä½œç©ºé—´
            best_joint_actions = self.compute_virtual_joint_policy(
                next_observations, next_own_actions, current_action_states
            )
            
            # è®¡ç®—ç›®æ ‡Qå€¼: rwd_{r,t} + Î³ max_{a_{-r,t}} Q_r^Ï€â€ (o_{r,t+1}, a_{r,t+1}, a_{-r,t})
            # è¿™é‡Œçš„maxå·²ç»é€šè¿‡compute_virtual_joint_policyå®ç°
            target_q = self.critic_target(next_observations, next_own_actions, best_joint_actions)
            target_q = rewards.unsqueeze(1) + self.config.gamma * target_q * (~dones).unsqueeze(1)
        
        # è®¡ç®—TDè¯¯å·® (æ–¹ç¨‹24): M_r(Ï€) = E[(target - current)Â²]
        td_error = target_q - current_q
        critic_loss = torch.mean(td_error ** 2)  # è¿™å°±æ˜¯æ–¹ç¨‹(24)ä¸­çš„M_r(Ï€)
        
        # æ›´æ–°è¯„è®ºå®¶ç½‘ç»œ (æ–¹ç¨‹26: Î¸_r = Î¸_r - Î± âˆ‡_{Î¸_r} M_r(Ï€))
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        
        # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
        
        self.critic_optimizer.step()
        
        # è®°å½•Qå€¼ç»Ÿè®¡
        self.q_values.append(float(torch.mean(current_q).item()))
        
        return float(critic_loss.item())
    
    def update_actor(self, batch: Tuple[torch.Tensor, ...]) -> float:
        """
        ä½¿ç”¨æ–¹ç¨‹(25)çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°æ¼”å‘˜ç½‘ç»œï¼ŒåŒ…å«åŸºçº¿å€¼å‡å°‘æ–¹å·®ã€‚
        
        å®ç°æ–¹ç¨‹(27): Ï†_r = Ï†_r + Î¶ âˆ‡_{Ï†_r} J_r(Ï€)
        å…¶ä¸­ç­–ç•¥æ¢¯åº¦åŒ…å«åŸºçº¿å€¼ä»¥å‡å°‘ä¼°è®¡æ–¹å·®
        """
        observations, own_actions, joint_actions, rewards, next_observations, dones = batch
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„åŠ¨ä½œ
        current_actions = self.actor(observations)
        
        # ä¸ºå½“å‰åŠ¨ä½œç”Ÿæˆæœ€ä¼˜çš„è”åˆåŠ¨ä½œï¼ˆå®ç°æ–¹ç¨‹23ï¼‰
        # ä½¿ç”¨ä¸‰å…ƒåŠ¨ä½œç©ºé—´è€Œä¸æ˜¯éšæœºé‡‡æ ·
        current_action_states = current_actions
        
        # è®¡ç®—è™šæ‹Ÿè”åˆç­–ç•¥ï¼Œä½¿ç”¨ä¸‰å…ƒåŠ¨ä½œç©ºé—´
        best_joint_actions = self.compute_virtual_joint_policy(
            observations, current_actions, current_action_states
        )
        
        # è®¡ç®—Qå€¼
        q_values = self.critic(observations, current_actions, best_joint_actions)
        
        # è®¡ç®—åŸºçº¿å€¼ä»¥å‡å°‘æ–¹å·®ï¼ˆè®ºæ–‡æ–¹ç¨‹(25)åæåˆ°çš„åŸºçº¿ï¼‰
        # baseline = Î£ Ï€_r(a_{r,t}|o_{r,t}) * Q_r^Ï€â€ (o_{r,t}, a_{r,t}, a_{-r,t})
        baseline = self.compute_baseline_value(observations, current_actions)
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼šA = Q - baseline
        advantages = q_values - baseline
        
        # è®¡ç®—ç­–ç•¥æ¦‚ç‡çš„å¯¹æ•°
        # å‡è®¾è¿ç»­åŠ¨ä½œä½¿ç”¨æ­£æ€åˆ†å¸ƒç­–ç•¥
        action_means = current_actions
        action_stds = torch.ones_like(action_means) * 0.1  # å›ºå®šæ ‡å‡†å·®
        
        # è®¡ç®—çœŸå®æ‰§è¡ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        log_probs = -0.5 * ((own_actions - action_means) / action_stds) ** 2 - \
                   torch.log(action_stds) - 0.5 * torch.log(2 * torch.tensor(np.pi))
        log_probs = torch.sum(log_probs, dim=1, keepdim=True)
        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼šâˆ‡J_r(Ï€) = E[âˆ‡log Ï€_r * (Q - baseline)]
        # æ³¨æ„ï¼šPyTorchä¸­ä½¿ç”¨è´Ÿå·å› ä¸ºæˆ‘ä»¬æœ€å°åŒ–æŸå¤±è€Œä¸æ˜¯æœ€å¤§åŒ–å¥–åŠ±
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # æ›´æ–°æ¼”å‘˜ç½‘ç»œ (æ–¹ç¨‹27: Ï†_r = Ï†_r + Î¶ âˆ‡_{Ï†_r} J_r(Ï€))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
        
        self.actor_optimizer.step()
        
        return float(actor_loss.item())
    
    def update_target_networks(self):
        """ç›®æ ‡ç½‘ç»œçš„è½¯æ›´æ–°."""
        
        # æ›´æ–°æ¼”å‘˜ç›®æ ‡ç½‘ç»œ
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(
                self.config.tau * param.data + (1 - self.config.tau) * target_param.data
            )
        
        # æ›´æ–°è¯„è®ºå®¶ç›®æ ‡ç½‘ç»œ
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(
                self.config.tau * param.data + (1 - self.config.tau) * target_param.data
            )
    
    def add_experience(self,
                      observation: np.ndarray,
                      own_action: np.ndarray,
                      joint_action: np.ndarray,
                      reward: float,
                      next_observation: np.ndarray,
                      done: bool):
        """å‘å›æ”¾ç¼“å†²åŒºæ·»åŠ ç»éªŒ."""
        
        self.replay_buffer.add(
            observation, own_action, joint_action, reward, next_observation, done
        )
    
    def train_step(self) -> Dict[str, float]:
        """
        å¦‚æœæœ‰è¶³å¤Ÿçš„ç»éªŒï¼Œæ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤.
        
        è¿”å›:
            åŒ…å«è®­ç»ƒæŒ‡æ ‡çš„å­—å…¸
        """
        if len(self.replay_buffer) < self.config.batch_size:
            return {}
        
        # ä»å›æ”¾ç¼“å†²åŒºé‡‡æ ·æ‰¹æ¬¡
        batch = self.replay_buffer.sample(self.config.batch_size)
        
        # æ›´æ–°ç½‘ç»œ
        critic_loss = self.update_critic(batch)
        actor_loss = self.update_actor(batch)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_target_networks()
        
        self.training_step += 1
        
        return {
            'actor_loss': actor_loss,
            'critic_loss': critic_loss,
            'q_value': self.q_values[-1] if self.q_values else 0.0,
            'buffer_size': len(self.replay_buffer)
        }
    
    def save_models(self, filepath_prefix: str):
        """ä¿å­˜æ¼”å‘˜å’Œè¯„è®ºå®¶æ¨¡å‹."""
        
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'training_step': self.training_step
        }, f"{filepath_prefix}_agent_{self.agent_id}.pt")
    
    def load_models(self, filepath_prefix: str):
        """åŠ è½½æ¼”å‘˜å’Œè¯„è®ºå®¶æ¨¡å‹."""
        
        checkpoint = torch.load(f"{filepath_prefix}_agent_{self.agent_id}.pt")
        
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.training_step = checkpoint['training_step']
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())


class PACMCoFLTrainer:
    """
    å®ç°ç®—æ³•1çš„PAC-MCoFLè®­ç»ƒå™¨.
    
    åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­åè°ƒå¤šä¸ªPACæ™ºèƒ½ä½“çš„è®­ç»ƒ.
    """
    
    def __init__(self,
                 service_ids: List[int],
                 environment: MultiServiceFLEnvironment,
                 fl_system: MultiServiceFLSystem,
                 config: PACConfig,
                 constraints: OptimizationConstraints):
        """
        åˆå§‹åŒ–PAC-MCoFLè®­ç»ƒå™¨.
        
        å‚æ•°:
            service_ids: æœåŠ¡æä¾›å•†IDåˆ—è¡¨
            environment: è”é‚¦å­¦ä¹ ç¯å¢ƒ
            fl_system: è”é‚¦å­¦ä¹ ç³»ç»Ÿ
            config: PACé…ç½®
            constraints: ç³»ç»Ÿçº¦æŸ
        """
        self.service_ids = service_ids
        self.environment = environment
        self.fl_system = fl_system
        self.config = config
        self.constraints = constraints
        
        # åˆå§‹åŒ–PACæ™ºèƒ½ä½“
        if hasattr(environment.observation_space, 'shape'):
            observation_dim = environment.observation_space.shape[0]
        else:
            # å­—å…¸å®ç°çš„å›é€€
            observation_dim = environment.observation_space['shape'][0]
        
        if hasattr(environment.action_space, 'shape'):
            action_dim = environment.action_space.shape[0]
        else:
            # å­—å…¸å®ç°çš„å›é€€
            action_dim = environment.action_space['shape'][0]
        
        num_agents = len(service_ids)
        
        self.agents = {}
        for service_id in service_ids:
            self.agents[service_id] = PACAgent(
                service_id, observation_dim, action_dim, num_agents, config, constraints
            )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = defaultdict(list)
        self.episode_lengths = []
        self.training_metrics = defaultdict(list)
        # è¿½åŠ ï¼šè®°å½•æ¯æ­¥çš„åŠ¨ä½œä¸è¯„ä¼°æŒ‡æ ‡ã€å‡†ç¡®ç‡è¶‹åŠ¿
        self.episode_step_logs: List[List[Dict[str, Any]]] = []  # æ¯ä¸ªepisodeä¸€ä¸ªåˆ—è¡¨ï¼Œå†…å«é€æ­¥æ—¥å¿—
        self.accuracy_trends = defaultdict(list)  # {service_id: [acc_step1, acc_step2, ...]}
        
        # ç´¯ç§¯æœŸæœ›å¥–åŠ±J_r(Ï€)è·Ÿè¸ª
        self.cumulative_rewards = defaultdict(list)
        
        print(f"åˆå§‹åŒ–PAC-MCoFLè®­ç»ƒå™¨ï¼ŒåŒ…å«{num_agents}ä¸ªæ™ºèƒ½ä½“")
        print(f"è§‚å¯Ÿç»´åº¦: {observation_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    
    def compute_cumulative_expected_reward(self, 
                                         rewards_trajectory: List[Dict[int, float]]) -> Dict[int, float]:
        """
        å¦‚æ–¹ç¨‹(19-20)æ‰€ç¤ºè®¡ç®—ç´¯ç§¯æœŸæœ›å¥–åŠ±J_r(Ï€).
        
        å‚æ•°:
            rewards_trajectory: æ¯ä¸ªæ—¶é—´æ­¥çš„å¥–åŠ±åˆ—è¡¨
            
        è¿”å›:
            æ¯ä¸ªæ™ºèƒ½ä½“çš„ç´¯ç§¯æœŸæœ›å¥–åŠ±
        """
        cumulative_rewards = {}
        
        for service_id in self.service_ids:
            total_reward = 0.0
            for t, rewards in enumerate(rewards_trajectory):
                discount = (self.config.gamma ** t)
                total_reward += discount * rewards.get(service_id, 0.0)
            
            cumulative_rewards[service_id] = total_reward
        
        return cumulative_rewards
    
    def train_episode(self) -> Tuple[Dict[int, float], int, Dict[str, Any]]:
        """
        æŒ‰ç…§ç®—æ³•1è®­ç»ƒä¸€ä¸ªå›åˆ (Algorithm 1: PAC-MCoFL Training).
        
        å®ç°ç®—æ³•1çš„æ­¥éª¤2-29çš„å®Œæ•´è®­ç»ƒå¾ªç¯
        
        è¿”å›:
            (å›åˆå¥–åŠ±, å›åˆé•¿åº¦, å›åˆä¿¡æ¯)çš„å…ƒç»„
        """
        # æ­¥éª¤3: åˆå§‹åŒ–å…¨å±€æ¨¡å‹å‚æ•°å’Œå®¢æˆ·ç«¯ç‰¹å¾
        # Initialize global model parameters Ï‰_{r,0} and client features
        observations = self.environment.reset()
        
        episode_rewards = {service_id: 0.0 for service_id in self.service_ids}
        episode_trajectory = []
        step = 0
        # æœ¬å›åˆé€æ­¥æ—¥å¿—
        episode_logs: List[Dict[str, Any]] = []
        
        # åœ¨ä»»ä½•è®­ç»ƒå‘ç”Ÿå‰ï¼Œè¿›è¡Œä¸€æ¬¡æœªè®­ç»ƒè¯„ä¼°å¹¶è®°å½•(step=0)
        try:
            initial_step_logs: Dict[int, Dict[str, Any]] = {}
            for sid in self.service_ids:
                acc = 0.0; avg_loss = 0.0
                try:
                    dl = self.fl_system.service_data_loaders.get(sid)
                    test_sets = dl.fl_test_set() if dl else []
                    model_module = self.fl_system.service_models[sid].model
                    model_module.eval()
                    device = next(model_module.parameters()).device
                    correct = 0; total = 0; loss_sum = 0.0; num_batches = 0
                    criterion = torch.nn.CrossEntropyLoss()
                    for entry in test_sets:
                        for batch in entry.get('batches', []):
                            feats = batch.get('features'); labels = batch.get('labels')
                            if feats is None or labels is None:
                                continue
                            feats = feats.to(device); labels = labels.to(device)
                            with torch.no_grad():
                                out = model_module(feats)
                                loss = criterion(out, labels)
                            loss_sum += loss.item()
                            preds = out.argmax(1)
                            correct += (preds == labels).sum().item()
                            total += labels.size(0)
                            num_batches += 1
                    if total > 0:
                        acc = correct / total
                        avg_loss = loss_sum / max(num_batches, 1)
                except Exception as e:
                    try:
                        print(f"[WARN][RL-FL Link] åˆå§‹æœªè®­ç»ƒè¯„ä¼°å¤±è´¥(æœåŠ¡{sid}): {e}")
                    except Exception:
                        pass
                # è®°å½•æ—¥å¿—ä¸è¶‹åŠ¿
                initial_step_logs[sid] = {
                    'accuracy': float(acc),
                    'loss': float(avg_loss),
                    'q_level': int(getattr(self.fl_system.service_models.get(sid, None), 'quantization_level', 0)) if self.fl_system and sid in self.fl_system.service_models else 0
                }
                self.accuracy_trends[sid].append(float(acc))
            episode_logs.append({'step': 0, 'services': initial_step_logs})
        except Exception:
            pass

        # ç®—æ³•1æ­¥éª¤20: åˆ›å»ºå›åˆæ‰¹æ¬¡å­˜å‚¨ç»éªŒ
        episode_batch = []
        
        # æ­¥éª¤4: å¯¹äºæ¯è½®t = 0, 1, ..., T-1
        # for each round t = 0, 1, ..., T-1 do
        while step < self.config.max_rounds_per_episode:
            current_obs = {sid: observations[sid] for sid in self.service_ids}

            # ç­–ç•¥é‡‡æ ·
            actions = {sid: self.agents[sid].select_action(current_obs[sid], training=True)
                       for sid in self.service_ids}

            rewards = {}
            next_observations = {}
            dones = {sid: False for sid in self.service_ids}

            # å°†è¿ç»­åŠ¨ä½œæ•°ç»„è½¬æ¢ä¸ºç¯å¢ƒæ‰€éœ€çš„Actionå¯¹è±¡ï¼ˆç”¨äºèƒ½è€—/æ—¶å»¶/é€šä¿¡é‡ä»¿çœŸï¼‰
            try:
                action_objects = {
                    sid: Action.from_array(np.array(actions[sid], dtype=float))
                    for sid in self.service_ids
                }
            except Exception:
                # å›é€€ï¼šé€ä¸ªå®‰å…¨è½¬æ¢
                action_objects = {}
                for sid in self.service_ids:
                    arr = actions[sid]
                    arr = np.array(arr, dtype=float) if not isinstance(arr, np.ndarray) else arr
                    action_objects[sid] = Action.from_array(arr)

            # ä½¿ç”¨ç¯å¢ƒçš„ä»¿çœŸè·¯å¾„ä¸€æ¬¡æ€§è®¡ç®—ç³»ç»Ÿä»£ä»·ï¼ˆèƒ½è€—/æ—¶å»¶ï¼‰ä¸é€šä¿¡é‡
            # æ³¨æ„ï¼šæ­¤å¤„ä¸æ›´æ–°ç¯å¢ƒå†…éƒ¨çŠ¶æ€ï¼Œä»…ç”¨äºå¥–åŠ±è®¡ç®—çš„æ•°æ®æ¥æº
            try:
                sim_new_observations, sim_communication_volumes = self.environment._simulate_fl_round(action_objects)
            except Exception:
                # è‹¥ä»¿çœŸå¤±è´¥ï¼Œåˆ™æ„é€ ç©ºå ä½ï¼Œå¥–åŠ±å°†é€€åŒ–ä¸ºä»…åŸºäºç²¾åº¦
                sim_new_observations = {sid: self.environment.observations[sid] for sid in self.service_ids}
                sim_communication_volumes = {}

            # æ‰“å°å½“å‰RLæ­¥çš„æ¦‚è§ˆ
            try:
                print(f"[PAC][RL] Step {step + 1}/{self.config.max_rounds_per_episode}")
            except Exception:
                pass

            # è¯„ä¼°(è®­ç»ƒå‰) + å¥–åŠ±è®¡ç®—ï¼Œç„¶åå†è¿›è¡ŒçœŸå®è®­ç»ƒ
            # å½“å‰æ­¥å„æœåŠ¡æ—¥å¿—å®¹å™¨
            step_service_logs: Dict[int, Dict[str, Any]] = {}
            for sid, act_arr in actions.items():
                n_clients = int(max(1, min(len(self.fl_system.service_configs[sid].client_ids), round(act_arr[0]))))
                q_level = int(max(1, min(32, round(act_arr[3])))) if len(act_arr) > 3 else 8
                # ä»åŠ¨ä½œæ•°ç»„æå–få’ŒBï¼ˆä¿æŒå®¢æˆ·ç«¯é€‰æ‹©é€»è¾‘ä¸å˜ï¼‰
                cpu_freq = float(act_arr[1]) if len(act_arr) > 1 else self.constraints.min_frequency
                bandwidth = float(act_arr[2]) if len(act_arr) > 2 else self.constraints.min_bandwidth

                trainer = self.fl_system.service_trainers.get(sid)
                if trainer is not None and hasattr(trainer, 'users_per_round'):
                    trainer.users_per_round = n_clients
                if trainer is not None and hasattr(trainer, 'cfg'):
                    try:
                        if 'users_per_round' in trainer.cfg:
                            trainer.cfg['users_per_round'] = n_clients
                    except Exception:
                        pass

                # æ³¨å…¥f/B/qè¦†ç›–ï¼Œå½±å“ç³»ç»Ÿèƒ½è€—/æ—¶å»¶ä¸é‡åŒ–é€šä¿¡é‡
                try:
                    self.fl_system.set_service_action(
                        service_id=sid,
                        n_clients=n_clients,
                        cpu_frequency=cpu_freq,
                        bandwidth=bandwidth,
                        quantization_level=q_level,
                    )
                except Exception as e:
                    print(f"[WARN] set_service_action å¤±è´¥: {e}")

                fl_model = self.fl_system.service_models.get(sid)
                if fl_model and getattr(self.fl_system, 'debug_disable_quant', False) is False:
                    if hasattr(fl_model, 'quantization_level'):
                        fl_model.quantization_level = q_level

                # å…¨é‡æµ‹è¯•è¯„ä¼°ï¼ˆé™ä½é¢‘ç‡ï¼‰ï¼Œåœ¨è®­ç»ƒå‰è¿›è¡Œ
                # æ¯éš” EVAL_FREQUENCY æ­¥æ‰§è¡Œä¸€æ¬¡å…¨é‡è¯„ä¼°ï¼›å…¶ä½™æ­¥å¤ç”¨ä¸Šä¸€æ—¶åˆ»çš„å‡†ç¡®ç‡ä»¥é™ä½è¯„ä¼°å¼€é”€
                EVAL_FREQUENCY = 2
                do_full_eval = ((step % EVAL_FREQUENCY) == 0) or ((step + 1) == self.config.max_rounds_per_episode)
                acc = 0.0; avg_loss = 0.0
                try:
                    model_module = self.fl_system.service_models[sid].model
                    model_module.eval()
                    device = next(model_module.parameters()).device
                    if do_full_eval:
                        dl = self.fl_system.service_data_loaders.get(sid)
                        test_sets = dl.fl_test_set() if dl else []
                        correct = 0; total = 0; loss_sum = 0.0; num_batches = 0
                        criterion = torch.nn.CrossEntropyLoss()
                        for entry in test_sets:
                            for batch in entry.get('batches', []):
                                feats = batch.get('features'); labels = batch.get('labels')
                                if feats is None or labels is None:
                                    continue
                                feats = feats.to(device); labels = labels.to(device)
                                with torch.no_grad():
                                    out = model_module(feats)
                                    loss = criterion(out, labels)
                                loss_sum += loss.item()
                                preds = out.argmax(1)
                                correct += (preds == labels).sum().item()
                                total += labels.size(0)
                                num_batches += 1
                        if total > 0:
                            acc = correct / total
                            avg_loss = loss_sum / max(num_batches, 1)
                    else:
                        # å¤ç”¨ä¸Šä¸€æ—¶åˆ»çš„å‡†ç¡®ç‡ï¼Œé¿å…é¢å¤–è¯„ä¼°å¼€é”€
                        prev_acc_list = self.accuracy_trends.get(sid, [])
                        acc = float(prev_acc_list[-1]) if isinstance(prev_acc_list, list) and len(prev_acc_list) > 0 else 0.0
                        avg_loss = 0.0
                except Exception as e:
                    print(f"[WARN][RL-FL Link] å…¨é‡æµ‹è¯•è¯„ä¼°å¤±è´¥: {e}")

                # æ›´æ–°è§‚æµ‹å¹¶è®¡ç®—å¥–åŠ±
                # åŸºäºç¯å¢ƒä»¿çœŸå¾—åˆ°çš„ç³»ç»Ÿä»£ä»·è§‚æµ‹ï¼Œå†è¦†ç›–çœŸå®è¯„æµ‹å¾—åˆ°çš„ç²¾åº¦/æŸå¤±/é‡åŒ–çº§åˆ«
                obs_obj = sim_new_observations.get(sid, self.environment.observations[sid])
                obs_obj.fl_state.accuracy = acc
                obs_obj.fl_state.loss = avg_loss
                obs_obj.fl_state.quantization_level = q_level

                # ä½¿ç”¨å®Œæ•´å‚æ•°ï¼ˆå«é€šä¿¡é‡ï¼‰è®¡ç®—å¥–åŠ±ï¼›è‹¥reward_functionsç¼ºå¤±åˆ™å›é€€ä¸ºacc
                if hasattr(self.environment, 'reward_functions') and sid in self.environment.reward_functions:
                    reward_val = self.environment.reward_functions[sid].calculate(
                        service_id=sid,
                        observation=obs_obj,
                        action=action_objects[sid],
                        all_actions=action_objects,
                        communication_volumes=sim_communication_volumes,
                    )
                else:
                    reward_val = acc

                rewards[sid] = reward_val
                # æ›´æ–°ä¸ºnumpyæ•°ç»„æ ¼å¼ä¾›å¼ºåŒ–å­¦ä¹ ä½¿ç”¨
                next_observations[sid] = obs_obj.to_array()

                # è®°å½•å½“å‰æœåŠ¡åœ¨è¯¥æ­¥çš„å†³ç­–ä¸ç»“æœ
                try:
                    action_struct = {
                        'n_clients': int(getattr(action_objects[sid], 'n_clients', n_clients)),
                        'cpu_frequency': float(getattr(action_objects[sid], 'cpu_frequency', float(act_arr[1]) if len(act_arr) > 1 else 0.0)),
                        'bandwidth': float(getattr(action_objects[sid], 'bandwidth', float(act_arr[2]) if len(act_arr) > 2 else 0.0)),
                        'quantization_level': int(getattr(action_objects[sid], 'quantization_level', q_level)),
                    }
                except Exception:
                    action_struct = {
                        'n_clients': n_clients,
                        'cpu_frequency': float(act_arr[1]) if len(act_arr) > 1 else 0.0,
                        'bandwidth': float(act_arr[2]) if len(act_arr) > 2 else 0.0,
                        'quantization_level': q_level,
                    }

                step_service_logs[sid] = {
                    'action': action_struct,
                    'accuracy': float(acc),
                    'loss': float(avg_loss),
                    'reward': float(reward_val),
                    'q_level': int(q_level),
                }

                # æ‰“å°æ¯ä¸ªæœåŠ¡åœ¨è¯¥æ­¥çš„å†³ç­–ä¸ç»“æœæ‘˜è¦ï¼Œä¾¿äºè§‚å¯Ÿå‚æ•°å˜åŒ–
                try:
                    action_print = action_objects[sid]
                    energy = obs_obj.system_state.total_energy
                    delay = obs_obj.system_state.total_delay
                    comm_vol = sim_communication_volumes.get(sid, None)
                    print(
                        f"  [S{sid}] action="
                        f"{{n={action_print.n_clients}, f={action_print.cpu_frequency/1e9:.2f}GHz, "
                        f"B={action_print.bandwidth/1e6:.2f}MHz, q={action_print.quantization_level}}} "
                        f"metrics="
                        f"{{acc={acc:.4f}, loss={avg_loss:.4f}, E={energy:.6e}J, T={delay:.6f}s, vol={comm_vol}}} "
                        f"reward={reward_val:.4f}"
                    )
                except Exception:
                    pass

            # å­˜å‚¨ç»éªŒï¼ˆåŸºäºè®­ç»ƒå‰è¯„ä¼°çš„å¥–åŠ±ï¼‰
            for sid in self.service_ids:
                joint_action = np.concatenate([actions[o] for o in self.service_ids if o != sid])
                exp = {
                    'observation': current_obs[sid],
                    'own_action': actions[sid],
                    'joint_action': joint_action,
                    'reward': rewards[sid],
                    'next_observation': next_observations[sid],
                    'done': dones[sid]
                }
                episode_batch.append((sid, exp))
                self.agents[sid].add_experience(**exp)

            # åœ¨å®Œæˆè®°å½•ä¸ç»éªŒå­˜å‚¨åï¼Œæ‰§è¡Œå•è½®çœŸå®è®­ç»ƒ
            for sid, act_arr in actions.items():
                trainer = self.fl_system.service_trainers.get(sid)
                prev_epochs = 1
                prev_eval_freq = None
                if trainer is not None and hasattr(trainer, 'cfg'):
                    prev_epochs = getattr(trainer.cfg, 'epochs', 1)
                    prev_eval_freq = getattr(trainer.cfg, 'eval_epoch_frequency', None)
                    trainer.cfg.epochs = 1
                    if prev_eval_freq is not None:
                        trainer.cfg.eval_epoch_frequency = 10**6
                try:
                    self.fl_system.train_service(sid, num_rounds=1, enable_metrics=False)
                except Exception as e:
                    print(f"[WARN][RL-FL Link] æœåŠ¡{sid} çœŸå®è®­ç»ƒå¤±è´¥: {e}")
                finally:
                    if trainer is not None and hasattr(trainer, 'cfg'):
                        trainer.cfg.epochs = prev_epochs
                        if prev_eval_freq is not None:
                            trainer.cfg.eval_epoch_frequency = prev_eval_freq

            for sid, rw in rewards.items():
                episode_rewards[sid] += rw
            episode_trajectory.append(rewards)
            observations = next_observations
            step += 1

            if all(dones.values()):
                break

            # è¿½åŠ è¯¥æ­¥èšåˆæ—¥å¿—å¹¶æ›´æ–°å‡†ç¡®ç‡è¶‹åŠ¿
            episode_logs.append({
                'step': int(step),
                'services': step_service_logs
            })
            for sid, s_log in step_service_logs.items():
                self.accuracy_trends[sid].append(float(s_log.get('accuracy', 0.0)))
        
        # æ­¥éª¤20: å°†å›åˆæ‰¹æ¬¡æ’å…¥é‡æ”¾ç¼“å†²åŒºH
        # Insert the episode batch into a replay buffer H
        
        # æ­¥éª¤21: æ¸…ç©ºå›åˆæ‰¹æ¬¡
        # Clear the episode batch (å·²åœ¨add_experienceä¸­å¤„ç†)
        
        # è®¡ç®—ç´¯ç§¯æœŸæœ›å¥–åŠ±J_r(Ï€) (æ–¹ç¨‹19-20)
        cumulative_rewards = self.compute_cumulative_expected_reward(episode_trajectory)
        for service_id, cum_reward in cumulative_rewards.items():
            self.cumulative_rewards[service_id].append(cum_reward)
        
        # ä¿å­˜æœ¬å›åˆé€æ­¥æ—¥å¿—
        self.episode_step_logs.append(episode_logs)

        episode_info = {
            'cumulative_rewards': cumulative_rewards,
            'step_rewards': episode_trajectory,
            'episode_length': step,
            'episode_batch_size': len(episode_batch),
            'step_logs': episode_logs
        }
        
        return episode_rewards, step, episode_info
    
    def train(self) -> Dict[str, Any]:
        """
        å®ç°ç®—æ³•1çš„ä¸»è®­ç»ƒå¾ªç¯ (Algorithm 1: PAC-MCoFL Training).
        
        å®Œæ•´å®ç°ç®—æ³•1çš„æ­¥éª¤1-29
        
        è¿”å›:
            è®­ç»ƒç»Ÿè®¡å’ŒæŒ‡æ ‡
        """
        print(f"ğŸš€ å¼€å§‹PAC-MCoFLè®­ç»ƒ (Algorithm 1)")
        print(f"   æ€»å›åˆæ•°: {self.config.num_episodes}")
        print(f"   æ¯å›åˆæœ€å¤§è½®æ¬¡: {self.config.max_rounds_per_episode}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.service_ids)}")
        
        # ç®—æ³•1æ­¥éª¤1: åŠ è½½è”é‚¦å­¦ä¹ æœåŠ¡çš„è®­ç»ƒæ•°æ®é›†
        # Load the training dataset of FL service
        print("ğŸ“Š è”é‚¦å­¦ä¹ æ•°æ®é›†å·²ç”±FLç³»ç»ŸåŠ è½½")
        
        # æ­¥éª¤2: while each episode num_eps â‰¤ num_max do
        for episode in range(self.config.num_episodes):
            
            # è®­ç»ƒä¸€ä¸ªå›åˆ (æ­¥éª¤3-21)
            episode_rewards, episode_length, episode_info = self.train_episode()
            print(f"[PAC][RL] episode : {episode + 1}/{self.config.num_episodes}")
            # å­˜å‚¨å›åˆç»Ÿè®¡
            for service_id, reward in episode_rewards.items():
                self.episode_rewards[service_id].append(reward)
            self.episode_lengths.append(episode_length)
            
            # æ­¥éª¤22-27: ç½‘ç»œæ›´æ–°
            # if H reaches the buffer size for training then
            training_metrics = {}
            for service_id in self.service_ids:
                # æ­¥éª¤23: Sample a single batch from H
                # æ­¥éª¤24-26: æ›´æ–°ç½‘ç»œ
                agent_metrics = self.agents[service_id].train_step()
                if agent_metrics:
                    for key, value in agent_metrics.items():
                        self.training_metrics[f"{service_id}_{key}"].append(value)
                        training_metrics[f"agent_{service_id}_{key}"] = value
            
            # å®šæœŸè¯„ä¼°å’Œæ—¥å¿—è®°å½•
            if (episode + 1) % self.config.eval_frequency == 0:
                # è®¡ç®—æœ€è¿‘10ä¸ªå›åˆçš„å¹³å‡æ€§èƒ½
                avg_rewards = {sid: np.mean(self.episode_rewards[sid][-10:]) 
                              for sid in self.service_ids}
                avg_cumulative = {sid: np.mean(self.cumulative_rewards[sid][-10:])
                                 for sid in self.service_ids}
                
                print(f"\nğŸ“ˆ å›åˆ {episode + 1}/{self.config.num_episodes}")
                print(f"   å¹³å‡å›åˆå¥–åŠ±: {avg_rewards}")
                print(f"   ç´¯ç§¯æœŸæœ›å¥–åŠ±J_r(Ï€): {avg_cumulative}")
                print(f"   å›åˆé•¿åº¦: {episode_length}")
                
                if training_metrics:
                    print(f"   è®­ç»ƒæŒ‡æ ‡: {training_metrics}")
                
                # æ˜¾ç¤ºPACç®—æ³•çš„å…³é”®æŒ‡æ ‡
                print(f"   é‡æ”¾ç¼“å†²åŒºå¤§å°: {[len(self.agents[sid].replay_buffer) for sid in self.service_ids]}")
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if (episode + 1) % self.config.save_frequency == 0:
                self.save_models(f"pac_mcofl_episode_{episode + 1}")
                print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° pac_mcofl_episode_{episode + 1}")
            
            # æ­¥éª¤28: num_eps â† num_eps + 1
            # (ç”±forå¾ªç¯è‡ªåŠ¨å¤„ç†)
        
        # æ­¥éª¤29: end
        print("\nâœ… PAC-MCoFLè®­ç»ƒå®Œæˆ!")
        print(f"   æ€»è®­ç»ƒå›åˆ: {self.config.num_episodes}")
        print(f"   å¹³å‡å›åˆé•¿åº¦: {np.mean(self.episode_lengths):.2f}")
        
        # æœ€ç»ˆæ€§èƒ½æ€»ç»“
        final_avg_rewards = {sid: np.mean(self.episode_rewards[sid][-50:]) 
                            for sid in self.service_ids}
        final_cumulative = {sid: np.mean(self.cumulative_rewards[sid][-50:])
                           for sid in self.service_ids}
        
        print(f"   æœ€ç»ˆå¹³å‡å›åˆå¥–åŠ±: {final_avg_rewards}")
        print(f"   æœ€ç»ˆç´¯ç§¯æœŸæœ›å¥–åŠ±J_r(Ï€): {final_cumulative}")
        
        return {
            'episode_rewards': dict(self.episode_rewards),
            'episode_lengths': self.episode_lengths,
            'cumulative_rewards': dict(self.cumulative_rewards),
            'training_metrics': dict(self.training_metrics),
            'accuracy_trends': dict(self.accuracy_trends),
            'action_logs': self.episode_step_logs,
            'final_performance': {
                'avg_episode_rewards': final_avg_rewards,
                'avg_cumulative_rewards': final_cumulative
            }
        }
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, Any]:
        """
        è¯„ä¼°è®­ç»ƒå¥½çš„PACæ™ºèƒ½ä½“.
        
        å‚æ•°:
            num_episodes: è¯„ä¼°å›åˆæ•°
            
        è¿”å›:
            è¯„ä¼°æŒ‡æ ‡
        """
        eval_rewards = defaultdict(list)
        eval_cumulative = defaultdict(list)
        eval_lengths = []
        
        for episode in range(num_episodes):
            observations = self.environment.reset()
            episode_rewards = {service_id: 0.0 for service_id in self.service_ids}
            episode_trajectory = []
            step = 0
            
            while step < self.config.max_rounds_per_episode:
                # é€‰æ‹©åŠ¨ä½œæ—¶ä¸æ·»åŠ æ¢ç´¢å™ªå£°
                actions = {}
                for service_id in self.service_ids:
                    action = self.agents[service_id].select_action(
                        observations[service_id], training=False
                    )
                    actions[service_id] = action
                
                observations, rewards, dones, _ = self.environment.step(actions)
                
                for service_id, reward in rewards.items():
                    episode_rewards[service_id] += reward
                
                episode_trajectory.append(rewards)
                step += 1
                
                if all(dones.values()):
                    break
            
            # å­˜å‚¨è¯„ä¼°ç»“æœ
            for service_id, reward in episode_rewards.items():
                eval_rewards[service_id].append(reward)
            
            cumulative_rewards = self.compute_cumulative_expected_reward(episode_trajectory)
            for service_id, cum_reward in cumulative_rewards.items():
                eval_cumulative[service_id].append(cum_reward)
            
            eval_lengths.append(step)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_rewards = {service_id: np.mean(rewards) 
                      for service_id, rewards in eval_rewards.items()}
        avg_cumulative = {service_id: np.mean(rewards)
                         for service_id, rewards in eval_cumulative.items()}
        avg_length = np.mean(eval_lengths)
        
        return {
            'avg_episode_rewards': avg_rewards,
            'avg_cumulative_rewards': avg_cumulative,
            'avg_episode_length': avg_length,
            'all_episode_rewards': dict(eval_rewards),
            'all_cumulative_rewards': dict(eval_cumulative),
            'all_episode_lengths': eval_lengths
        }
    
    def save_models(self, filepath_prefix: str):
        """ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹."""
        for service_id in self.service_ids:
            self.agents[service_id].save_models(filepath_prefix)
        print(f"æ¨¡å‹å·²ä¿å­˜ï¼Œå‰ç¼€: {filepath_prefix}")
    
    def load_models(self, filepath_prefix: str):
        """åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹."""
        for service_id in self.service_ids:
            self.agents[service_id].load_models(filepath_prefix)
        print(f"æ¨¡å‹å·²åŠ è½½ï¼Œå‰ç¼€: {filepath_prefix}")
    
    def get_training_summary(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆè®­ç»ƒæ€»ç»“."""
        
        summary = {
            'config': {
                'num_episodes': self.config.num_episodes,
                'max_rounds_per_episode': self.config.max_rounds_per_episode,
                'actor_lr': self.config.actor_lr,
                'critic_lr': self.config.critic_lr,
                'gamma': self.config.gamma
            },
            'final_performance': {},
            'pareto_optimality': {}
        }
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½
        if self.episode_rewards:
            for service_id in self.service_ids:
                rewards = self.episode_rewards[service_id]
                cumulative = self.cumulative_rewards[service_id]
                
                if rewards and cumulative:
                    summary['final_performance'][service_id] = {
                        'final_episode_reward': rewards[-1],
                        'avg_episode_reward': np.mean(rewards),
                        'final_cumulative_reward': cumulative[-1],
                        'avg_cumulative_reward': np.mean(cumulative),
                        'improvement': rewards[-1] - rewards[0] if len(rewards) > 1 else 0
                    }
        
        # åˆ†æå¸•ç´¯æ‰˜æœ€ä¼˜æ€§
        if self.cumulative_rewards:
            # æ£€æŸ¥æœ€ç»ˆè”åˆç­–ç•¥æ˜¯å¦å®ç°å¸•ç´¯æ‰˜æœ€ä¼˜
            final_cumulative = {sid: self.cumulative_rewards[sid][-1] 
                              for sid in self.service_ids if self.cumulative_rewards[sid]}
            
            summary['pareto_optimality'] = {
                'final_joint_cumulative_reward': sum(final_cumulative.values()),
                'individual_cumulative_rewards': final_cumulative,
                'pareto_improvement_achieved': all(
                    final_cumulative[sid] > self.cumulative_rewards[sid][0]
                    for sid in final_cumulative.keys() if len(self.cumulative_rewards[sid]) > 1
                )
            }
        
        return summary


def test_pac_mcofl():
    """æµ‹è¯•PAC-MCoFLå®ç°."""
    print("æµ‹è¯•PAC-MCoFLå®ç°...")
    
    # è®¾ç½®
    from optimization_problem import OptimizationConstraints
    from mdp_framework import MultiServiceFLEnvironment
    from multi_service_fl import MultiServiceFLSystem, ServiceProviderConfig, ClientResourceConfig
    
    service_ids = [1, 2]
    constraints = OptimizationConstraints(
        max_energy=0.1, max_delay=5.0, max_clients=5, max_bandwidth=5e6
    )
    
    config = PACConfig(
        num_episodes=10,  # æµ‹è¯•ç”¨çš„å°å€¼
        max_rounds_per_episode=8,
        buffer_size=100,
        batch_size=16,
        actor_lr=0.001,
        critic_lr=0.003,
        update_frequency=2
    )
    
    # åˆ›å»ºç¯å¢ƒ
    environment = MultiServiceFLEnvironment(service_ids, constraints, max_rounds=config.max_rounds_per_episode)
    
    # åˆ›å»ºè”é‚¦å­¦ä¹ ç³»ç»Ÿ(æµ‹è¯•ç”¨ç®€åŒ–ç‰ˆæœ¬)
    service_configs = [
        ServiceProviderConfig(service_id=1, name="Service1", client_ids=[1, 2], 
                            model_architecture={"type": "simple_nn", "hidden_size": 64}),
        ServiceProviderConfig(service_id=2, name="Service2", client_ids=[3, 4],
                            model_architecture={"type": "simple_nn", "hidden_size": 64})
    ]
    client_configs = {i: ClientResourceConfig(i, 1e-28, 1000, 1e9, 0.1, 1e-3, 1000) 
                     for i in range(1, 5)}
    fl_system = MultiServiceFLSystem(service_configs, client_configs)
    
    # åˆ›å»ºPACè®­ç»ƒå™¨
    trainer = PACMCoFLTrainer(service_ids, environment, fl_system, config, constraints)
    
    print(f"åˆ›å»ºäº†åŒ…å«{len(trainer.agents)}ä¸ªPACæ™ºèƒ½ä½“çš„è®­ç»ƒå™¨")
    
    # æµ‹è¯•å•ä¸ªå›åˆ
    episode_rewards, episode_length, episode_info = trainer.train_episode()
    print(f"æµ‹è¯•å›åˆ: å¥–åŠ±={episode_rewards}, é•¿åº¦={episode_length}")
    print(f"ç´¯ç§¯å¥–åŠ±J_r(Ï€): {episode_info['cumulative_rewards']}")
    
    # æµ‹è¯•è®­ç»ƒ
    training_results = trainer.train()
    
    # æµ‹è¯•è¯„ä¼°
    eval_results = trainer.evaluate(num_episodes=3)
    print(f"è¯„ä¼°ç»“æœ: {eval_results['avg_cumulative_rewards']}")
    
    # è·å–è®­ç»ƒæ€»ç»“
    summary = trainer.get_training_summary()
    print(f"è®­ç»ƒæ€»ç»“é”®: {list(summary.keys())}")
    
    print("\nâœ… PAC-MCoFLæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_pac_mcofl()