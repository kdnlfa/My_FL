#!/usr/bin/env python3
"""
è®ºæ–‡å®éªŒå¤ç°æ¨¡å—

åŸºäºè®ºæ–‡çš„å®éªŒè®¾è®¡
å¤ç°åŒ…å«5ä¸ªå®¢æˆ·ç«¯ã€3ä¸ªæœåŠ¡æä¾›å•†çš„å¤šæœåŠ¡è”é‚¦å­¦ä¹ å®éªŒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field
import json
import os
from pathlib import Path
import time
import copy

# å¯¼å…¥ç°æœ‰æ¨¡å—
from multi_service_fl import MultiServiceFLSystem, ServiceProviderConfig, ClientResourceConfig, QuantizedFLModel
from optimization_problem import OptimizationConstraints
from mdp_framework import MultiServiceFLEnvironment, Action, Observation
from pac_mcofl import PACMCoFLTrainer, PACConfig
from quantization import QuantizationModule
from communication import SystemMetrics


@dataclass
class PaperExperimentConfig:
    """è®ºæ–‡å®éªŒé…ç½®ï¼Œä¸¥æ ¼æŒ‰ç…§è®ºæ–‡å‚æ•°è¡¨è®¾ç½®"""
    
    # åŸºæœ¬è®¾ç½®ï¼ˆè®ºæ–‡Table 1ï¼‰
    N: int = 5  # å®¢æˆ·ç«¯æ•°é‡
    R: int = 3  # ä»»åŠ¡æ•°é‡ï¼ˆæœåŠ¡æä¾›å•†æ•°é‡ï¼‰
    rho: float = 1.0  # éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ç¨‹åº¦ï¼ˆIIDï¼‰
    tau: int = 3  # è”é‚¦å­¦ä¹ æœ¬åœ°æ›´æ–°æ­¥æ•°
    # T: int = 35  # è”é‚¦å­¦ä¹ å…¨å±€è®­ç»ƒè½®æ¬¡
    T: int = 5  # è”é‚¦å­¦ä¹ å…¨å±€è®­ç»ƒè½®æ¬¡

    
    # é€šä¿¡å‚æ•°
    g_i_t_range: Tuple[float, float] = (-73.0, -63.0)  # ä¿¡é“å¢ç›Š [dB]
    N_0_range: Tuple[float, float] = (-174.0, -124.0)  # å™ªå£°åŠŸç‡è°±å¯†åº¦ [dBm/Hz]
    p_i_t_range: Tuple[float, float] = (10.0, 33.0)  # å®¢æˆ·ç«¯å‘å°„åŠŸç‡ [dBm]
    
    # è®¡ç®—å‚æ•°
    mu_i: float = 1e-27  # æœ‰æ•ˆåˆ‡æ¢ç”µå®¹å¸¸æ•°
    c_i_1_range: Tuple[float, float] = (6.07e5, 7.41e5)  # æ¯æ ·æœ¬CPUå‘¨æœŸæ¶ˆè€—é‡(r1)
    c_i_2_range: Tuple[float, float] = (6.07e5, 7.41e5)  # æ¯æ ·æœ¬CPUå‘¨æœŸæ¶ˆè€—é‡(r2)
    c_i_3_range: Tuple[float, float] = (1.10e8, 1.34e8)  # æ¯æ ·æœ¬CPUå‘¨æœŸæ¶ˆè€—é‡(r3)
    
    # æƒé‡å› å­ï¼ˆè®ºæ–‡æ–¹ç¨‹17çš„Ïƒå‚æ•°ï¼‰
    sigma_1_values: List[float] = field(default_factory=lambda: [100.0, 100.0, 100.0])  # r1,r2,r3
    sigma_2_values: List[float] = field(default_factory=lambda: [4.8, 31.25, 12.5])
    sigma_3_values: List[float] = field(default_factory=lambda: [0.8, 25.0, 16.6])
    sigma_4_values: List[float] = field(default_factory=lambda: [0.8, 25.0, 16.6])
    
    # PACç®—æ³•å‚æ•°
    zeta: float = 0.001  # è¡ŒåŠ¨è€…ç½‘ç»œå­¦ä¹ ç‡
    alpha: float = 0.001  # è¯„è®ºå®¶ç½‘ç»œå­¦ä¹ ç‡
    
    # æŠ–åŠ¨å› å­
    sigma_q: float = 0.25  # é‡åŒ–æŠ–åŠ¨å› å­
    sigma_f: float = 0.5  # CPUé¢‘ç‡æŠ–åŠ¨å› å­
    
    # ç³»ç»Ÿçº¦æŸ
    f_min: float = 0.5e9  # æœ€å°CPUé¢‘ç‡ [Hz]
    f_max: float = 3.5e9  # æœ€å¤§CPUé¢‘ç‡ [Hz]
    B_min: float = 0.0  # æœ€å°å¸¦å®½ [Hz]
    B_max: float = 30e6  # æœ€å¤§å¸¦å®½ [Hz]
    
    # æ•°æ®é›†é…ç½®
    batch_size: int = 64
    learning_rate: float = 0.001
    
    def get_service_sigma_values(self, service_id: int) -> Dict[str, float]:
        """è·å–ç‰¹å®šæœåŠ¡çš„æƒé‡å› å­"""
        idx = service_id - 1  # æœåŠ¡IDä»1å¼€å§‹ï¼Œæ•°ç»„ç´¢å¼•ä»0å¼€å§‹
        return {
            'sigma_1': self.sigma_1_values[idx],
            'sigma_2': self.sigma_2_values[idx], 
            'sigma_3': self.sigma_3_values[idx],
            'sigma_4': self.sigma_4_values[idx]
        }


class TaskModels:
    """è®ºæ–‡ä¸­ä¸‰ä¸ªä»»åŠ¡çš„æ¨¡å‹æ¶æ„å®šä¹‰"""
    
    @staticmethod
    def create_task_r1_model(num_classes: int = 10) -> nn.Module:
        """
        ä»»åŠ¡r1çš„æ¨¡å‹ï¼šå››å±‚å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ç”¨äºCIFAR-10
        å·ç§¯å±‚ï¼š48ã€96ã€192ã€256ä¸ªæ»¤æ³¢å™¨ï¼Œ3Ã—3æ ¸å°ºå¯¸
        å…¨è¿æ¥ï¼š512ã€64ã€10ä¸ªç¥ç»å…ƒå±‚
        """
        class TaskR1CNN(nn.Module):
            def __init__(self, num_classes=10):
                super(TaskR1CNN, self).__init__()
                
                # å››å±‚å·ç§¯ç½‘ç»œ
                self.conv1 = nn.Conv2d(3, 48, kernel_size=3, padding=1)
                self.conv2 = nn.Conv2d(48, 96, kernel_size=3, padding=1)
                self.conv3 = nn.Conv2d(96, 192, kernel_size=3, padding=1)
                self.conv4 = nn.Conv2d(192, 256, kernel_size=3, padding=1)
                
                # æ± åŒ–å±‚
                self.pool = nn.MaxPool2d(2, 2)
                
                # å…¨è¿æ¥ç½‘ç»œï¼š512ã€64ã€10ä¸ªç¥ç»å…ƒ
                self.fc1 = nn.Linear(256 * 2 * 2, 512)  # CIFAR-10æ˜¯32Ã—32ï¼Œç»è¿‡4æ¬¡æ± åŒ–åæ˜¯2Ã—2
                self.fc2 = nn.Linear(512, 64)
                self.fc3 = nn.Linear(64, num_classes)
                
                self.relu = nn.ReLU()
                self.dropout = nn.Dropout(0.3)
                
            def forward(self, x):
                # å·ç§¯å±‚
                x = self.pool(self.relu(self.conv1(x)))
                x = self.pool(self.relu(self.conv2(x)))
                x = self.pool(self.relu(self.conv3(x)))
                x = self.pool(self.relu(self.conv4(x)))
                
                # å±•å¹³
                x = x.view(-1, 256 * 2 * 2)
                
                # å…¨è¿æ¥å±‚
                x = self.relu(self.fc1(x))
                x = self.dropout(x)
                x = self.relu(self.fc2(x))
                x = self.dropout(x)
                x = self.fc3(x)
                
                return x
        
        return TaskR1CNN(num_classes)
    
    @staticmethod
    def create_task_r2_model(num_classes: int = 10) -> nn.Module:
        """
        ä»»åŠ¡r2çš„æ¨¡å‹ï¼šåŒå±‚CNNæ¶æ„ç”¨äºFashionMNIST
        å·ç§¯å±‚ï¼š32å’Œ64ä¸ª5Ã—5æ»¤æ³¢å™¨
        å…¨è¿æ¥ï¼š64å’Œ10ä¸ªç¥ç»å…ƒ
        """
        class TaskR2CNN(nn.Module):
            def __init__(self, num_classes=10):
                super(TaskR2CNN, self).__init__()
                
                # åŒå±‚å·ç§¯ç½‘ç»œ
                self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)  # FashionMNISTæ˜¯ç°åº¦å›¾
                self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)
                
                # æ± åŒ–å±‚
                self.pool = nn.MaxPool2d(2, 2)
                
                # å…¨è¿æ¥ç½‘ç»œï¼š64å’Œ10ä¸ªç¥ç»å…ƒ
                self.fc1 = nn.Linear(64 * 7 * 7, 64)  # FashionMNISTæ˜¯28Ã—28ï¼Œç»è¿‡2æ¬¡æ± åŒ–åæ˜¯7Ã—7
                self.fc2 = nn.Linear(64, num_classes)
                
                self.relu = nn.ReLU()
                self.dropout = nn.Dropout(0.3)
                
            def forward(self, x):
                # å·ç§¯å±‚
                x = self.pool(self.relu(self.conv1(x)))
                x = self.pool(self.relu(self.conv2(x)))
                
                # å±•å¹³
                x = x.view(-1, 64 * 7 * 7)
                
                # å…¨è¿æ¥å±‚
                x = self.relu(self.fc1(x))
                x = self.dropout(x)
                x = self.fc2(x)
                
                return x
        
        return TaskR2CNN(num_classes)
    
    @staticmethod 
    def create_task_r3_model(num_classes: int = 10) -> nn.Module:
        """
        ä»»åŠ¡r3çš„æ¨¡å‹ï¼šåŒå±‚å…¨è¿æ¥ç½‘ç»œç”¨äºMNIST
        å…¨è¿æ¥ï¼š128å’Œ10ä¸ªç¥ç»å…ƒ
        """
        class TaskR3MLP(nn.Module):
            def __init__(self, num_classes=10):
                super(TaskR3MLP, self).__init__()
                
                # åŒå±‚å…¨è¿æ¥ç½‘ç»œï¼š128å’Œ10ä¸ªç¥ç»å…ƒ
                self.fc1 = nn.Linear(28 * 28, 128)  # MNISTæ˜¯28Ã—28ç°åº¦å›¾
                self.fc2 = nn.Linear(128, num_classes)
                
                self.relu = nn.ReLU()
                self.dropout = nn.Dropout(0.3)
                
            def forward(self, x):
                # å±•å¹³è¾“å…¥
                x = x.view(-1, 28 * 28)
                
                # å…¨è¿æ¥å±‚
                x = self.relu(self.fc1(x))
                x = self.dropout(x)
                x = self.fc2(x)
                
                return x
        
        return TaskR3MLP(num_classes)


class DatasetLoader:
    """è®ºæ–‡å®éªŒæ•°æ®é›†åŠ è½½å™¨ï¼Œæ”¯æŒIIDåˆ†å¸ƒ"""
    
    def __init__(self, data_dir: str = "../data"):
        """
        åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨
        
        å‚æ•°:
            data_dir: æ•°æ®é›†å­˜å‚¨ç›®å½•ï¼ˆä½¿ç”¨ä¸Šçº§ç›®å½•çš„dataæ–‡ä»¶å¤¹ï¼‰
        """
        self.data_dir = Path(data_dir)
        if not self.data_dir.exists():
            # å¦‚æœä¸Šçº§ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰ç›®å½•
            self.data_dir = Path("./data")
            self.data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é¢„å¤„ç†é…ç½®
        self.transform_configs = {
            'cifar10': {
                'train': transforms.Compose([
                    transforms.RandomHorizontalFlip(),
                    transforms.RandomCrop(32, padding=4),
                    transforms.ToTensor(),
                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
                ]),
                'test': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
                ])
            },
            'fashionmnist': {
                'train': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.2860,), (0.3530,))
                ]),
                'test': transforms.Compose([
                    transforms.ToTensor(), 
                    transforms.Normalize((0.2860,), (0.3530,))
                ])
            },
            'mnist': {
                'train': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.1307,), (0.3081,))
                ]),
                'test': transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.1307,), (0.3081,))
                ])
            }
        }
    
    def load_datasets(self) -> Dict[str, Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]]:
        """
        åŠ è½½ä¸‰ä¸ªæ•°æ®é›†ï¼šCIFAR-10ã€FashionMNISTã€MNIST
        
        è¿”å›:
            åŒ…å«è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†çš„å­—å…¸
        """
        datasets = {}
        
        # CIFAR-10 (ä»»åŠ¡r1)
        cifar10_train = torchvision.datasets.CIFAR10(
            root=str(self.data_dir), train=True, download=False,
            transform=self.transform_configs['cifar10']['train']
        )
        cifar10_test = torchvision.datasets.CIFAR10(
            root=str(self.data_dir), train=False, download=False,
            transform=self.transform_configs['cifar10']['test']
        )
        datasets['cifar10'] = (cifar10_train, cifar10_test)
        
        # FashionMNIST (ä»»åŠ¡r2)
        fashionmnist_train = torchvision.datasets.FashionMNIST(
            root=str(self.data_dir), train=True, download=False,
            transform=self.transform_configs['fashionmnist']['train']
        )
        fashionmnist_test = torchvision.datasets.FashionMNIST(
            root=str(self.data_dir), train=False, download=False,
            transform=self.transform_configs['fashionmnist']['test']
        )
        datasets['fashionmnist'] = (fashionmnist_train, fashionmnist_test)
        
        # MNIST (ä»»åŠ¡r3)
        mnist_train = torchvision.datasets.MNIST(
            root=str(self.data_dir), train=True, download=False,
            transform=self.transform_configs['mnist']['train']
        )
        mnist_test = torchvision.datasets.MNIST(
            root=str(self.data_dir), train=False, download=False,
            transform=self.transform_configs['mnist']['test']
        )
        datasets['mnist'] = (mnist_train, mnist_test)
        
        return datasets
    

    def create_iid_client_splits(self, 
                                datasets: Dict[str, Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]],
                                num_clients: int = 5,
                                rho: float = 1.0) -> Dict[int, Dict[str, torch.utils.data.Dataset]]:
        """
        åˆ›å»ºæœåŠ¡æ„ŸçŸ¥çš„IIDæ•°æ®åˆ†å‰²ï¼š
        CIFAR-10 -> å®¢æˆ·ç«¯1,2
        FashionMNIST -> å®¢æˆ·ç«¯3,4
        MNIST -> å®¢æˆ·ç«¯5
        é¿å…åŸå®ç°æŠŠæ¯ä¸ªæ•°æ®é›†åˆ‡æˆ5ä»½åªç”¨å…¶ä¸­éƒ¨åˆ†ï¼Œå¯¼è‡´æ•°æ®æµªè´¹ã€‚
        """
        mapping = {
            'cifar10': [1, 2],
            'fashionmnist': [3, 4],
            'mnist': [5]
        }
        client_datasets = {i: {} for i in range(1, num_clients + 1)}
        
        for dataset_name, (train_ds, test_ds) in datasets.items():
            target_clients = mapping.get(dataset_name, [])
            if not target_clients:
                continue
            train_size = len(train_ds)
            per_client = train_size // len(target_clients)
            torch.manual_seed(42)  # å¯é‡ç°
            indices = torch.randperm(train_size).tolist()
            
            for idx, client_id in enumerate(target_clients):
                start = idx * per_client
                end = start + per_client if idx < len(target_clients) - 1 else train_size
                sub_idx = indices[start:end]
                client_datasets[client_id][dataset_name] = torch.utils.data.Subset(train_ds, sub_idx)
        
        print(f"âœ… æœåŠ¡æ„ŸçŸ¥IIDæ•°æ®åˆ†å‰²å®Œæˆ (Ï={rho})")
        for cid, ds_dict in client_datasets.items():
            if not ds_dict:
                continue
            total = sum(len(ds) for ds in ds_dict.values())
            print(f"  å®¢æˆ·ç«¯ {cid}: {total} æ ·æœ¬ -> {[f'{k}:{len(v)}' for k,v in ds_dict.items()]}")
        return client_datasets


class PaperExperimentRunner:
    """è®ºæ–‡å®éªŒæ‰§è¡Œå™¨ï¼Œé›†æˆæ‰€æœ‰ç»„ä»¶è¿›è¡Œå®Œæ•´å®éªŒ"""
    
    def __init__(self, config: PaperExperimentConfig, output_dir: str = "./experimental_data"):
        """
        åˆå§‹åŒ–å®éªŒæ‰§è¡Œå™¨
        
        å‚æ•°:
            config: è®ºæ–‡å®éªŒé…ç½®
            output_dir: å®éªŒç»“æœè¾“å‡ºç›®å½•
        """
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.dataset_loader = DatasetLoader()
        self.datasets = None
        self.client_datasets = None
        self.fl_system = None
        self.pac_trainer = None
        self.environment = None
        
        # å®éªŒç»“æœå­˜å‚¨
        self.experiment_results = {}
        
        print(f"ğŸ“Š è®ºæ–‡å®éªŒæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   å®¢æˆ·ç«¯æ•°é‡: {self.config.N}")
        print(f"   æœåŠ¡æä¾›å•†æ•°é‡: {self.config.R}")
        print(f"   è®­ç»ƒè½®æ¬¡: {self.config.T}")
    
    def setup_datasets(self):
        """è®¾ç½®å’Œåˆ†å‰²æ•°æ®é›†"""
        print("\nğŸ”„ è®¾ç½®æ•°æ®é›†...")
        
        # åŠ è½½åŸå§‹æ•°æ®é›†
        self.datasets = self.dataset_loader.load_datasets()
        
        # åˆ›å»ºIIDå®¢æˆ·ç«¯åˆ†å‰²
        self.client_datasets = self.dataset_loader.create_iid_client_splits(
            self.datasets, 
            num_clients=self.config.N,
            rho=self.config.rho
        )
    
    def setup_models_and_services(self):
        """è®¾ç½®æ¨¡å‹å’ŒæœåŠ¡æä¾›å•†é…ç½®"""
        print("\nğŸ”„ è®¾ç½®æ¨¡å‹å’ŒæœåŠ¡...")
        
        # æŒ‰ç…§è®ºæ–‡è®¾è®¡åˆ›å»ºæœåŠ¡æä¾›å•†é…ç½®
        service_configs = []
        
        # æœåŠ¡1ï¼šCIFAR-10ä»»åŠ¡ï¼Œå®¢æˆ·ç«¯1-2
        service_configs.append(ServiceProviderConfig(
            service_id=1,
            name="CIFAR-10åˆ†ç±»æœåŠ¡",
            client_ids=[1, 2],
            model_architecture={
                "type": "task_r1_cnn",
                "description": "å››å±‚CNNï¼š48,96,192,256æ»¤æ³¢å™¨+512,64,10å…¨è¿æ¥",
                "dataset": "cifar10"
            },
            quantization_level=8,
            local_epochs=self.config.tau,
            learning_rate=self.config.learning_rate
        ))
        
        # æœåŠ¡2ï¼šFashionMNISTä»»åŠ¡ï¼Œå®¢æˆ·ç«¯3-4
        service_configs.append(ServiceProviderConfig(
            service_id=2,
            name="FashionMNISTåˆ†ç±»æœåŠ¡", 
            client_ids=[3, 4],
            model_architecture={
                "type": "task_r2_cnn",
                "description": "åŒå±‚CNNï¼š32,64æ»¤æ³¢å™¨+64,10å…¨è¿æ¥",
                "dataset": "fashionmnist"
            },
            quantization_level=8,
            local_epochs=self.config.tau,
            learning_rate=self.config.learning_rate
        ))
        
        # æœåŠ¡3ï¼šMNISTä»»åŠ¡ï¼Œå®¢æˆ·ç«¯5
        service_configs.append(ServiceProviderConfig(
            service_id=3,
            name="MNISTåˆ†ç±»æœåŠ¡",
            client_ids=[5],
            model_architecture={
                "type": "task_r3_mlp", 
                "description": "åŒå±‚MLPï¼š128,10ç¥ç»å…ƒ",
                "dataset": "mnist"
            },
            quantization_level=8,
            local_epochs=self.config.tau,
            learning_rate=self.config.learning_rate
        ))
        
        # åˆ›å»ºå®¢æˆ·ç«¯èµ„æºé…ç½®ï¼ˆæŒ‰ç…§è®ºæ–‡å‚æ•°è¡¨ï¼‰
        client_configs = {}
        for client_id in range(1, self.config.N + 1):
            # æ ¹æ®å®¢æˆ·ç«¯æ‰€å±æœåŠ¡ç¡®å®šè®¡ç®—å¤æ‚åº¦
            if client_id in [1, 2]:  # CIFAR-10å®¢æˆ·ç«¯
                c_ir_range = self.config.c_i_1_range
            elif client_id in [3, 4]:  # FashionMNISTå®¢æˆ·ç«¯  
                c_ir_range = self.config.c_i_2_range
            else:  # MNISTå®¢æˆ·ç«¯
                c_ir_range = self.config.c_i_3_range
            
            # éšæœºé‡‡æ ·å‚æ•°ï¼ˆåœ¨è®ºæ–‡ç»™å®šèŒƒå›´å†…ï¼‰
            np.random.seed(42 + client_id)  # ç¡®ä¿å¯é‡ç°æ€§
            
            client_configs[client_id] = ClientResourceConfig(
                client_id=client_id,
                mu_i=self.config.mu_i,
                c_ir=np.random.uniform(*c_ir_range),
                max_frequency=self.config.f_max,
                max_power=10**(np.random.uniform(*self.config.p_i_t_range) / 10),  # dBmè½¬ç“¦ç‰¹
                channel_gain=10**(np.random.uniform(*self.config.g_i_t_range) / 10),  # dBè½¬çº¿æ€§
                dataset_size=1000  # å‡è®¾æ¯ä¸ªå®¢æˆ·ç«¯æœ‰1000ä¸ªæ ·æœ¬
            )
        
        # åˆ›å»ºå¤šæœåŠ¡FLç³»ç»Ÿ
        self.fl_system = MultiServiceFLSystem(
            service_configs,
            client_configs,
            debug_disable_quant=False,   # æš‚æ—¶å…³é—­é‡åŒ–å½±å“ï¼Œç¡®ä¿å…ˆæŠŠåŸºç¡€è®­ç»ƒè·‘é€š
            debug_verbose=False        # æ‰“å°æ›´å¤šè°ƒè¯•ä¿¡æ¯
        )
        
        print(f"âœ… åˆ›å»ºäº† {len(service_configs)} ä¸ªæœåŠ¡æä¾›å•†")
        for config in service_configs:
            print(f"   æœåŠ¡{config.service_id}: {config.name}, å®¢æˆ·ç«¯{config.client_ids}")
    
    def setup_models_for_services(self):
        """ä¸ºæ¯ä¸ªæœåŠ¡è®¾ç½®å…·ä½“çš„æ¨¡å‹å’Œæ•°æ®"""
        print("\nğŸ”„ ä¸ºæœåŠ¡è®¾ç½®æ¨¡å‹...")
        
        # æœåŠ¡1ï¼šCIFAR-10
        model_r1 = TaskModels.create_task_r1_model(num_classes=10)
        service1_train_datasets = {
            client_id: self.client_datasets[client_id]['cifar10'] 
            for client_id in [1, 2]
        }
        self.fl_system.setup_service(
            service_id=1,
            model=model_r1,
            train_datasets=service1_train_datasets
        )
        print(f"âœ… æœåŠ¡1è®¾ç½®å®Œæˆï¼šCIFAR-10ä»»åŠ¡ï¼Œå››å±‚CNNæ¨¡å‹")
        
        # æœåŠ¡2ï¼šFashionMNIST
        model_r2 = TaskModels.create_task_r2_model(num_classes=10)
        service2_train_datasets = {
            client_id: self.client_datasets[client_id]['fashionmnist']
            for client_id in [3, 4]
        }
        self.fl_system.setup_service(
            service_id=2,
            model=model_r2,
            train_datasets=service2_train_datasets
        )
        print(f"âœ… æœåŠ¡2è®¾ç½®å®Œæˆï¼šFashionMNISTä»»åŠ¡ï¼ŒåŒå±‚CNNæ¨¡å‹")
        
        # æœåŠ¡3ï¼šMNIST
        model_r3 = TaskModels.create_task_r3_model(num_classes=10)
        service3_train_datasets = {
            client_id: self.client_datasets[client_id]['mnist']
            for client_id in [5]
        }
        self.fl_system.setup_service(
            service_id=3,
            model=model_r3,
            train_datasets=service3_train_datasets
        )
        print(f"âœ… æœåŠ¡3è®¾ç½®å®Œæˆï¼šMNISTä»»åŠ¡ï¼ŒåŒå±‚MLPæ¨¡å‹")
    
    def setup_pac_environment(self):
        """è®¾ç½®PAC-MCoFLç¯å¢ƒå’Œè®­ç»ƒå™¨"""
        print("\nğŸ”„ è®¾ç½®PAC-MCoFLç¯å¢ƒ...")
        
        # åˆ›å»ºçº¦æŸæ¡ä»¶ï¼ˆåŸºäºè®ºæ–‡å‚æ•°ï¼‰
        constraints = OptimizationConstraints(
            max_energy=0.1,  # 100 mJ
            max_delay=5.0,   # 5ç§’
            max_clients=self.config.N,
            min_frequency=self.config.f_min,
            max_frequency=self.config.f_max,
            min_bandwidth=self.config.B_min,
            max_bandwidth=self.config.B_max
        )
        
        # åˆ›å»ºç¯å¢ƒ
        service_ids = [1, 2, 3]
        
        # ä¸ºæ¯ä¸ªæœåŠ¡åˆ›å»ºæƒé‡é…ç½®ï¼ˆåŸºäºè®ºæ–‡Ïƒå‚æ•°ï¼‰
        per_service_reward_weights = {}
        for service_id in service_ids:
            sigma_values = self.config.get_service_sigma_values(service_id)
            # ç¯å¢ƒæ”¯æŒä¼ å…¥æŒ‰æœåŠ¡IDï¼ˆå­—ç¬¦ä¸²ï¼‰ç´¢å¼•çš„æƒé‡è¡¨
            per_service_reward_weights[str(service_id)] = sigma_values
        
        self.environment = MultiServiceFLEnvironment(
            service_ids=service_ids,
            constraints=constraints,
            max_rounds=self.config.T,
            reward_weights=per_service_reward_weights  # ä¸ºæ¯ä¸ªæœåŠ¡åˆ†åˆ«è®¾ç½®å¥–åŠ±æƒé‡
        )

        
        
        # åˆ›å»ºPACé…ç½®ï¼ˆåŸºäºè®ºæ–‡PACç®—æ³•å‚æ•°ï¼‰
        pac_config = PACConfig(
            num_episodes=2,  # è°ƒè¯•è”åŠ¨ï¼šå…ˆè·‘å°‘é‡episodeéªŒè¯
            max_rounds_per_episode=15,  # å‡å°‘æ¯episodeè½®æ•°ä»¥åŠ é€Ÿ
            buffer_size=2000,
            batch_size=8,
            actor_hidden_dim=64,    # è®ºæ–‡ï¼šç­–ç•¥ç½‘ç»œ64-128-64
            critic_hidden_dim=64,   # è®ºæ–‡ï¼šQç½‘ç»œ64-128
            num_layers=3,
            actor_lr=self.config.zeta,   # è®ºæ–‡ï¼šÎ¶=0.001
            critic_lr=self.config.alpha, # è®ºæ–‡ï¼šÎ±=0.001
            gamma=0.95,
            joint_action_samples=100,
            update_frequency=4,
            # ä¸‰ä»½è®­ç»ƒæ–¹æ¡ˆï¼ˆæŒ‰æœåŠ¡ï¼‰
            # æ–¹æ¡ˆAï¼ˆæœåŠ¡1ï¼ŒCIFAR-10ï¼‰: å‡†ç¡®ç‡ä¼˜å…ˆ + ç¨³å®šæ€§ï¼Œæå‡è®­ç»ƒé‡ä¸èµ„æºä¸‹é™ï¼Œè¯„ä¼°æ¯æ­¥
            step_eval_frequency=1,
            service_eval_frequency={1: 1, 2: 1, 3: 2},
            service_epochs_per_step={1: 5, 2: 3, 3: 1},
            service_action_floors={
                1: { 'min_clients': 2, 'min_frequency': 1.5e9, 'min_bandwidth': 15e6, 'min_quantization': 8 },
                2: { 'min_clients': 2, 'min_frequency': 1.2e9, 'min_bandwidth': 10e6, 'min_quantization': 6 },
                3: { 'min_clients': 1, 'min_frequency': 1.0e9, 'min_bandwidth': 5e6,  'min_quantization': 4 }
            }
        )

        # pac_config = PACConfig(
        #     num_episodes=5,  # è°ƒè¯•è”åŠ¨ï¼šå…ˆè·‘å°‘é‡episodeéªŒè¯
        #     max_rounds_per_episode=10,  # å‡å°‘æ¯episodeè½®æ•°ä»¥åŠ é€Ÿ
        #     buffer_size=10000,
        #     batch_size=64,
        #     actor_hidden_dim=64,    # è®ºæ–‡ï¼šç­–ç•¥ç½‘ç»œ64-128-64
        #     critic_hidden_dim=64,   # è®ºæ–‡ï¼šQç½‘ç»œ64-128
        #     num_layers=3,
        #     actor_lr=self.config.zeta,   # è®ºæ–‡ï¼šÎ¶=0.001
        #     critic_lr=self.config.alpha, # è®ºæ–‡ï¼šÎ±=0.001
        #     gamma=0.95,
        #     joint_action_samples=100,
        #     update_frequency=4,
        #     # ä¸‰ä»½è®­ç»ƒæ–¹æ¡ˆï¼ˆæŒ‰æœåŠ¡ï¼‰
        #     # æ–¹æ¡ˆAï¼ˆæœåŠ¡1ï¼ŒCIFAR-10ï¼‰: å‡†ç¡®ç‡ä¼˜å…ˆ + ç¨³å®šæ€§ï¼Œæå‡è®­ç»ƒé‡ä¸èµ„æºä¸‹é™ï¼Œè¯„ä¼°æ¯æ­¥
        #     step_eval_frequency=1,
        #     service_eval_frequency={1: 1, 2: 1, 3: 2},
        #     service_epochs_per_step={1: 5, 2: 3, 3: 1},
        #     service_action_floors={
        #         1: { 'min_clients': 2, 'min_frequency': 1.5e9, 'min_bandwidth': 15e6, 'min_quantization': 8 },
        #         2: { 'min_clients': 2, 'min_frequency': 1.2e9, 'min_bandwidth': 10e6, 'min_quantization': 6 },
        #         3: { 'min_clients': 1, 'min_frequency': 1.0e9, 'min_bandwidth': 5e6,  'min_quantization': 4 }
        #     }
        # )

        
        # åˆ›å»ºPACè®­ç»ƒå™¨
        self.pac_trainer = PACMCoFLTrainer(
            service_ids=service_ids,
            environment=self.environment,
            fl_system=self.fl_system,
            config=pac_config,
            constraints=constraints
        )
        
        print(f"âœ… PAC-MCoFLç¯å¢ƒè®¾ç½®å®Œæˆ")
        print(f"   Qç½‘ç»œæ¶æ„: {pac_config.critic_hidden_dim}-128èŠ‚ç‚¹ï¼ˆåŒå±‚ï¼‰")
        print(f"   ç­–ç•¥ç½‘ç»œæ¶æ„: 64-128-64èŠ‚ç‚¹ï¼ˆä¸‰å±‚ï¼‰") 
        print(f"   å­¦ä¹ ç‡: æ¼”å‘˜={pac_config.actor_lr}, è¯„è®ºå®¶={pac_config.critic_lr}")
    
    def run_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è®ºæ–‡å®éªŒ"""
        print("\nğŸš€ å¼€å§‹è®ºæ–‡å®éªŒå¤ç°...")
        
        start_time = time.time()
        
        try:
            # 1. è®¾ç½®æ•°æ®é›†
            self.setup_datasets()
            
            # 2. è®¾ç½®æ¨¡å‹å’ŒæœåŠ¡
            self.setup_models_and_services()
            
            # 3. ä¸ºæœåŠ¡è®¾ç½®å…·ä½“æ¨¡å‹
            self.setup_models_for_services()
            
            # 4. è®¾ç½®PACç¯å¢ƒ
            self.setup_pac_environment()
            
            # 5. å…ˆè¿è¡Œä¸€æ¬¡çœŸå®çš„è”é‚¦å­¦ä¹ åŸºçº¿è®­ç»ƒ(é¢„çƒ­)ï¼Œç¡®ä¿æ¨¡å‹æƒé‡å‘ç”Ÿæœ‰æ•ˆæ›´æ–°
            # baseline_plan = {1: 5, 2: 3, 3: 1}
            # print(f"\nğŸ”„ å…ˆè¿›è¡ŒåŸºçº¿è”é‚¦è®­ç»ƒ(é¢„çƒ­): {baseline_plan} è½®ï¼Œç¦ç”¨é‡åŒ–è¯„ä¼°å¹²æ‰°...")
            # saved_epochs = {}
            # for sid, trainer in self.fl_system.service_trainers.items():
            #     if hasattr(trainer, 'cfg'):
            #         saved_epochs[sid] = getattr(trainer.cfg, 'epochs', 1)
            #         trainer.cfg.epochs = max(3, saved_epochs[sid])
            # baseline_fl_training = {}
            # for sid, rounds in baseline_plan.items():
            #     try:
            #         self.fl_system.train_service(sid, num_rounds=rounds, enable_metrics=False)
            #         baseline_fl_training[sid] = (self.fl_system.service_models[sid], {'rounds': rounds})
            #     except Exception as e:
            #         print(f"[WARN] åŸºçº¿é¢„çƒ­å¤±è´¥ - æœåŠ¡{sid}: {e}")
            # for sid, trainer in self.fl_system.service_trainers.items():
            #     try:
            #         if hasattr(trainer, 'cfg') and sid in saved_epochs:
            #             trainer.cfg.epochs = saved_epochs[sid]
            #     except Exception:
            #         pass

            baseline_fl_training = {}

            # 6. è¿è¡ŒPAC-MCoFLè®­ç»ƒ
            print(f"\nğŸ”„ å¼€å§‹PAC-MCoFLè®­ç»ƒ...")
            training_results = self.pac_trainer.train()
            # è®­ç»ƒå®Œæˆåç«‹å³æŒä¹…åŒ–ä¸€æ¬¡åŸå§‹è®­ç»ƒç»“æœå¿«ç…§ï¼Œä¾¿äºæ’æŸ¥å¥–åŠ±ç›´çº¿é—®é¢˜
            try:
                snapshot_path = self.output_dir / f"training_results_snapshot_{time.strftime('%Y%m%d_%H%M%S')}.json"
                with open(snapshot_path, 'w', encoding='utf-8') as sf:
                    import json as _json
                    def _to_serializable(o):
                        import numpy as _np
                        if isinstance(o, _np.ndarray):
                            return o.tolist()
                        if isinstance(o, (float, int, str, bool)) or o is None:
                            return o
                        if isinstance(o, dict):
                            return {k: _to_serializable(v) for k, v in o.items()}
                        if isinstance(o, list):
                            return [_to_serializable(x) for x in o]
                        return str(o)
                    _json.dump(_to_serializable(training_results), sf, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾  å·²ä¿å­˜è®­ç»ƒå¿«ç…§: {snapshot_path}")
            except Exception as e:
                print(f"[WARN] ä¿å­˜è®­ç»ƒå¿«ç…§å¤±è´¥: {e}")
            
            # 7. è¯„ä¼°è®­ç»ƒç»“æœï¼ˆRLå±‚é¢ï¼‰
            print(f"\nğŸ”„ è¯„ä¼°è®­ç»ƒç»“æœ...")
            evaluation_results = self.pac_trainer.evaluate(num_episodes=10)
            
            # 8. è·å–è®­ç»ƒæ€»ç»“
            training_summary = self.pac_trainer.get_training_summary()
            
            # 9. è¯„ä¼°è”é‚¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ï¼ˆçœŸå®æ¨¡å‹ï¼‰
            print(f"\nğŸ”„ è¯„ä¼°è”é‚¦å­¦ä¹ æ¨¡å‹æ€§èƒ½...")
            model_performance = self.evaluate_model_performance()
            
            # 10. ç¼–è¯‘å®éªŒç»“æœ
            end_time = time.time()
            
            self.experiment_results = {
                'config': {
                    'N': self.config.N,
                    'R': self.config.R,
                    'T': self.config.T,
                    'tau': self.config.tau,
                    'rho': self.config.rho,
                    'learning_rates': {
                        'fl': self.config.learning_rate,
                        'actor': self.config.zeta,
                        'critic': self.config.alpha
                    },
                    'model_architectures': {
                        'r1': "å››å±‚CNN(48,96,192,256)+FC(512,64,10)",
                        'r2': "åŒå±‚CNN(32,64)+FC(64,10)", 
                        'r3': "åŒå±‚MLP(128,10)"
                    }
                },
                'baseline_fl_training': {
                    sid: {
                        'metrics': metrics,
                        'has_model': sid in self.fl_system.service_models
                    } for sid, (model, metrics) in baseline_fl_training.items()
                },
                'training_results': training_results,
                'evaluation_results': evaluation_results,
                'model_performance': model_performance,  # æ¨¡å‹æ€§èƒ½ç»“æœ
                'training_summary': training_summary,
                'experiment_duration': end_time - start_time,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # 9. ä¿å­˜å®éªŒç»“æœ
            self.save_results()
            
            print(f"\nâœ… è®ºæ–‡å®éªŒå¤ç°å®Œæˆï¼")
            print(f"   æ€»ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"   ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            
            return self.experiment_results
            
        except Exception as e:
            print(f"\nâŒ å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}
    
    def evaluate_model_performance(self) -> Dict[str, Dict[str, float]]:
        """
        è¯„ä¼°ä¸‰ä¸ªæ¨¡å‹åœ¨å„è‡ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½
        è¿”å›æ¯ä¸ªæœåŠ¡çš„å‡†ç¡®ç‡ã€æŸå¤±ç­‰æŒ‡æ ‡
        """
        results = {}
        
        print("ğŸ“Š å¼€å§‹è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹æ€§èƒ½...")
        print("-" * 50)
        
        for service_id in range(1, 4):  # ä¸‰ä¸ªæœåŠ¡
            service_name = {1: "CIFAR-10", 2: "FashionMNIST", 3: "MNIST"}[service_id]
            dataset_name = {1: "cifar10", 2: "fashionmnist", 3: "mnist"}[service_id]
            
            print(f"\nğŸ” è¯„ä¼°æœåŠ¡ {service_id} ({service_name}):")
            
            try:
                # âœ… ä½¿ç”¨GPUè®¾å¤‡ä¿æŒä¸è®­ç»ƒä¸€è‡´
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                print(f"   ğŸ”§ è¯„ä¼°è®¾å¤‡: {device}")
                
                # âœ… ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è€Œä¸æ˜¯éšæœºåˆå§‹åŒ–çš„æ¨¡å‹
                if hasattr(self.fl_system, 'service_models') and service_id in self.fl_system.service_models:
                    # è·å–è®­ç»ƒåçš„é‡åŒ–æ¨¡å‹
                    fl_model = self.fl_system.service_models[service_id]
                    model = fl_model.model  # æå–PyTorchæ¨¡å‹
                    
                    # æ£€æŸ¥æ¨¡å‹å½“å‰è®¾å¤‡å¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    current_device = next(model.parameters()).device
                    if current_device != device:
                        model = model.to(device)
                        print(f"   ğŸ”„ æ¨¡å‹ä» {current_device} ç§»åŠ¨åˆ° {device}")
                    else:
                        print(f"   âœ… ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹ (è®¾å¤‡: {device})")
                else:
                    # å¦‚æœæ²¡æœ‰è®­ç»ƒåçš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹ä½œä¸ºåå¤‡
                    print(f"   âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°è®­ç»ƒåçš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
                    if service_id == 1:
                        model = TaskModels.create_task_r1_model()
                    elif service_id == 2:
                        model = TaskModels.create_task_r2_model()
                    else:
                        model = TaskModels.create_task_r3_model()
                    model = model.to(device)
                
                # è·å–æµ‹è¯•æ•°æ®
                test_dataset = self.datasets[dataset_name][1]  # [1]æ˜¯æµ‹è¯•é›†
                test_loader = torch.utils.data.DataLoader(
                    test_dataset, batch_size=64, shuffle=False
                )
                
                # è¯„ä¼°æ¨¡å‹
                model.eval()
                correct = 0
                total = 0
                total_loss = 0.0
                criterion = nn.CrossEntropyLoss()
                
                with torch.no_grad():
                    batch_count = 0
                    for data, target in test_loader:
                        batch_count += 1
                        
                        # ç¡®ä¿æ•°æ®ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                        data = data.to(device)
                        target = target.to(device)
                        
                        # å¯¹äºMNISTï¼Œéœ€è¦å±•å¹³è¾“å…¥
                        if dataset_name == "mnist":
                            data = data.view(data.size(0), -1)
                        
                        output = model(data)
                        loss = criterion(output, target)
                        total_loss += loss.item()
                        
                        pred = output.argmax(dim=1, keepdim=True)
                        correct += pred.eq(target.view_as(pred)).sum().item()
                        total += target.size(0)
                        
                        # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´
                        # if batch_count >= 20:  # åªè¯„ä¼°å‰20ä¸ªbatch
                        #     break
                
                accuracy = 100. * correct / total if total > 0 else 0.0
                avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')
                
                results[service_id] = {
                    'dataset': service_name,
                    'accuracy': accuracy,
                    'loss': avg_loss,
                    'correct': correct,
                    'total': total,
                    'batches_evaluated': batch_count
                }
                
                print(f"   âœ… å‡†ç¡®ç‡: {accuracy:.2f}% ({correct}/{total})")
                print(f"   âœ… å¹³å‡æŸå¤±: {avg_loss:.4f}")
                print(f"   âœ… è¯„ä¼°æ‰¹æ¬¡: {batch_count}")
                
            except Exception as e:
                print(f"   âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
                results[service_id] = {
                    'dataset': service_name,
                    'accuracy': 0.0,
                    'loss': float('inf'),
                    'error': str(e)
                }
        
        return results

    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / f"paper_experiment_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            # å¤„ç†numpyç±»å‹åºåˆ—åŒ–
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.int64, np.int32)):
                    return int(obj)
                elif isinstance(obj, (np.float64, np.float32)):
                    return float(obj)
                return obj
            
            def serialize_dict(d):
                if isinstance(d, dict):
                    return {k: serialize_dict(v) for k, v in d.items()}
                elif isinstance(d, list):
                    return [serialize_dict(item) for item in d]
                else:
                    return convert_numpy(d)
            
            serializable_results = serialize_dict(self.experiment_results)
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜: {results_file}")

        # é™„åŠ ï¼šä¿å­˜å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨ä½œæ—¥å¿—ä¸å‡†ç¡®ç‡è¶‹åŠ¿
        try:
            if 'training_results' in self.experiment_results:
                tr = self.experiment_results['training_results']
                # ä¿å­˜åŠ¨ä½œæ—¥å¿—
                if 'action_logs' in tr:
                    rl_logs_file = self.output_dir / f"rl_action_logs_{timestamp}.json"
                    with open(rl_logs_file, 'w', encoding='utf-8') as rf:
                        json.dump(tr['action_logs'], rf, indent=2, ensure_ascii=False)
                    print(f"ğŸ’¾ å¼ºåŒ–å­¦ä¹ åŠ¨ä½œæ—¥å¿—å·²ä¿å­˜: {rl_logs_file}")
        except Exception as e:
            print(f"[WARN] ä¿å­˜RLè¿‡ç¨‹æ—¥å¿—å¤±è´¥: {e}")

        # è¿½åŠ ï¼šç»˜åˆ¶å¹¶ä¿å­˜ä¸‰ä¸ªæœåŠ¡çš„å‡†ç¡®ç‡è¶‹åŠ¿å›¾ï¼ˆæ¥è‡ªRLå¿«é€Ÿè¯„ä¼°ç»“æœï¼‰
        try:
            import matplotlib
            matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
            import matplotlib.pyplot as plt

            tr = self.experiment_results.get('training_results', {})
            acc_trends = tr.get('accuracy_trends', {})
            if acc_trends:
                fig, ax = plt.subplots(figsize=(8, 4))
                for sid_str, series in acc_trends.items():
                    try:
                        sid = int(sid_str)
                    except Exception:
                        sid = sid_str
                    ax.plot(range(1, len(series) + 1), series, label=f'Service {sid}')
                ax.set_xlabel('RL steps')
                ax.set_ylabel('Accuracy (quick eval)')
                ax.set_title('Accuracy Trend')
                ax.legend()
                ax.grid(True, alpha=0.3)
                fig.tight_layout()
                fig_path = self.output_dir / f"accuracy_trend_rl_{timestamp}.png"
                fig.savefig(fig_path)
                plt.close(fig)
                print(f"ğŸ–¼ï¸  å‡†ç¡®ç‡è¶‹åŠ¿å›¾å·²ä¿å­˜: {fig_path}")

            # æ–°å¢ï¼šç»˜åˆ¶å¥–åŠ±å€¼éšè®­ç»ƒè¿‡ç¨‹çš„å˜åŒ–ï¼ˆæ¯å›åˆå¥–åŠ±ï¼‰
            episode_rewards = tr.get('episode_rewards', {})
            if episode_rewards:
                fig_r, ax_r = plt.subplots(figsize=(8, 4))
                for sid_str, rewards in episode_rewards.items():
                    try:
                        sid = int(sid_str)
                    except Exception:
                        sid = sid_str
                    x = list(range(1, len(rewards) + 1))
                    ax_r.plot(x, rewards, marker='o', label=f'Service {sid}')
                ax_r.set_xlabel('Episode')
                ax_r.set_ylabel('Reward')
                ax_r.set_title('Episode Reward Trend')
                ax_r.legend()
                ax_r.grid(True, alpha=0.3)
                fig_r.tight_layout()
                reward_fig_path = self.output_dir / f"rl_reward_trends_{timestamp}.png"
                fig_r.savefig(reward_fig_path)
                plt.close(fig_r)
                print(f"ğŸ–¼ï¸  å¥–åŠ±è¶‹åŠ¿å›¾å·²ä¿å­˜: {reward_fig_path}")

            # æ–°å¢ï¼šç»˜åˆ¶æŒ‰æ­¥å¥–åŠ±è¶‹åŠ¿ï¼ˆå°†æ¯ä¸ªepisodeçš„é€æ­¥å¥–åŠ±æ‹¼æ¥ï¼‰
            step_logs = tr.get('action_logs', [])
            if step_logs:
                # step_logs: List[episode] -> episode: List[{step, services:{sid:{reward,...}}}]
                per_service_step_rewards = {}
                for ep in step_logs:
                    for entry in ep:
                        if not isinstance(entry, dict) or 'services' not in entry:
                            continue
                        for sid, sdict in entry['services'].items():
                            try:
                                sid_int = int(sid)
                            except Exception:
                                sid_int = sid
                            per_service_step_rewards.setdefault(sid_int, []).append(float(sdict.get('reward', 0.0)))
                if per_service_step_rewards:
                    fig_sr, ax_sr = plt.subplots(figsize=(9, 4))
                    for sid, series in per_service_step_rewards.items():
                        ax_sr.plot(range(1, len(series) + 1), series, label=f'Service {sid}', alpha=0.9)
                    ax_sr.set_xlabel('RL steps (concatenated across episodes)')
                    ax_sr.set_ylabel('Reward (per step)')
                    ax_sr.set_title('Per-step Reward Trend')
                    ax_sr.legend()
                    ax_sr.grid(True, alpha=0.3)
                    fig_sr.tight_layout()
                    step_reward_fig_path = self.output_dir / f"rl_step_reward_trends_{timestamp}.png"
                    fig_sr.savefig(step_reward_fig_path)
                    plt.close(fig_sr)
                    print(f"ğŸ–¼ï¸  æŒ‰æ­¥å¥–åŠ±è¶‹åŠ¿å›¾å·²ä¿å­˜: {step_reward_fig_path}")

            # å¯é€‰ï¼šç»˜åˆ¶ç´¯ç§¯æœŸæœ›å¥–åŠ±çš„å˜åŒ–
            cumulative_rewards = tr.get('cumulative_rewards', {})
            if cumulative_rewards:
                fig_c, ax_c = plt.subplots(figsize=(8, 4))
                for sid_str, cum_rewards in cumulative_rewards.items():
                    try:
                        sid = int(sid_str)
                    except Exception:
                        sid = sid_str
                    x = list(range(1, len(cum_rewards) + 1))
                    ax_c.plot(x, cum_rewards, marker='s', label=f'Service {sid}')
                ax_c.set_xlabel('Episode')
                ax_c.set_ylabel('Cumulative Reward J_r(Ï€)')
                ax_c.set_title('Cumulative Expected Reward Trend')
                ax_c.legend()
                ax_c.grid(True, alpha=0.3)
                fig_c.tight_layout()
                cum_fig_path = self.output_dir / f"rl_cumulative_reward_trends_{timestamp}.png"
                fig_c.savefig(cum_fig_path)
                plt.close(fig_c)
                print(f"ğŸ–¼ï¸  ç´¯ç§¯æœŸæœ›å¥–åŠ±è¶‹åŠ¿å›¾å·²ä¿å­˜: {cum_fig_path}")
        except Exception as e:
            print(f"[WARN] ç»˜åˆ¶å‡†ç¡®ç‡è¶‹åŠ¿å›¾å¤±è´¥: {e}")
    
    def print_experiment_summary(self):
        """æ‰“å°å®éªŒç»“æœæ‘˜è¦"""
        if not self.experiment_results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„å®éªŒç»“æœ")
            return
        
        print(f"\n" + "="*60)
        print(f"ğŸ“Š è®ºæ–‡å®éªŒå¤ç°ç»“æœæ‘˜è¦")
        print(f"="*60)
        
        # é…ç½®ä¿¡æ¯
        config = self.experiment_results.get('config', {})
        print(f"\nğŸ”§ å®éªŒé…ç½®:")
        print(f"   å®¢æˆ·ç«¯æ•°é‡: {config.get('N', 'N/A')}")
        print(f"   æœåŠ¡æä¾›å•†æ•°é‡: {config.get('R', 'N/A')}")
        print(f"   å…¨å±€è®­ç»ƒè½®æ¬¡: {config.get('T', 'N/A')}")
        print(f"   æœ¬åœ°æ›´æ–°æ­¥æ•°: {config.get('tau', 'N/A')}")
        print(f"   æ•°æ®åˆ†å¸ƒ: IID (Ï={config.get('rho', 'N/A')})")
        
        # æ¨¡å‹æ¶æ„
        models = config.get('model_architectures', {})
        print(f"\nğŸ§  æ¨¡å‹æ¶æ„:")
        for task, arch in models.items():
            print(f"   ä»»åŠ¡{task}: {arch}")
        
        # æ¨¡å‹æ€§èƒ½ç»“æœï¼ˆæ–°å¢ï¼‰
        model_performance = self.experiment_results.get('model_performance', {})
        if model_performance:
            print(f"\nğŸ¯ æ¨¡å‹æ€§èƒ½ç»“æœ:")
            total_accuracy = 0
            valid_services = 0
            
            for service_id, result in model_performance.items():
                if 'error' not in result:
                    print(f"   æœåŠ¡{service_id} ({result['dataset']}):")
                    print(f"     å‡†ç¡®ç‡: {result['accuracy']:.2f}% ({result['correct']}/{result['total']})")
                    print(f"     æŸå¤±: {result['loss']:.4f}")
                    print(f"     è¯„ä¼°æ‰¹æ¬¡: {result['batches_evaluated']}")
                    total_accuracy += result['accuracy']
                    valid_services += 1
                else:
                    print(f"   æœåŠ¡{service_id} ({result['dataset']}): è¯„ä¼°å¤±è´¥")
            
            if valid_services > 0:
                avg_accuracy = total_accuracy / valid_services
                print(f"   ğŸ“Š å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")
        
        # PACç®—æ³•æ€§èƒ½ç»“æœ
        eval_results = self.experiment_results.get('evaluation_results', {})
        if eval_results:
            print(f"\nğŸ“ˆ PACç®—æ³•æ€§èƒ½:")
            
            avg_cumulative = eval_results.get('avg_cumulative_rewards', {})
            if avg_cumulative:
                print(f"   ç´¯ç§¯å¥–åŠ±J_r(Ï€) (æ–¹ç¨‹19-20):")
                for service_id, reward in avg_cumulative.items():
                    print(f"     æœåŠ¡{service_id}: {reward:.4f}")
            
            avg_episode = eval_results.get('avg_episode_rewards', {})
            if avg_episode:
                print(f"   å¹³å‡å›åˆå¥–åŠ±:")
                for service_id, reward in avg_episode.items():
                    print(f"     æœåŠ¡{service_id}: {reward:.4f}")
        
        # è®­ç»ƒä¿¡æ¯
        duration = self.experiment_results.get('experiment_duration', 0)
        print(f"\nâ±ï¸  è®­ç»ƒç”¨æ—¶: {duration:.2f} ç§’")
        
        print(f"="*60)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œè®ºæ–‡å®éªŒå¤ç°"""
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = PaperExperimentConfig()
    
    # åˆ›å»ºå®éªŒæ‰§è¡Œå™¨
    runner = PaperExperimentRunner(config)
    
    # è¿è¡Œå®éªŒ
    results = runner.run_experiment()
    
    # æ‰“å°ç»“æœæ‘˜è¦
    runner.print_experiment_summary()
    
    return results


if __name__ == "__main__":
    results = main()
