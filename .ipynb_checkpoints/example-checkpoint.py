#!/usr/bin/env python3
"""
å¤šæœåŠ¡æä¾›å•†è”é‚¦å­¦ä¹ ç³»ç»Ÿçš„ç¤ºä¾‹ç”¨æ³•
æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•å°†å·²å®ç°çš„ç³»ç»Ÿæ¨¡å‹ä¸åˆæˆæ•°æ®ç»“åˆä½¿ç”¨ï¼Œä»¥éªŒè¯è¯¥æ¡†æ¶ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import json
from typing import Dict, List, Tuple

from multi_service_fl import (
    MultiServiceFLSystem, 
    ServiceProviderConfig, 
    ClientResourceConfig
)
from quantization import QuantizationModule
from communication import SystemMetrics


class SyntheticDataset(Dataset):
    """ç”¨äºæµ‹è¯•çš„åˆæˆæ•°æ®é›†ã€‚"""
    
    def __init__(self, num_samples: int, input_dim: int, num_classes: int, seed: int = 42):
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        self.num_samples = num_samples
        self.input_dim = input_dim
        self.num_classes = num_classes
        
        # ç”Ÿæˆåˆæˆæ•°æ®
        self.data = torch.randn(num_samples, input_dim)
        
        # åˆ›å»ºå…·æœ‰ä¸€å®šç»“æ„çš„æ ‡ç­¾ï¼ˆå¹¶éå®Œå…¨éšæœºï¼‰
        weights = torch.randn(input_dim, num_classes)
        logits = torch.matmul(self.data, weights)
        self.labels = torch.argmax(logits, dim=1)
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]


class SimpleMLP(nn.Module):
    """"ç”¨äºæµ‹è¯•çš„ç®€å•å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰"""
    
    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x


class SimpleCNN(nn.Module):
    """ç”¨äºç±»å›¾åƒæ•°æ®çš„ç®€å•å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰"""
    
    def __init__(self, input_channels: int, num_classes: int):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # å‡è®¾è¾“å…¥ä¸º 32x32 å¤§å° -> ç»è¿‡æ± åŒ–åå˜ä¸º 8x8 å¤§å°
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def create_synthetic_datasets(num_clients_per_service: int) -> Tuple[Dict[int, Dataset], Dict[int, Dataset]]:
    """åˆ›å»ºç”¨äºæµ‹è¯•çš„åˆæˆæ•°æ®é›†ã€‚"""
    
    train_datasets = {}
    test_datasets = {}
    
    # æœåŠ¡ 1ï¼šç±»å›¾åƒæ•°æ®ï¼ˆä¸ºå¤šå±‚æ„ŸçŸ¥æœºå±•å¹³å¤„ç†ï¼‰
    for i in range(1, num_clients_per_service + 1):
        # ä¸ºä¸åŒå®¢æˆ·ç«¯åˆ›å»ºéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰çš„æ•°æ®
        base_seed = 42 + i
        dataset_size = 500 + (i * 100)  # ä¸åŒå®¢æˆ·ç«¯æ‹¥æœ‰ä¸åŒæ•°é‡çš„æ ·æœ¬
        
        train_datasets[i] = SyntheticDataset(
            num_samples=dataset_size,
            input_dim=784,  # 28*28 like MNIST
            num_classes=10,
            seed=base_seed
        )
        
        test_datasets[i] = SyntheticDataset(
            num_samples=100,
            input_dim=784,
            num_classes=10,
            seed=base_seed + 1000
        )
    
    # æœåŠ¡ 2ï¼šç±»æ–‡æœ¬æ•°æ®ï¼ˆä¸åŒç»´åº¦ï¼‰
    for i in range(num_clients_per_service + 1, 2 * num_clients_per_service + 1):
        base_seed = 42 + i
        dataset_size = 400 + (i * 80)
        
        train_datasets[i] = SyntheticDataset(
            num_samples=dataset_size,
            input_dim=300,  # ç±»ä¼¼ word embeddings
            num_classes=5,
            seed=base_seed
        )
        
        test_datasets[i] = SyntheticDataset(
            num_samples=80,
            input_dim=300,
            num_classes=5,
            seed=base_seed + 1000
        )
    
    return train_datasets, test_datasets


def run_quantization_analysis():
    """åˆ†æé‡åŒ–å¯¹æ¨¡å‹å‚æ•°çš„å½±å“ã€‚"""
    print("\n" + "="*60)
    print("é‡åŒ–åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•model
    model = SimpleMLP(input_dim=784, hidden_dim=128, num_classes=10)
    
    # æµ‹è¯•ä¸åŒçš„é‡åŒ–ç­‰çº§
    quantizer = QuantizationModule()
    quantization_levels = [2, 4, 8, 16, 32]
    
    results = []
    
    for q in quantization_levels:
        total_volume = 0
        total_params = 0
        
        for name, param in model.named_parameters():
            param_size = param.numel()
            total_params += param_size
            
            # é‡åŒ–å‚æ•°
            quantized_param, norm_value = quantizer.quantize_parameters(param, q)
            
            # è®¡ç®—é€šä¿¡é‡
            volume = quantizer.calculate_communication_volume(param_size, q)
            total_volume += volume
        
        # è®¡ç®—å‹ç¼©ç‡
        original_volume = total_params * 32  # 32 bits per float
        compression_ratio = original_volume / total_volume
        
        results.append({
            'quantization_level': q,
            'total_params': total_params,
            'original_volume': original_volume,
            'compressed_volume': total_volume,
            'compression_ratio': compression_ratio,
            'bits_per_param': total_volume / total_params
        })
        
        print(f"Quantization Level {q:2d}: "
              f"Compression {compression_ratio:.2f}x, "
              f"Bits/param {total_volume/total_params:.2f}")
    
    return results


def run_communication_analysis():
    """åˆ†æé€šä¿¡æˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚"""
    print("\n" + "="*60)
    print("é€šä¿¡åˆ†æ")
    print("="*60)
    
    system_metrics = SystemMetrics()
    
    # æµ‹è¯•ä¸åŒçš„å®¢æˆ·ç«¯é…ç½®
    client_configs = [
        {'id': 1, 'mu': 1e-28, 'c': 1000, 'freq': 1e9, 'power': 0.1, 'gain': 1e-3, 'size': 1000},
        {'id': 2, 'mu': 1.5e-28, 'c': 1200, 'freq': 1.2e9, 'power': 0.15, 'gain': 1.2e-3, 'size': 1200},
        {'id': 3, 'mu': 0.8e-28, 'c': 800, 'freq': 0.8e9, 'power': 0.08, 'gain': 0.8e-3, 'size': 800},
    ]
    
    communication_volumes = [10000, 25000, 50000, 100000]  # ä¸åŒçš„å¤§å°
    bandwidth = 1e6  # 1 MHz
    
    results = []
    
    for vol in communication_volumes:
        print(f"\né€šä¿¡é‡: {vol} bits")
        vol_results = {'volume': vol, 'clients': []}
        
        for client in client_configs:
            metrics = system_metrics.calculate_client_metrics(
                client_id=client['id'],
                service_id=1,
                mu_i=client['mu'],
                c_ir=client['c'],
                dataset_size=client['size'],
                cpu_frequency=client['freq'],
                communication_volume=vol,
                bandwidth=bandwidth,
                channel_gain=client['gain'],
                transmit_power=client['power']
            )
            
            vol_results['clients'].append({
                'client_id': client['id'],
                'computation_energy': metrics.computation_energy,
                'computation_delay': metrics.computation_delay,
                'communication_energy': metrics.communication_energy,
                'communication_delay': metrics.communication_delay,
                'total_energy': metrics.total_energy,
                'total_delay': metrics.total_delay
            })
            
            print(f"  Client {client['id']}: "
                  f"æ€»èƒ½è€—={metrics.total_energy:.6f}J, "
                  f"æ€»å»¶è¿Ÿ={metrics.total_delay:.6f}s")
        
        results.append(vol_results)
    
    return results


def run_federated_learning_example():
    """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„è”é‚¦å­¦ä¹ ç¤ºä¾‹ã€‚"""
    print("\n" + "="*60)
    print("è”é‚¦å­¦ä¹ ç¤ºä¾‹ã€‚")
    print("="*60)
    
    # Configuration
    num_clients_per_service = 3
    
    # Create synthetic datasets
    train_datasets, test_datasets = create_synthetic_datasets(num_clients_per_service)
    
    # Define service configurations
    service_configs = [
        ServiceProviderConfig(
            service_id=1,
            name="å›¾åƒåˆ†ç±»æœåŠ¡",
            client_ids=list(range(1, num_clients_per_service + 1)),
            model_architecture={"type": "mlp", "input_dim": 784, "num_classes": 10},
            quantization_level=8,
            local_epochs=1,
            learning_rate=0.01,
            users_per_round=2
        ),
        ServiceProviderConfig(
            service_id=2,
            name="æ–‡æœ¬åˆ†ç±»æœåŠ¡", 
            client_ids=list(range(num_clients_per_service + 1, 2 * num_clients_per_service + 1)),
            model_architecture={"type": "mlp", "input_dim": 300, "num_classes": 5},
            quantization_level=4,
            local_epochs=1,
            learning_rate=0.01,
            users_per_round=2
        )
    ]
    
    # å®šä¹‰å®¢æˆ·ç«¯èµ„æºé…ç½®
    client_configs = {}
    for i in range(1, 2 * num_clients_per_service + 1):
        client_configs[i] = ClientResourceConfig(
            client_id=i,
            mu_i=(0.8 + 0.1 * i) * 1e-28,  # å¯å˜ç”µå®¹
            c_ir=1000 + (i * 100),          # å¯å˜CPUå‘¨æœŸæ•°
            max_frequency=(0.8 + 0.1 * i) * 1e9,  # å¯å˜é¢‘ç‡
            max_power=0.08 + (0.01 * i),    # å¯å˜åŠŸç‡
            channel_gain=(0.8 + 0.1 * i) * 1e-3,  # å¯å˜ä¿¡é“å¢ç›Š
            dataset_size=len(train_datasets[i])
        )
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = MultiServiceFLSystem(service_configs, client_configs)
    
    # è®¾ç½®æœåŠ¡å’Œæ¨¡å‹
    # æœåŠ¡1: å›¾åƒåˆ†ç±»
    model1 = SimpleMLP(input_dim=784, hidden_dim=128, num_classes=10)
    service1_datasets = {i: train_datasets[i] for i in range(1, num_clients_per_service + 1)}
    system.setup_service(1, model1, service1_datasets)
    
    # æœåŠ¡2: æ–‡æœ¬åˆ†ç±»
    model2 = SimpleMLP(input_dim=300, hidden_dim=64, num_classes=5)
    service2_datasets = {i: train_datasets[i] for i in range(num_clients_per_service + 1, 2 * num_clients_per_service + 1)}
    system.setup_service(2, model2, service2_datasets)
    
    print("ç³»ç»Ÿè®¾ç½®å®Œæˆ")
    
    # è®­ç»ƒæœåŠ¡
    print("\nå¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ...")
    results = system.train_all_services(num_rounds=3)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("è®­ç»ƒç»“æœ")
    print("="*60)
    
    for service_id, (model, metrics) in results.items():
        print(f"\næœåŠ¡ {service_id} ç»“æœ:")
        print(f"  é‡åŒ–ç­‰çº§: {metrics.get('quantization_level', 'N/A')}")
        print(f"  æ€»å‚æ•°é‡: {metrics.get('total_parameters', 'N/A')}")
        print(f"  å‹ç¼©æ¯”: {metrics.get('compression_ratio', 'N/A'):.2f}x")
        print(f"  æ¯è½®é€šä¿¡é‡: {metrics.get('communication_volume_per_round', 'N/A')} bits")
        print(f"  æ€»èƒ½è€—: {metrics.get('total_energy', 'N/A'):.6f}ç„¦")
        print(f"  æ€»å»¶è¿Ÿ: {metrics.get('total_delay', 'N/A'):.6f}ç§’")
        print(f"  å®¢æˆ·ç«¯æ•°é‡: {metrics.get('num_clients', 'N/A')}")
    
    return results

def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹çš„ä¸»å‡½æ•°ã€‚"""
    print("å¤šæœåŠ¡æä¾›å•†è”é‚¦å­¦ä¹  - ç³»ç»ŸéªŒè¯")
    print("=" * 70)
    
    # è¿è¡Œå„ä¸ªç»„ä»¶åˆ†æ
    quantization_results = run_quantization_analysis()
    communication_results = run_communication_analysis()
    
    # è¿è¡Œå®Œæ•´çš„è”é‚¦å­¦ä¹ ç¤ºä¾‹
    try:
        fl_results = run_federated_learning_example()
        print("\nâœ… æ‰€æœ‰æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
        # ä¿å­˜ç»“æœ
        results = {
            'é‡åŒ–åˆ†æ': quantization_results,
            'é€šä¿¡åˆ†æ': communication_results,
            'è”é‚¦å­¦ä¹ ç»“æœ': {
                str(k): v[1] for k, v in fl_results.items()  # åªä¿å­˜æŒ‡æ ‡ï¼Œä¸ä¿å­˜æ¨¡å‹
            }
        }
        
        with open('My_FL/validation_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print("ç»“æœå·²ä¿å­˜åˆ° validation_results.json")
        
    except Exception as e:
        print(f"\nâŒ è”é‚¦å­¦ä¹ ç¤ºä¾‹è¿è¡Œå‡ºé”™: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äº FLSim é›†æˆé—®é¢˜ - è¯·æ£€æŸ¥ä¾èµ–é¡¹")
    
    print("\n" + "="*70)
    print("æ€»ç»“")
    print("="*70)
    print("âœ… é‡åŒ–æ¨¡å—: æ­£å¸¸å·¥ä½œ")
    print("âœ… é€šä¿¡æ¨¡å‹: æ­£å¸¸å·¥ä½œ") 
    print("âœ… å¤šæœåŠ¡è”é‚¦å­¦ä¹ æ¡†æ¶: å·²å®ç°")
    print("ğŸ“‹ è®ºæ–‡å…¬å¼ (1)-(13): å…¨éƒ¨å·²å®ç°")
    print("\nç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œè®ºæ–‡å¤ç°å®éªŒï¼")


if __name__ == "__main__":
    main()